 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Machine Learning 

9 − Classification 

SS 2016 

Gunther Heidemann 

Overview 

 

1. Definition of the classification problem 

2. Bayesian classification 

3.

Important properties of classifiers 

4. Overview of standard types of classifiers 

5. Support vector machine (overview) 

 

 

For details on the SVM, see, e.g.,  

Vladimir Cherkassky, Filip Mulier: Learning from Data, IEEE Press 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

ML-9 Classification 

2 

Classification 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Classification: 



Assigns a discrete class to an object, fact, person, event etc. 
based on attributes. 

 Classification results might be, e.g., {dry, wet}, {good customer, 

bad customer}, {go, stop, left, right}, {spoon, fork, knife}. 





Attributes might be color, length, age, income, voltage …  

Technically,  







attributes are represented by a feature vector xℝd, 

output are natural numbers c(x)  C  ℕ assigned to the |C| 
different classes, 

alternatively, there are |C| real valued functions c1(x) … c|C|(x), 
each of which is the “confidence” that belongs to the class. 
The ci are called discriminant functions. The output is  
 

 

 

  

 

 

c(x) = arg maxi ci(x),  

c(x)  C. 

ML-9 Classification 

3 

Bayes classifier 

Idea of the Bayes classifier: 

Classify an input x such that the expected cost is minimized! 

That is, choose output class c such that 

 

 

 

 P(c|x)  =  N  P(x|c)  P(c), 

is maximized, where 

P(x|c) 

is the probability density that class c has features x, 

P(c) 

is the a priori probability of class c, and 

N    

is the normalization factor. 

Representing classes as probabilities,  





overlapping classes can be 
represented, 

a unique class assignment is not 
possible. 

ML-9 Classification 

P(c=1 | x ) 

P(c=2 | x ) 

x 

[H] 

4 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Bayes classifier 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Problems of the Bayes classifier: 

In 

 

 

 

 P(c|x)  =  N  P(x|c)  P(c), 

the probability densities P(x|c), and sometimes also P(c), are not 
known explicitly but need to be estimated from the data. 

 

Two strategies: 

1. Estimate P(x|c) and - if necessary -  P(c) from the data, then apply 

original Bayes classifier. May require a lot of effort and more 
information than necessary to obtain the classifier (because a 
classifier needs information to derive decision boundaries, not 
necessarily the complete densities. 

2. Construct an approximation to the Bayes classifier directly from 

the data.  

ML-9 Classification 

5 

Bayes classifier 

Complete knowledge of P(x|c) may be unnecessary to find a 
separatrix 

 

 

 

 

 

 

… or necessary:  

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

[H] 

ML-9 Classification 

6 

Types of classifiers 

We will review basic types of classifiers.  

These are relevant criteria: 

 

General issues: 

 What assumptions are made about the data distribution? 

 What is the bias? 

 

Technical issues: 

 Representation: 

 How are separatrices represented (implicitly, explicitly)? 



Type of separatrices? 

 Generalization properties? 



Sensitivity to errors and noise? 

ML-9 Classification 

7 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Types of classifiers 



Locality: 



Is the influence of a single example on the separatrices local 
or global? 

 How is locality controlled by parameters? 



Parameters: 





Parameters for knowledge representation, e.g., architecture, 
flexibility of separatrices, smoothness? 

Adaptation parameters, e.g., step size? 

 Do the parameters have an intuitive interpretation? 

 How much depends the outcome on the choice of the 

parameters? 



Speed: 





Training phase 

Application phase 

ML-9 Classification 

8 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Types of classifiers 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

The following, we compare standard classifiers according to the 
criteria. 

 

We assume  

the input data have been transformed to zero mean,  

there are only two classes, c+ and c–.   x+, x– denote feature 
vectors of the classes. 





 

The classifier c is based on a discriminant function R(x) such that 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 c+  if R(x)  0, 

c(x)  = 

  

 

 

 c– 

if R(x) < 0. 

 

 

 

 

ML-9 Classification 

9 

Euclidean classifier 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

– 

– 

– 

– 

– 

+ 

Center of mass 
of “+” class 

+ 

– 
– – 
– 

– 
– 

– 
– 
– 

– 
+ 

– 

– 

– 
– 
– – 
– 
– 
+ 
+ 
– 
+ 

+ 

+ 
+ 

+ 
Separatrix 

+ 

+ 

+  + 
+  + 
+ 
+ 
+ 
+ 
w 
+ 
+ 

[H] 

Discriminant is found from the 
centers of mass of the classes:  

 

R(x) 

=  w  x 

with 

 

 



     w 

=  <x+> – <x–>. 

Linear separatrix, explicit 
representation. 

 Not local. 



Very fast “training”. 

 No parameters. 



 

 

Sensitive to far outliers. 

ML-9 Classification 

10 

Linear discriminant analysis  (LDA) 

Assumptions: 

 Classes have identical a priori 

probability. 



Both classes exhibit a Gaussian 
distribution with means <x+>, <x–> 
and covariance matrices +, –. 

R(x)  =  wx 

with 

    w  = –1 (<x+> – <x–>), 

       =  <x xT>. 

 Covariances are assumed to be 

identical for both classes  = += –. 

 

 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

 
 

 

Same properties as 
Euclidean classifier 
(qualitatively). 

 



 

 

– 

– 
– 

– 

+ 
+ 
+ 

w 

+ 

+ 
+ 
+ 

+ 

+ 

+ 

+ 

+ 

+ 
– 
+ 

+ 
+ 

+ 

+ 
+ 
+ 

– 

– 
– 

– 
– 

– 

– 

– 

– 
+ 
– 

– 

+ 

– 

– 
– 
– 

ML-9 Classification 

11 

Separatrix 

[H] 

Quadratic classifier 

– 

– 

– 

– 
– 
– 
– 
– 
– 
– 
– 
– 
– 
+  + 
+ 
– 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
– 
+ 
+ 
+ 
+ 
+  + 
+ 
+ 
– 
+ 
– 
+ 
– 
+ 
– 
– 

– 
– 
– 

Separatrix 

[H] 

– 

– 
– 

– 

– 
– 
– 

– 
– 

– 

Discriminant is quadratic   

 

 









R(x)  = x T A x + bTx + c,  

 

 

Aℝd xd ,  b,xℝd,  cℝ. 

A, b, c are found, e.g., by quadratic 
discriminant analysis (QDA). 

Separatrix is a conic section, i.e., a 









hyperbola, 

parabola, 

ellipsis (special case: circle), 

line. 

Effect of A, b, c is not local. 

Fast “training”. 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

 No parameters. 

 

ML-9 Classification 

12 

Polynom classifier 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Polynomial discriminant function: 

 



R(x) 

=   Polynom(x). 

 

Separatrix of almost arbitrary 
complexity (like MLP). 

 Generalization can be controlled by 

degree of polynom,  

 … but the effect of the degree as a 

parameter is difficult to foresee in high 
dimensions. 

 Not local. 



“Training” is relatively efficient. 

 Well established technique, e.g., in 

– 

– 

– 
+ 
– 
+ 
+ 
– 
– 
– 
– 
– 
+ 
+ 
– 
– 
– 
+ 
+ 
– 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
+  + 
+ 
– 
– 
+ 
+ + 
– 
+ 
+ 
+ 
– 
+ 
+ 
– 
+ 
– 
Separatrix 

– 
– 
– 
– 
– 
– 
– 

– 
– 
– 
– 
– 

[H] 

NLP. 

 

ML-9 Classification 

13 

Nearest neighbor classifier 





Separatrix is implicitly defined by 
neighbors.  

Local. 

– 

 No generalization (as far as 

classification boundary is concerned). 

– 

 No parameters. 

 No training time but may have 

considerable memory consumption. 



Application time depends on size. 

 

– 

– 
– 
– – 
– 
–  – 

+ 
+ 

+ + 
+ 
+ 

– 

– 
– 
– 
– 

– 
– 
– 

– 
– 

– 

– 

– 
+ 
– 
+ 
– 
+ 
+ 
+ 
+ 
+ + 
+ 
+ 
+ 
+ + 
– 
+ + 

+ 

+ 

+ 
+ 
+ 

+ 
+ 

[H] 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

ML-9 Classification 

14 

Support vector machine 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Support vector machine (SVM) is one of the most popular classifiers. 

Principle: 

1. SVM computes a hyperplane (linear separatrix) 







based on the examples (support vectors) close to the class 
boundary, 

such that the margin is maximized. 

Slack variables allow to deal with outliers to avoid overfitting. 

2. To solve nonlinear problems, the kernel trick is used: 







Project data into a space of higher dimension. 

For sufficiently high dimension, every problem becomes 
(linearly) separable by a hyperplane. 

The projection of this hyperplane back to the original data 
space is a nonlinear separatrix. 

ML-9 Classification 

15 

Support vector machine 

Binary classification problem D = {(x1,y1), (x2,y2), …}, xiℝd,  yi  [–1,1]. 

SVM represents a hyperplane 

 

 

 

 

 

 

w  x – b = 0, 

where w is the normal to the hyperplane and b / |w| is its offset from 
the origin. 

 

 For a linearly separable problem, the 
SVM selects w and b such that the 
separatrix is in the middle of the margin. 

Margin 

An extension provides a way to deal with 
outliers that disturb linear separability. 

 

  

 

ML-9 Classification 

– 
– 
–  – 

– 
–  – 
– 
–  – 
– 
– 
– 

+ 
+ 

+ 
+ 

+ 

+ 

Support 
vectors 

+ 

+ 

+ 
+ 

+ 

+ 

[H] 

16 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Support vector machine 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

To apply the SVM to not linearly separable problems, map the data 
from the original input space (xℝd) to a higher dimensional space H 
where it becomes linearly separable. 

Remember solving the XOR problem with a simple perceptron in a 
space of higher dimension: 

x2 

1 

x1 

0 

1 
XOR 

 

 

 

 

 

 

x3=x1x2 

 

x2 

x1 

[H] 

If this is insufficient for a problem, the 2d input vectors (x1,x2)T might 
be mapped, e.g., using polynomials to  

 

 

 

(x1, x2, x1

2, x2

2, x1x2, x1

3, x2

3, x1

2x2, x1x2

2)T .  

ML-9 Classification 

17 

Support vector machine 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

Problem of mapping to a space of higher dimension: 



For more input dimensions, the mapping ℝd H using, e.g., 
polynomials becomes huge. 

 Mapping causes high effort. 

Solution by the kernel trick: 









Fortunately, the SVM procedure to find the optimal hyperplane 
within the margin only requires the inner product of vectors xaxb, 
not the vectors themselves. 

Thus, only the inner products need to be computed in H. 

This can be done without actually computing the representation of 
xaxb  in H. 

Instead, the inner product of xa and xb in H can be computed 
directly in the original input space using a kernel function K as 
K(xa,xb).  

ML-9 Classification 

18 

Support vector machine 







The kernel trick allows the very efficient implicit computation of a 
hyperplane in H and thus a nonlinear separatrix in ℝd. 

The kernel function should satisfy Mercer’s condition, but in 
practice useful results can be achieved also with other kernel 
functions. 

Suitable mapping functions with a kernel that satisfies Mercer’s 
condition are, e.g., polynomials / splines, radial basis functions or 
Fourier expansions. 

 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

ML-9 Classification 

19 

Summary 

 Classification is one of the most important ML tasks. 

 Classifiers differ with respect to many criteria, the most important 

of which is the shape, “flexibility” and representation of the 
separatrices. 





SVMs are one of the most popular classifiers, along with the MLP. 
Numerous implementations exist. 

Success of classification depends very much on the choice of 
suitable features from the problem. As a rule of thumb, features 
are more important than the classifier. 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

ML-9 Classification 

20 

Image sources 

[M] Online material available at www.cs.cmu.edu/~tom/mlbook.html 

for the textbook: Tom M. Mitchell: Machine Learning, McGraw-Hill 

[H]  Gunther Heidemann, 2012. 

 

 

e
c
n
e

i

c
S
e
v

 

i
t
i
n
g
o
C

 

 
f
o
e
t
u
t
i
t
s
n

I
 
,

k
c
ü
r
b
a
n
s
O

 
f
o
y
t
i

 

s
r
e
v
n
U

i

ML-9 Classification 

21 

