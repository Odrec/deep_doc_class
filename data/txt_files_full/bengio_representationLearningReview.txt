                                                                                   1
          Representation Learning: A Review and New
                                    Perspectives
                                     †                          †
                          Yoshua Bengio , Aaron Courville, and Pascal Vincent
                    Department of computer science and operations research, U. Montreal
                         † also, Canadian Institute for Advanced Research (CIFAR)
                                             ✦
        Abstract—                             explanatory factors for the observed input. A good representa-
         The success of machine learning algorithms generally depends ontion is also one that is useful as input to a supervised predictor.
        data representation, and we hypothesize that this is because different
        representations can entangle and hide more or less the different ex-Amongthevariouswaysoflearningrepresentations, this paper
        planatory factors of variation behind the data. Although speciﬁc domainfocuses on deep learning methods: those that are formed by
        knowledge can be used to help design representations, learning withthe composition of multiple non-linear transformations, with
        generic priors can also be used, and the quest for AI is motivatingthe goal of yielding more abstract – and ultimately more useful
        the design of more powerful representation-learning algorithms imple-– representations. Here we survey this rapidly developing area
        menting such priors. This paper reviews recent work in the area ofwith special emphasis on recent progress. We consider some
        unsupervised feature learning and deep learning, covering advances
        in probabilistic models, auto-encoders, manifold learning, and deepof the fundamental questions that have been driving research
        networks. This motivates longer-term unanswered questions about thein this area. Speciﬁcally, what makes one representation better
        appropriate objectives for learning good representations, for computingthan another? Given an example, how should we compute its
        representations (i.e., inference), and the geometrical connections be-representation, i.e. perform feature extraction? Also, what are
        tweenrepresentationlearning,densityestimationandmanifoldlearning.appropriate objectives for learning good representations?
        IndexTerms—Deeplearning,representationlearning,featurelearning,
        unsupervised learning, Boltzmann Machine, autoencoder, neural nets2 WHY SHOULD WE CARE ABOUT LEARNING
        1 INTRODUCTION                        REPRESENTATIONS?
                                              Representation learning has become a ﬁeld in itself in the
        The performance of machine learning methods is heavilymachine learning community, with regular workshops at the
        dependent on the choice of data representation (or features)leading conferences such as NIPS and ICML, and a new
        on which they are applied. For that reason, much of the actualconference dedicated to it, ICLR1, sometimes under the header
        effort in deploying machine learning algorithms goes into theof Deep Learning or Feature Learning. Although depth is an
        design of preprocessing pipelines and data transformations thatimportant part of the story, many other priors are interesting
        result in a representation of the data that can support effectiveand can be conveniently captured when the problem is cast as
        machine learning. Such feature engineering is important butone of learning a representation, as discussed in the next sec-
        labor-intensive and highlights the weakness of current learningtion. The rapid increase in scientiﬁc activity on representation
        algorithms: their inability to extract and organize the discrimi-learning has been accompanied and nourished by a remarkable
     arXiv:1206.5538v3  [cs.LG]  23 Apr 2014native information from the data. Feature engineering is a waystring of empirical successes both in academia and in industry.
        to take advantage of human ingenuity and prior knowledge toBelow, we brieﬂy highlight some of these high points.
        compensate for that weakness. In order to expand the scopeSpeech Recognition and Signal Processing
        and ease of applicability of machine learning, it would beSpeech was one of the early applications of neural networks,
        highly desirable to make learning algorithms less dependentin particular convolutional (or time-delay) neural networks 2.
        on feature engineering, so that novel applications could beTherecent revival of interest in neural networks, deep learning,
        constructed faster, and more importantly, to make progressand representation learning has had a strong impact in the
        towards Artiﬁcial Intelligence (AI). An AI must fundamentallyarea of speech recognition, with breakthrough results (Dahl
        understand the world around us, and we argue that this canet al., 2010; Deng et al., 2010; Seide et al., 2011a; Mohamed
        only be achieved if it can learn to identify and disentangle theet al., 2012; Dahl et al., 2012; Hinton et al., 2012) obtained
        underlying explanatory factors hidden in the observed milieuby several academics as well as researchers at industrial labs
        of low-level sensory data.            bringing these algorithms to a larger scale and into products.
         This paper is about representation learning, i.e., learningFor example, Microsoft has released in 2012 a new version
        representations of the data that make it easier to extract usefulof their MAVIS (Microsoft Audio Video Indexing Service)
        information when building classiﬁers or other predictors. In
        the case of probabilistic models, a good representation is often1. International Conference on Learning Representations
        one that captures the posterior distribution of the underlying2. See Bengio (1993) for a review of early work in this area.
                                                                                                                                           2
             speech system based on deep learning (Seide et al., 2011a).      2010) and it has recently been extended to deeper multi-modal
             These authors managed to reduce the word error rate on           representations (Srivastava and Salakhutdinov, 2012).
             four major benchmarks by about 30% (e.g. from 27.4% to             The neural net language model was also improved by
             18.5% on RT03S) compared to state-of-the-art models based        adding recurrence to the hidden layers (Mikolov et al., 2011),
             on Gaussian mixtures for the acoustic modeling and trained on    allowing it to beat the state-of-the-art (smoothed n-gram
             the same amount of data (309 hours of speech). The relative      models) not only in terms of perplexity (exponential of the
             improvement in error rate obtained by Dahl et al. (2012) on a    average negative log-likelihood of predicting the right next
             smaller large-vocabulary speech recognition benchmark (Bing      word, going down from 140 to 102) but also in terms of
             mobile business search dataset, with 40 hours of speech) is      word error rate in speech recognition (since the language
             between 16% and 23%.                                             model is an important component of a speech recognition
               Representation-learning algorithms have also been applied      system), decreasing it from 17.2% (KN5 baseline) or 16.9%
             to music, substantially beating the state-of-the-art in poly-    (discriminative language model) to 14.4% on the Wall Street
             phonic transcription (Boulanger-Lewandowski et al., 2012),       Journal benchmark task. Similar models have been applied
             with relative error improvement between 5% and 30% on a          in statistical machine translation (Schwenk et al., 2012; Le
             standard benchmark of 4 datasets. Deep learning also helped      et al., 2013), improving perplexity and BLEU scores. Re-
             to win MIREX (Music Information Retrieval) competitions,         cursive auto-encoders (which generalize recurrent networks)
             e.g. in 2011 on audio tagging (Hamel et al., 2011).              have also been used to beat the state-of-the-art in full sentence
             Object Recognition                                               paraphrase detection (Socher et al., 2011a) almost doubling the
               The beginnings of deep learning in 2006 have focused on        F1 score for paraphrase detection. Representation learning can
             the MNIST digit image classiﬁcation problem (Hinton et al.,      also be used to perform word sense disambiguation (Bordes
             2006; Bengio et al., 2007), breaking the supremacy of SVMs       et al., 2012), bringing up the accuracy from 67.8% to 70.2%
             (1.4% error) on this dataset3. The latest records are still held on the subset of Senseval-3 where the system could be applied
             by deep networks: Ciresan et al. (2012) currently claims the     (with subject-verb-object sentences). Finally, it has also been
             title of state-of-the-art for the unconstrained version of the tasksuccessfully used to surpass the state-of-the-art in sentiment
             (e.g., using a convolutional architecture), with 0.27% error,    analysis (Glorot et al., 2011b; Socher et al., 2011b).
             and Rifai et al. (2011c) is state-of-the-art for the knowledge-  Multi-Task and Transfer Learning, Domain Adaptation
             free version of MNIST, with 0.81% error.                           Transfer learning is the ability of a learning algorithm to
               In the last few years, deep learning has moved from            exploit commonalities between different learning tasks in order
             digits to object recognition in natural images, and the latest   to share statistical strength, and transfer knowledge across
                                                                          4   tasks. As discussed below, we hypothesize that representation
             breakthrough has been achieved on the ImageNet dataset
             bringing down the state-of-the-art error rate from 26.1% to      learning algorithms have an advantage for such tasks because
             15.3% (Krizhevsky et al., 2012).                                 they learn representations that capture underlying factors, a
             Natural Language Processing                                      subset of which may be relevant for each particular task, as
               Besides speech recognition, there are many other Natural       illustrated in Figure 1. This hypothesis seems conﬁrmed by a
             Language Processing (NLP) applications of representation         number of empirical results showing the strengths of repre-
                                                                              sentation learning algorithms in transfer learning scenarios.
             learning. Distributed representations for symbolic data were                         task 1    task 2     task 3  
                                                                                                  output y1 output y2  output y3 
             introduced by    Hinton (1986), and ﬁrst developed in the                            Task%A%   Task%B%    Task%C%
             context of statistical language modeling by Bengio et al.                    %output%
             (2003) in so-called neural net language models (Bengio,
             2008). They are all based on learning a distributed repre-
             sentation for each word, called a word embedding. Adding a                      %shared%
             convolutional architecture, Collobert et al. (2011) developed                 subsets%of%
                                 5                                                           factors%
             the SENNA system that shares representations across the
             tasks of language modeling, part-of-speech tagging, chunking,
             named entity recognition, semantic role labeling and syntactic
             parsing. SENNA approaches or surpasses the state-of-the-art                    %input%
             on these tasks but is simpler and much faster than traditional                                 raw input x 
                                                                              Fig. 1.  Illustration of representation-learning discovering ex-
             predictors. Learning word embeddings can be combined with        planatory factors (middle hidden layer, in red), some explaining
             learning image representations in a way that allow to associate  the input (semi-supervised setting), and some explaining target
             text and images. This approach has been used successfully to     for each task. Because these subsets overlap, sharing of statis-
                                                                              tical strength helps generalization.
             build Google’s image search, exploiting huge quantities of data  .
             to map images and queries in the same space (Weston et al.,        Most impressive are the two transfer learning challenges
                                                                              held in 2011 and won by representation learning algorithms.
               3. for the knowledge-free version of the task, where no image-speciﬁc priorFirst, the Transfer Learning Challenge, presented at an ICML
             is used, such as image deformations or convolutions              2011 workshop of the same name, was won using unsuper-
               4. The 1000-class ImageNet benchmark, whose results are detailed here:vised layer-wise pre-training (Bengio, 2011; Mesnil et al.,
             http://www.image-net.org/challenges/LSVRC/2012/results.html
               5. downloadable from http://ml.nec-labs.com/senna/             2011). A second Transfer Learning Challenge was held the
                                                                                                                            3
           same year and won by Goodfellow et al. (2011). Results    between examples of different classes in general involves
           were presented at NIPS 2011’s Challenges in Learning Hier-going through a low density region, i.e., P(X|Y = i) for
           archical Models Workshop. In the related domain adaptationdifferent i tend to be well separated and not overlap much. For
           setup, the target remains the same but the input distributionexample, this is exploited in the Manifold Tangent Classiﬁer
           changes (Glorot et al., 2011b; Chen et al., 2012). In the discussed in Section 8.3. This hypothesis is consistent with the
           multi-task learning setup, representation learning has also beenidea that humans have named categories and classes because
           found advantageous Krizhevsky et al. (2012); Collobert et al.of such statistical structure (discovered by their brain and
           (2011), because of shared factors across tasks.           propagated by their culture), and machine learning tasks often
           3 WHATMAKESAREPRESENTATION GOOD?                          involves predicting such categorical variables.
           3.1   Priors for Representation Learning in AI            • Temporal and spatial coherence: consecutive (from a se-
                                                                     quence) or spatially nearby observations tend to be associated
           In Bengio and LeCun (2007), one of us introduced the      with the same value of relevant categorical concepts, or result
           notion of AI-tasks, which are challenging for current machinein a small move on the surface of the high-density manifold.
           learning algorithms, and involve complex but highly structuredMore generally, different factors change at different temporal
           dependencies. One reason why explicitly dealing with repre-and spatial scales, and many categorical concepts of interest
           sentations is interesting is because they can be convenient tochange slowly. When attempting to capture such categorical
           express many general priors about the world around us, i.e.,variables, this prior can be enforced by making the associated
           priors that are not task-speciﬁc but would be likely to be usefulrepresentations slowly changing, i.e., penalizing changes in
           for a learning machine to solve AI-tasks. Examples of suchvalues over time or space. This prior was introduced in Becker
           general-purpose priors are the following:                 and Hinton (1992) and is discussed in Section 11.3.
           • Smoothness: assumes the function to be learned f is s.t.• Sparsity: for any given observation x, only a small fraction
           x ≈ y generally implies f(x) ≈ f(y). This most basic priorof the possible factors are relevant. In terms of representation,
           is present in most machine learning, but is insufﬁcient to getthis could be represented by features that are often zero (as
           around the curse of dimensionality, see Section 3.2.      initially proposed by Olshausen and Field (1996)), or by the
           • Multiple explanatory factors: the data generating distribu-fact that most of the extracted features are insensitive to small
           tion is generated by different underlying factors, and for thevariations of x. This can be achieved with certain forms of
           most part what one learns about one factor generalizes in manypriors on latent variables (peaked at 0), or by using a non-
           conﬁgurations of the other factors. The objective to recoverlinearity whose value is often ﬂat at 0 (i.e., 0 and with a
           or at least disentangle these underlying factors of variation is0 derivative), or simply by penalizing the magnitude of the
           discussed in Section 3.5. This assumption is behind the idea ofJacobian matrix (of derivatives) of the function mapping input
           distributed representations, discussed in Section 3.3 below.to representation. This is discussed in Sections 6.1.1 and 7.2.
           • A hierarchical organization of explanatory factors: the • Simplicity of Factor Dependencies: in good high-level
           concepts that are useful for describing the world around usrepresentations, the factors are related to each other through
           can be deﬁned in terms of other concepts, in a hierarchy, withsimple, typically linear dependencies. This can be seen in
           more abstract concepts higher in the hierarchy, deﬁned in many laws of physics, and is assumed when plugging a linear
           terms of less abstract ones. This assumption is exploited withpredictor on top of a learned representation.
           deep representations, elaborated in Section 3.4 below.       We can view many of the above priors as ways to help the
           • Semi-supervised learning: with inputs X and target Y to learner discover and disentangle some of the underlying (and
           predict, a subset of the factors explaining X’s distributiona priori unknown) factors of variation that the data may reveal.
           explain much of Y, given X. Hence representations that areThis idea is pursued further in Sections 3.5 and 11.4.
           useful for P(X) tend to be useful when learning P(Y|X),   3.2   SmoothnessandtheCurseofDimensionality
           allowing sharing of statistical strength between the unsuper-For AI-tasks, such as vision and NLP, it seems hopeless to
           vised and supervised learning tasks, see Section 4.       rely only on simple parametric models (such as linear models)
           • Shared factors across tasks: with many Y ’s of interest orbecause they cannot capture enough of the complexity of in-
           many learning tasks in general, tasks (e.g., the correspondingterest unless provided with the appropriate feature space. Con-
           P(Y|X,task)) are explained by factors that are shared withversely, machine learning researchers have sought ﬂexibility in
           other tasks, allowing sharing of statistical strengths acrosslocal6 non-parametric learners such as kernel machines with
           tasks, as discussed in the previous section (Multi-Task anda ﬁxed generic local-response kernel (such as the Gaussian
           Transfer Learning, Domain Adaptation).                    kernel). Unfortunately, as argued at length by Bengio and
           • Manifolds: probability mass concentrates near regions thatMonperrus (2005); Bengio et al. (2006a); Bengio and LeCun
           have a much smaller dimensionality than the original space(2007); Bengio (2009); Bengio et al. (2010), most of these
           where the data lives. This is explicitly exploited in somealgorithms only exploit the principle of local generalization,
           of the auto-encoder algorithms and other manifold-inspiredi.e., the assumption that the target function (to be learned)
           algorithms described respectively in Sections 7.2 and 8.  is smooth enough, so they rely on examples to explicitly
           • Natural clustering: different values of categorical variablesmap out the wrinkles of the target function. Generalization
           such as object classes are associated with separate manifolds.
           More precisely, the local variations on the manifold tend to6. local in the sense that the value of the learned function at x depends
                                                                                           (t)
           preserve the value of a category, and a linear interpolationmostly on training examples x’s close to x
                                                                                                      4
         is mostly achieved by a form of local interpolation betweenBoltzmann Machine) can be re-used in many examples that are
         neighboring training examples. Although smoothness can benot simply near neighbors of each other, whereas with local
         a useful assumption, it is insufﬁcient to deal with the cursegeneralization, different regions in input space are basically
         of dimensionality, because the number of such wrinkles (upsassociated with their own private set of parameters, e.g., as
         and downs of the target function) may grow exponentiallyin decision trees, nearest-neighbors, Gaussian SVMs, etc. In
         with the number of relevant interacting factors, when the dataa distributed representation, an exponentially large number of
         are represented in raw input space. We advocate learningpossible subsets of features or hidden units can be activated
         algorithms that are ﬂexible and non-parametric7 but do notin response to a given input. In a single-layer model, each
         rely exclusively on the smoothness assumption. Instead, wefeature is typically associated with a preferred input direction,
         propose to incorporate generic priors such as those enumeratedcorresponding to a hyperplane in input space, and the code
         above into representation-learning algorithms. Smoothness-or representation associated with that input is precisely the
         based learners (such as kernel machines) and linear modelspattern of activation (which features respond to the input,
         can still be useful on top of such learned representations. Inand how much). This is in contrast with a non-distributed
         fact, the combination of learning a representation and kernelrepresentation such as the one learned by most clustering
         machine is equivalent to learning the kernel, i.e., the featurealgorithms, e.g., k-means, in which the representation of a
         space. Kernel machines are useful, but they depend on a priorgiven input vector is a one-hot code identifying which one of
         deﬁnition of a suitable similarity metric, or a feature spacea small number of cluster centroids best represents the input 10.
         in which naive similarity metrics sufﬁce. We would like to3.4Depthandabstraction
         use the data, along with very generic priors, to discover thoseDepth is a key aspect to representation learning strategies we
         features, or equivalently, a similarity function.consider in this paper. As we will discuss, deep architectures
         3.3  Distributed representations                are often challenging to train effectively and this has been
         Good representationsareexpressive,meaningthata  the subject of much recent research and progress. However,
         reasonably-sized learned representation can capture a hugedespite these challenges, they carry two signiﬁcant advantages
         number of possible input conﬁgurations. A simple countingthat motivate our long-term interest in discovering successful
         argument helps us to assess the expressiveness of a model pro-training strategies for deep architectures. These advantages
         ducing a representation: how many parameters does it requireare: (1) deep architectures promote the re-use of features, and
         compared to the number of input regions (or conﬁgurations) it(2) deep architectures can potentially lead to progressively
         can distinguish? Learners of one-hot representations, such asmore abstract features at higher layers of representations
         traditional clustering algorithms, Gaussian mixtures, nearest-(more removed from the data).
         neighbor algorithms, decision trees, or Gaussian SVMs all re-Feature re-use. The notion of re-use, which explains the
         quire O(N)parameters (and/or O(N) examples) to distinguishpower of distributed representations, is also at the heart of the
         O(N)inputregions. One could naively believe that one cannottheoretical advantages behind deep learning, i.e., constructing
         do better. However, RBMs, sparse coding, auto-encoders ormultiple levels of representation or learning a hierarchy of
                                                 k
         multi-layer neural networks can all represent up to O(2 ) inputfeatures. The depth of a circuit is the length of the longest
         regions using only O(N) parameters (with k the number ofpath from an input node of the circuit to an output node of
         non-zero elements in a sparse representation, and k = N inthe circuit. The crucial property of a deep circuit is that its
         non-sparse RBMs and other dense representations). These arenumber of paths, i.e., ways to re-use different parts, can grow
                    8       9
         all distributedor sparse representations. The generalizationexponentially with its depth. Formally, one can change the
         of clustering to distributed representations is multi-clustering,depth of a given circuit by changing the deﬁnition of what
         where either several clusterings take place in parallel or theeach node can compute, but only by a constant factor. The
         same clustering is applied on different parts of the input,typical computations we allow in each node include: weighted
         such as in the very popular hierarchical feature extraction forsum,product, artiﬁcial neuron model (such as a monotone non-
         object recognition based on a histogram of cluster categorieslinearity on top of an afﬁne transformation), computation of a
         detected in different patches of an image (Lazebnik et al.,kernel, or logic gates. Theoretical results clearly show families
         2006; Coates and Ng, 2011a). The exponential gain fromof functions where a deep representation can be exponentially
         distributed or sparse representations is discussed further in                             ˚
                                                         more efﬁcient than one that is insufﬁciently deep (Hastad,
         section 3.2 (and Figure 3.2) ofBengio (2009). It comes ˚
                                                         1986; Hastad and Goldmann, 1991; Bengio et al., 2006a;
         about because each parameter (e.g. the parameters of one ofBengio and LeCun, 2007; Bengio and Delalleau, 2011). If
         the units in a sparse code, or one of the units in a Restrictedthe same family of functions can be represented with fewer
           7. We understand non-parametric as including all learning algorithms10. As discussed in (Bengio, 2009), things are only slightly better when
         whose capacity can be increased appropriately as the amount of data and itsallowing continuous-valued membership values, e.g., in ordinary mixture
         complexity demands it, e.g. including mixture models and neural networksmodels (with separate parameters for each mixture component), but the
         where the number of parameters is a data-selected hyper-parameter.difference in representational power is still exponential (Montufar and Morton,
           8. Distributed representations: where k out of N representation elements2012). The situation may also seem better with a decision tree, where each
         or feature values can be independently varied, e.g., they are not mutuallygiven input is associated with a one-hot code over the tree leaves, which
         exclusive. Each concept is represented by having k features being turned ondeterministically selects associated ancestors (the path from root to node).
         or active, while each feature is involved in representing many concepts.Unfortunately, the number of different regions represented (equal to the
           9. Sparse representations: distributed representations where only a few ofnumber of leaves of the tree) still only grows linearly with the number of
         the elements can be varied at a time, i.e., k < N.parameters used to specify it (Bengio and Delalleau, 2011).
                                                                                   5
        parameters (or more precisely with a smaller VC-dimension),Further, as is often the case in the context of deep learning
        learning theory would suggest that it can be learned withmethods, the feature set being trained may be destined to
        fewer examples, yielding improvements in both computationalbe used in multiple tasks that may have distinct subsets of
        efﬁciency (less nodes to visit) and statistical efﬁciency (lessrelevant features. Considerations such as these lead us to the
        parameters to learn, and re-use of these parameters over manyconclusion that the most robust approach to feature learning
        different kinds of inputs).           is to disentangle as many factors as possible, discarding as
         Abstraction and invariance. Deep architectures can leadlittle information about the data as is practical. If some form
        to abstract representations because more abstract concepts canof dimensionality reduction is desirable, then we hypothesize
        often be constructed in terms of less abstract ones. In somethat the local directions of variation least represented in the
        cases, such as in the convolutional neural network (LeCuntraining data should be ﬁrst to be pruned out (as in PCA,
        et al., 1998b), we build this abstraction in explicitly via afor example, which does it globally instead of around each
        pooling mechanism (see section 11.2). More abstract conceptsexample).
        are generally invariant to most local changes of the input. That3.6Goodcriteria for learning representations?
        makestherepresentations that capture these concepts generally
        highly non-linear functions of the raw input. This is obviouslyOne of the challenges of representation learning that distin-
        true of categorical concepts, where more abstract representa-guishes it from other machine learning tasks such as classi-
        tions detect categories that cover more varied phenomena (e.g.ﬁcation is the difﬁculty in establishing a clear objective, or
        larger manifolds with more wrinkles) and thus they potentiallytarget for training. In the case of classiﬁcation, the objective
        have greater predictive power. Abstraction can also appear inis (at least conceptually) obvious, we want to minimize the
        high-level continuous-valued attributes that are only sensitivenumber of misclassiﬁcations on the training dataset. In the
        to some very speciﬁc types of changes in the input. Learningcase of representation learning, our objective is far-removed
        these sorts of invariant features has been a long-standing goalfrom the ultimate objective, which is typically learning a
        in pattern recognition.               classiﬁer or some other predictor. Our problem is reminiscent
                                              of the credit assignment problem encountered in reinforcement
        3.5Disentangling Factors of Variation learning. We have proposed that a good representation is one
        Beyondbeingdistributed and invariant, we would like our rep-that disentangles the underlying factors of variation, but how
        resentations to disentangle the factors of variation. Differentdo we translate that into appropriate training criteria? Is it even
        explanatory factors of the data tend to change independentlynecessary to do anything but maximize likelihood under a good
        of each other in the input distribution, and only a few at a timemodel or can we introduce priors such as those enumerated
        tend to change when one considers a sequence of consecutiveabove (possibly data-dependent ones) that help the representa-
        real-world inputs.                    tion better do this disentangling? This question remains clearly
         Complex data arise from the rich interaction of manyopen but is discussed in more detail in Sections 3.5 and 11.4.
        sources. These factors interact in a complex web that can4 BUILDING DEEP REPRESENTATIONS
        complicate AI-related tasks such as object classiﬁcation. For
        example, an image is composed of the interaction between oneIn 2006, a breakthrough in feature learning and deep learning
        or more light sources, the object shapes and the material prop-was initiated by Geoff Hinton and quickly followed up in
        erties of the various surfaces present in the image. Shadowsthe same year (Hinton et al., 2006; Bengio et al., 2007;
        from objects in the scene can fall on each other in complexRanzato et al., 2007), and soon after by Lee et al. (2008)
        patterns, creating the illusion of object boundaries where thereand many more later. It has been extensively reviewed and
        are none and dramatically effect the perceived object shape.discussed in Bengio (2009). A central idea, referred to as
        How can we cope with these complex interactions? How cangreedy layerwise unsupervised pre-training, was to learn a
        we disentangle the objects and their shadows? Ultimately,hierarchy of features one level at a time, using unsupervised
        we believe the approach we adopt for overcoming thesefeature learning to learn a new transformation at each level
        challenges must leverage the data itself, using vast quantitiesto be composed with the previously learned transformations;
        of unlabeled examples, to learn representations that separateessentially, each iteration of unsupervised feature learning adds
        the various explanatory sources. Doing so should give rise toone layer of weights to a deep neural network. Finally, the set
        a representation signiﬁcantly more robust to the complex andof layers could be combined to initialize a deep supervised pre-
        richly structured variations extant in natural data sources fordictor, such as a neural network classiﬁer, or a deep generative
        AI-related tasks.                     model, such as a Deep Boltzmann Machine (Salakhutdinov
         It is important to distinguish between the related but distinctand Hinton, 2009).
        goals of learning invariant features and learning to disentangleThis paper is mostly about feature learning algorithms
        explanatory factors. The central difference is the preservationthat can be used to form deep architectures. In particular, it
        of information. Invariant features, by deﬁnition, have reducedwas empirically observed that layerwise stacking of feature
        sensitivity in the direction of invariance. This is the goal ofextraction often yielded better representations, e.g., in terms
        building features that are insensitive to variation in the dataof classiﬁcation error (Larochelle et al., 2009; Erhan et al.,
        that are uninformative to the task at hand. Unfortunately, it2010b), quality of the samples generated by a probabilistic
        is often difﬁcult to determine a priori which set of featuresmodel (Salakhutdinov and Hinton, 2009) or in terms of the
        and variations will ultimately be relevant to the task at hand.invariance properties of the learned features (Goodfellow
                                                                                   6
        et al., 2009). Whereas this section focuses on the idea ofon this avenue clearly needs to be done, and it was probably
        stacking single-layer models, Section 10 follows up with aavoided by fear of the challenges in training deep feedfor-
        discussion on joint training of all the layers.ward networks, discussed in the Section 10 along with very
         After greedy layerwise unsuperivsed pre-training, the re-encouraging recent results.
        sulting deep features can be used either as input to a standardYet another recently proposed approach to training deep
        supervised machine learning predictor (such as an SVM) or asarchitectures (Ngiam et al., 2011) is to consider the iterative
        initialization for a deep supervised neural network (e.g., by ap-construction of a free energy function (i.e., with no explicit
        pending a logistic regression layer or purely supervised layerslatent variables, except possibly for a top-level layer of hidden
        of a multi-layer neural network). The layerwise procedure canunits) for a deep architecture as the composition of transforma-
        also be applied in a purely supervised setting, called the greedytions associated with lower layers, followed by top-level hid-
        layerwise supervised pre-training (Bengio et al., 2007). Forden units. The question is then how to train a model deﬁned by
        example, after the ﬁrst one-hidden-layer MLP is trained, itsan arbitrary parametrized (free) energy function. Ngiam et al.
        output layer is discarded and another one-hidden-layer MLP(2011) have used Hybrid Monte Carlo (Neal, 1993), but other
        can be stacked on top of it, etc. Although results reportedoptions include contrastive divergence (Hinton, 1999; Hinton
        in Bengio et al. (2007) were not as good as for unsupervised¨          ¨
                                              et al., 2006), score matching (Hyvarinen, 2005; Hyvarinen,
        pre-training, they were nonetheless better than without pre-2008), denoising score matching (Kingma and LeCun, 2010;
        training at all. Alternatively, the outputs of the previous layer¨
                                              Vincent, 2011), ratio-matching (Hyvarinen, 2007) and noise-
        can be fed as extra inputs for the next layer (in addition to thecontrastive estimation (Gutmann and Hyvarinen, 2010).
        raw input), as successfully done in Yu et al. (2010). Another5 SINGLE-LAYER LEARNING MODULES
        variant (Seide et al., 2011b) pre-trains in a supervised way all
        the previously added layers at each step of the iteration, andWithin the community of researchers interested in representa-
        in their experiments this discriminant variant yielded bettertion learning, there has developed two broad parallel lines of
        results than unsupervised pre-training.inquiry: one rooted in probabilistic graphical models and one
         Whereas combining single layers into a supervised modelrooted in neural networks. Fundamentally, the difference be-
        is straightforward, it is less clear how layers pre-trained bytween these two paradigms is whether the layered architecture
        unsupervised learning should be combined to form a betterof a deep learning model is to be interpreted as describing a
        unsupervised model. We cover here some of the approachesprobabilistic graphical model or as describing a computation
        to do so, but no clear winner emerges and much work has tograph. In short, are hidden units considered latent random
        be done to validate existing proposals or improve them.variables or as computational nodes?
         The ﬁrst proposal was to stack pre-trained RBMs into aTo date, the dichotomy between these two paradigms has
        Deep Belief Network (Hinton et al., 2006) or DBN, whereremained in the background, perhaps because they appear to
        the top layer is interpreted as an RBM and the lower layershave more characteristics in common than separating them.
        as a directed sigmoid belief network. However, it is not clearWe suggest that this is likely a function of the fact that
        how to approximate maximum likelihood training to furthermuch recent progress in both of these areas has focused
        optimize this generative model. One option is the wake-sleepon single-layer greedy learning modules and the similarities
        algorithm (Hinton et al., 2006) but more work should be donebetween the types of single-layer models that have been
        to assess the efﬁciency of this procedure in terms of improvingexplored: mainly, the restricted Boltzmann machine (RBM)
        the generative model.                 on the probabilistic side, and the auto-encoder variants on the
         The second approach that has been put forward is toneural network side. Indeed, as shown by one of us (Vincent,
        combinetheRBMparametersintoaDeepBoltzmannMachine2011) and others (Swersky et al., 2011), in the case of
        (DBM), by basically halving the RBM weights to obtainthe restricted Boltzmann machine, training the model via
                                                                               ¨
        the DBM weights (Salakhutdinov and Hinton, 2009). Thean inductive principle known as score matching (Hyvarinen,
        DBMcanthenbetrainedbyapproximatemaximumlikelihood2005) (to be discussed in sec. 6.4.3) is essentially identical
        as discussed in more details later (Section 10.2). This jointto applying a regularized reconstruction objective to an auto-
        training has brought substantial improvements, both in termsencoder. Another strong link between pairs of models on
        of likelihood and in terms of classiﬁcation performance ofboth sides of this divide is when the computational graph for
        the resulting deep feature learner (Salakhutdinov and Hinton,computing representation in the neural network model corre-
        2009).                                sponds exactly to the computational graph that corresponds to
         Another early approach was to stack RBMs or auto-inference in the probabilistic model, and this happens to also
        encoders into a deep auto-encoder (Hinton and Salakhutdi-correspond to the structure of graphical model itself (e.g., as
        nov, 2006). If we have a series of encoder-decoder pairsin the RBM).
        (f(i)(·),g(i)(·)), then the overall encoder is the composition ofTheconnectionbetweenthesetwoparadigmsbecomesmore
        the encoders, f(N)(...f(2)(f(1)(·))), and the overall decodertenuous when we consider deeper models where, in the case
        is its “transpose” (often with transposed weight matrices asof a probabilistic model, exact inference typically becomes
        well), g(1)(g(2)(...f(N)(·))). The deep auto-encoder (or itsintractable. In the case of deep models, the computational
        regularized version, as discussed in Section 7.2) can thengraph diverges from the structure of the model. For example,
        be jointly trained, with all the parameters optimized within the case of a deep Boltzmann machine, unrolling variational
        respect to a global reconstruction error criterion. More work(approximate) inference into a computational graph results in
                                                                                  7
        a recurrent graph structure. We have performed preliminarywhile in the simplest case (complete, noise-free) ICA yields
        exploration (Savard, 2011) of deterministic variants of deeplinear features, in the more general case it can be equated
        auto-encoders whose computational graph is similar to that ofwith a linear generative model with non-Gaussian independent
        a deep Boltzmann machine (in fact very close to the mean-latent variables, similar to sparse coding (section 6.1.1), which
        ﬁeld variational approximations associated with the Boltzmannresult in non-linear features. Therefore, ICA and its variants
        machine), and that is one interesting intermediate point to ex-   ¨
                                              like Independent and Topographic ICA (Hyvarinen et al.,
        plore (between the deterministic approaches and the graphical2001b) can and have been used to build deep networks (Le
        model approaches).                    et al., 2010, 2011c): see section 11.2. The notion of obtaining
         In the next few sections we will review the major de-independent components also appears similar to our stated
        velopments in single-layer training modules used to supportgoal of disentangling underlying explanatory factors through
        feature learning and particularly deep learning. We divide thesedeep networks. However, for complex real-world distributions,
        sections between (Section 6) the probabilistic models, withit is doubtful that the relationship between truly independent
        inference and training schemes that directly parametrize theunderlying factors and the observed high-dimensional data can
        generative – or decoding – pathway and (Section 7) the typ-be adequately characterized by a linear transformation.
        ically neural network-based models that directly parametrize6 PROBABILISTIC MODELS
        the encoding pathway. Interestingly, some models, like Pre-
        dictive Sparse Decomposition (PSD) (Kavukcuoglu et al.,From the probabilistic modeling perspective, the question of
        2008) inherit both properties, and will also be discussed (Sec-feature learning can be interpreted as an attempt to recover
        tion 7.2.4). We then present a different view of representationa parsimonious set of latent random variables that describe
        learning, based on the associated geometry and the manifolda distribution over the observed data. We can express as
        assumption, in Section 8.             p(x,h) a probabilistic model over the joint space of the latent
         First, let us consider an unsupervised single-layer represen-variables, h, and observed data or visible variables x. Feature
        tation learning algorithm spaning all three views: probabilistic,values are conceived as the result of an inference process to
        auto-encoder, and manifold learning.  determine the probability distribution of the latent variables
         Principal Components Analysis        given the data, i.e. p(h | x), often referred to as the posterior
         Wewilluseprobablytheoldest feature extraction algorithm,probability. Learning is conceived in term of estimating a set
        principal components analysis (PCA), to illustrate the proba-of model parameters that (locally) maximizes the regularized
        bilistic, auto-encoder and manifold views of representation-likelihood of the training data. The probabilistic graphical
        learning. PCA learns a linear transformation h = f(x) =model formalism gives us two possible modeling paradigms
        WTx+bof input x ∈ Rdx, where the columns of d × din which we can consider the question of inferring latent
                                        x  h  variables, directed and undirected graphical models, which
        matrix W form an orthogonal basis for the dh orthogonal
        directions of greatest variance in the training data. The resultdiffer in their parametrization of the joint distribution p(x,h),
        is dfeatures (the components of representation h) thatyielding major impact on the nature and computational costs
          h                                   of both inference and learning.
        are decorrelated. The three interpretations of PCA are the
        following: a) it is related to probabilistic models (Section 6)6.1Directed Graphical Models
        such as probabilistic PCA, factor analysis and the traditionalDirected latent factor models separately parametrize the con-
        multivariate Gaussian distribution (the leading eigenvectors ofditional likelihood p(x | h) and the prior p(h) to construct
        the covariance matrix are the principal components); b) thethe joint distribution, p(x,h) = p(x | h)p(h). Examples of
        representation it learns is essentially the same as that learnedthis decomposition include: Principal Components Analysis
        by a basic linear auto-encoder (Section 7.2); and c) it can be(PCA) (Roweis, 1997; Tipping and Bishop, 1999), sparse
        viewed as a simple linear form of linear manifold learningcoding (Olshausen and Field, 1996), sigmoid belief net-
        (Section 8), i.e., characterizing a lower-dimensional regionworks (Neal, 1992) and the newly introduced spike-and-slab
        in input space near which the data density is peaked. Thus,sparse coding model (Goodfellow et al., 2011).
        PCA may be in the back of the reader’s mind as a common
        thread relating these various viewpoints. Unfortunately the6.1.1Explaining Away
        expressive power of linear features is very limited: they cannotDirected models often leads to one important property: ex-
        be stacked to form deeper, more abstract representations sinceplaining away, i.e., a priori independent causes of an event can
        the composition of linear operations yields another linearbecome non-independent given the observation of the event.
        operation. Here, we focus on recent algorithms that haveLatent factor models can generally be interpreted as latent
        been developed to extract non-linear features, which can because models, where the h activations cause the observed x.
        stacked in the construction of deep networks, although someThis renders the a priori independent h to be non-independent.
        authors simply insert a non-linearity between learned single-As a consequence, recovering the posterior distribution of h,
        layer linear projections (Le et al., 2011c; Chen et al., 2012).p(h | x) (which we use as a basis for feature representation),
         Another rich family of feature extraction techniques that thisis often computationally challenging and can be entirely
        review does not cover in any detail due to space constraints isintractable, especially when h is discrete.
        Independent Component Analysis or ICA (Jutten and Herault,A classic example that illustrates the phenomenon is to
        1991; Bell and Sejnowski, 1997). Instead, we refer the readerimagine you are on vacation away from home and you receive
            ¨              ¨
        to Hyvarinen et al. (2001a); Hyvarinen et al. (2009). Note that,a phone call from the security system company, telling you that
                                                                                                                                                      8
              the alarm has been activated. You begin worrying your home               Theprobabilistic interpretation of sparse coding differs from
              has been burglarized, but then you hear on the radio that a           that of PCA, in that instead of a Gaussian prior on the latent
              minor earthquake has been reported in the area of your home.          random variable h, we use a sparsity inducing Laplace prior
              If you happen to know from prior experience that earthquakes          (corresponding to an L1 penalty):
                                                                                                                   d
                                                                                                                    h
              sometimes cause your home alarm system to activate, then                                p(h)    = Yλexp(−λ|h|)
                                                                                                                      2            i
              suddenly you relax, conﬁdent that your home has very likely                                           i                2
                                                                                                   p(x | h)   = N(x;Wh+µ ,σ I).                      (4)
              not been burglarized.                                                                                              x   x
                The example illustrates how the alarm activation rendered           In the case of sparse coding, because we will seek a sparse
              two otherwise entirely independent causes, burglarized and            representation (i.e., one with many features set to exactly zero),
              earthquake, to become dependent – in this case, the depen-            we will be interested in recovering the MAP (maximum a
              dency is one of mutual exclusivity. Since both burglarized                                          ∗
                                                                                    posteriori value of h: i.e. h = argmax p(h | x) rather than
              and earthquake are very rare events and both can cause                                                            h
              alarm activation, the observation of one explains away the            its expected value E[h|x]. Under this interpretation, dictionary
                                                                                    learning proceeds as maximizing the likelihood of the data
              other. Despite the computational obstacles we face when                                                 ∗             Q       (t)    ∗(t)
                                                                                    given these MAP values of h : argmax                p(x     | h   )
              attempting to recover the posterior over h, explaining away                                                         W t
                                                                                    subject to the norm constraint on W. Note that this pa-
              promises to provide a parsimonious p(h | x), which can                rameter learning scheme, subject to the MAP values of
              be an extremely useful characteristic of a feature encoding           the latent h, is not standard practice in the probabilistic
              scheme. If one thinks of a representation as being composed           graphical model literature. Typically the likelihood of the
              of various feature detectors and estimated attributes of the          data p(x) = P p(x | h)p(h) is maximized directly. In the
              observed input, it is useful to allow the different features to                        h
                                                                                    presence of latent variables, expectation maximization is em-
              compete and collaborate with each other to explain the input.         ployed where the parameters are optimized with respect to
              This is naturally achieved with directed graphical models, but        the marginal likelihood, i.e., summing or integrating the joint
              can also be achieved with undirected models (see Section 6.2)         log-likelihood over the all values of the latent variables under
              such as Boltzmann machines if there are lateral connections           their posterior P(h | x), rather than considering only the
              between the corresponding units or corresponding interaction          single MAP value of h. The theoretical properties of this
              terms in the energy function that deﬁnes the probability model.       form of parameter learning are not yet well understood but
                Probabilistic Interpretation of PCA. PCA can be given               seem to work well in practice (e.g. k-Means vs Gaussian
              a natural probabilistic interpretation (Roweis, 1997; Tipping         mixture models and Viterbi training for HMMs). Note also
              and Bishop, 1999) as factor analysis:                                 that the interpretation of sparse coding as a MAP estimation
                                                      2                             can be questioned (Gribonval, 2011), because even though the
                                p(h)   = N(h;0,σhI)           2                     interpretation of the L1 penalty as a log-prior is a possible
                             p(x | h)  = N(x;Wh+µx,σxI),                      (1)   interpretation, there can be other Bayesian interpretations
                              dx           dh                                       compatible with the training criterion.
              where x ∈ R , h ∈ R , N(v;µ,Σ) is the multivariate                       Sparse coding is an excellent example of the power of
              normal density of v with mean µ and covariance Σ, and
              columns of W span the same space as leading d            principal    explaining away. Even with a very overcomplete dictionary11,
                                                                     h
                                                                                                                                                      ∗
              components, but are not constrained to be orthonormal.                the MAP inference process used in sparse coding to ﬁnd h
                Sparse Coding. Like PCA, sparse coding has both a proba-            can pick out the most appropriate bases and zero the others,
              bilistic and non-probabilistic interpretation. Sparse coding also     despite them having a high degree of correlation with the input.
              relates a latent representation h (either a vector of random          This property arises naturally in directed graphical models
              variables or a feature vector, depending on the interpretation)       such as sparse coding and is entirely owing to the explaining
              to the data x through a linear mapping W, which we refer              away effect. It is not seen in commonly used undirected prob-
              to as the dictionary. The difference between sparse coding            abilistic models such as the RBM, nor is it seen in parametric
              and PCA is that sparse coding includes a penalty to ensure a          feature encoding methods such as auto-encoders. The trade-
              sparse activation of h is used to encode each input x. From           off is that, compared to methods such as RBMs and auto-
              a non-probabilistic perspective, sparse coding can be seen as         encoders, inference in sparse coding involves an extra inner-
                                                                                                                     ∗
              recovering the code or feature vector associated with a new           loop of optimization to ﬁnd h with a corresponding increase
              input x via:                                                          in the computational cost of feature extraction. Compared
                           ∗                              2                         to auto-encoders and RBMs, the code in sparse coding is a
                         h =f(x)=argminkx−Whk +λkhk ,                         (2)
                                                          2         1
                                          h                                         free variable for each example, and in that sense the implicit
              Learning the dictionary W can be accomplished by optimizing           encoder is non-parametric.
              the following training criterion with respect to W:                      One might expect that the parsimony of the sparse coding
                                       X (t)           ∗(t) 2
                                 JSC =     kx    −Wh k,                       (3)   representation and its explaining away effect would be advan-
                                                            2
                                         t                                          tageous and indeed it seems to be the case. Coates and Ng
              where x(t) is the t-th example and h∗(t) is the corresponding         (2011a) demonstrated on the CIFAR-10 object classiﬁcation
              sparse code determined by Eq. 2. W is usually constrained to          task (Krizhevsky and Hinton, 2009) with a patch-base feature
              have unit-norm columns (because one can arbitrarily exchange          extraction pipeline, that in the regime with few (< 1000)
                                                       (t)
              scaling of column i with scaling of h       , such a constraint is
                                                       i
              necessary for the L1 penalty to have any effect).                       11. Overcomplete: with more dimensions of h than dimensions of x.
                                                                                                                                                     9
             labeled training examples per class, the sparse coding repre-         These stochastic units can be divided into two groups: (1) the
                                                                                                            dx
             sentation signiﬁcantly outperformed other highly competitive          visible units x ∈ {0,1}      that represent the data, and (2) the
                                                                                                                      dh
             encoding schemes. Possibly because of these properties, and           hidden or latent units h ∈ {0,1}      that mediate dependencies
             because of the very computationally efﬁcient algorithms that          betweenthevisible units through their mutual interactions. The
             have been proposed for it (in comparison with the general             pattern of interaction is speciﬁed through the energy function:
             case of inference in the presence of explaining away), sparse             BM            1 T        1 T         T         T      T
             coding enjoys considerable popularity as a feature learning and         Eθ   (x,h) = −2x Ux− 2h Vh−x Wh−b x−d h, (7)
             encoding paradigm. There are numerous examples of its suc-            where θ = {U,V,W,b,d} are the model parameters which
             cessful application as a feature representation scheme, includ-       respectively encode the visible-to-visible interactions, the
             ing natural image modeling (Raina et al., 2007; Kavukcuoglu           hidden-to-hidden interactions, the visible-to-hidden interac-
             et al., 2008; Coates and Ng, 2011a; Yu et al., 2011), audio           tions,  the visible self-connections, and the hidden self-
             classiﬁcation (Grosse et al., 2007), NLP (Bagnell and Bradley,        connections (called biases). To avoid over-parametrization, the
             2009), as well as being a very successful model of the early          diagonals of U and V are set to zero.
             visual cortex (Olshausen and Field, 1996). Sparsity criteria can         The Boltzmann machine energy function speciﬁes the prob-
             also be generalized successfully to yield groups of features that     ability distribution over [x,h], via the Boltzmann distribution,
             prefer to all be zero, but if one or a few of them are active then    Eq. 6, with the partition function Zθ given by:
             the penalty for activating others in the group is small. Different             x =1    xd =1h =1      hd =1
                                                                                             1        x     1        h                      
             group sparsity patterns can incorporate different forms of prior         Zθ = X ··· X X ··· X exp −EBM(x,h;θ) . (8)
             knowledge (Kavukcuoglu et al., 2009; Jenatton et al., 2009;                                                          θ
                                                                                            x1=0    xdx=0h1=0      hd =0
             Bach et al., 2011; Gregor et al., 2011).                                                                h
                                                                                   This joint probability distribution gives rise to the set of
                Spike-and-SlabSparseCoding.Spike-and-slabsparsecod-                conditional distributions of the form:
             ing (S3C) is one example of a promising variation on sparse                                       X              X              
             coding for feature learning (Goodfellow et al., 2012). The               P(h | x,h ) = sigmoid         W x +         V ′h ′ +d  (9)
                                                                                          i      \i                     ji j        ii  i     i
             S3C model possesses a set of latent binary spike variables                                         j             i′6=i            
             together with a a set of latent real-valued slab variables. The                                     X             X
                                                                                     P(x | h,x ) = sigmoid          W x +         U ′x ′ +b .
             activation of the spike variables dictates the sparsity pattern.            j      \j                      ji j         jj  j     j
             S3Chas been applied to the CIFAR-10 and CIFAR-100 object                                              i           j′6=j
             classiﬁcation tasks (Krizhevsky and Hinton, 2009), and shows                                                                         (10)
             the same pattern as sparse coding of superior performance in          In general, inference in the Boltzmann machine is intractable.
                                                                                   For example, computing the conditional probability of h given
             the regime of relatively few (< 1000) labeled examples per                                                                       i
                                                                                   the visibles, P(h | x), requires marginalizing over the rest of
             class (Goodfellow et al., 2012). In fact, in both the CIFAR-                            i
                                                                                                                                         dh−1
             100 dataset (with 500 examples per class) and the CIFAR-              the hiddens, which implies evaluating a sum with 2          terms:
             10 dataset (when the number of examples is reduced to a                                h =1     h   =1h    =1    hd =1
                                                                                                     1        i−1    i+1        h
             similar range), the S3C representation actually outperforms                P(hi | x) = X ··· X          X ··· X P(h|x)               (11)
             sparse coding representations. This advantage was revealed                             h1=0     hi−1=0hi+1=0     hd =0
                                                                                                                                h
             clearly with S3C winning the NIPS’2011 Transfer Learning              However with some judicious choices in the pattern of inter-
             Challenge (Goodfellow et al., 2011).                                  actions between the visible and hidden units, more tractable
             6.2    Undirected Graphical Models                                    subsets of the model family are possible, as we discuss next.
                                                                                      Restricted Boltzmann Machines (RBMs). The RBM
             Undirected graphical models, also called Markov random                is  likely the most popular subclass of Boltzmann ma-
             ﬁelds (MRFs), parametrize the joint p(x,h) through a product          chine (Smolensky, 1986). It is deﬁned by restricting the
             of unnormalized non-negative clique potentials:                       interactions in the Boltzmann energy function, in Eq. 7, to
                        p(x,h) = 1 Yψ (x)Yη (h)Yν (x,h)                      (5)   only those between h and x, i.e. ERBM is EBM with U = 0
                                   Z       i         j        k                                                          θ          θ
                                    θ   i        j        k                        and V = 0. As such, the RBM can be said to form a bipartite
             where ψ (x), η (h) and ν (x,h) are the clique potentials de-
                       i      j          k                                         graph with the visibles and the hiddens forming two layers
             scribing the interactions between the visible elements, between       of vertices in the graph (and no connection between units of
             the hidden variables, and those interaction between the visible       the same layer). With this restriction, the RBM possesses the
             and hidden variables respectively. The partition function Zθ          useful property that the conditional distribution over the hidden
             ensures that the distribution is normalized. Within the context       units factorizes given the visibles:
             of unsupervised feature learning, we generally see a particular                    P(h|x)=               Q P(h |x)
                                                                                                                        i     i
             form of Markov random ﬁeld called a Boltzmann distribution                    P(h =1|x)=          sigmoidP W x +d .                (12)
             with clique potentials constrained to be positive:                                i                           j   ji j     i
                               p(x,h) = 1 exp(−E (x,h)),                     (6)
                                          Z          θ                              Likewise, the conditional distribution over the visible units
                                           θ                                       given the hiddens also factorizes:
             where E (x,h) is the energy function and contains the inter-
                      θ                                                                                       Y
             actions described by the MRF clique potentials and θ are the                          P(x|h)=        P(x | h)
             model parameters that characterize these interactions.                                                   j                 !
                                                                                                               j         X
                TheBoltzmannmachinewasoriginallydeﬁnedasanetwork                             P(x =1|h)=sigmoid               W h +b .             (13)
                                                                                                 j                             ji i    j
             of symmetrically-coupled binary random variables or units.                                                    i
                                                                                                       10
          This makes inferences readily tractable in RBMs. For exam-objective of better modeling non-diagonal conditional covari-
          ple, the RBM feature representation is taken to be the set ofances. (Ranzato and Hinton, 2010) introduced the mean and
          posterior marginals P(hi | x), which, given the conditionalcovariance RBM (mcRBM). Like the GRBM, the mcRBM is a
          independence described in Eq. 12, are immediately available.2-layer Boltzmann machine that explicitly models the visible
          Note that this is in stark contrast to the situation with popularunits as Gaussian distributed quantities. However unlike the
          directed graphical models for unsupervised feature extraction,GRBM, the mcRBM uses its hidden layer to independently
          where computing the posterior probability is intractable.parametrize both the mean and covariance of the data through
           Importantly, the tractability of the RBM does not extendtwo sets of hidden units. The mcRBM is a combination of the
          to its partition function, which still involves summing ancovariance RBM (cRBM) (Ranzato et al., 2010a), that models
          exponential number of terms. It does imply however that wethe conditional covariance, with the GRBM that captures the
                                      dx  dh              conditional mean. While the GRBM has shown considerable
          can limit the number of terms to min{2, 2}. Usually this ispotential as the basis of a highly successful phoneme recogni-
          still an unmanageable number of terms and therefore we must
          resort to approximate methods to deal with its estimation.tion system (Dahl et al., 2010), it seems that due to difﬁculties
           It is difﬁcult to overstate the impact the RBM has had toin training the mcRBM, the model has been largely superseded
          the ﬁelds of unsupervised feature learning and deep learning.by the mPoT model. The mPoT model (mean-product of
          It has been used in a truly impressive variety of applica-Student’s T-distributions model)(Ranzato et al., 2010b) is
          tions, including fMRI image classiﬁcation (Schmah et al.,a combination of the GRBM and the product of Student’s T-
          2009), motion and spatial transformations (Taylor and Hinton,distributions model (Welling et al., 2003). It is an energy-based
          2009; Memisevic and Hinton, 2010), collaborative ﬁlteringmodel where the conditional distribution over the visible units
          (Salakhutdinov et al., 2007) and natural image modelingconditioned on the hidden variables is a multivariate Gaussian
          (Ranzato and Hinton, 2010; Courville et al., 2011b).(non-diagonal covariance) and the complementary conditional
                                                          distribution over the hidden variables given the visibles are a
          6.3 Generalizations of the RBM to Real-valued dataset of independent Gamma distributions.
          Important progress has been made in the last few years inThe PoT model has recently been generalized to the mPoT
          deﬁning generalizations of the RBM that better capture real-model (Ranzato et al., 2010b) to include nonzero Gaussian
          valued data, in particular real-valued image data, by bettermeansbytheaddition of GRBM-like hidden units, similarly to
          modeling the conditional covariance of the input pixels. ThehowthemcRBMgeneralizesthe cRBM.ThemPoTmodelhas
          standard RBM, as discussed above, is deﬁned with both binarybeen used to synthesize large-scale natural images (Ranzato
          visible variables v ∈ {0,1} and binary latent variables h ∈et al., 2010b) that show large-scale features and shadowing
          {0,1}. The tractability of inference and learning in the RBMstructure. It has been used to model natural textures (Kivinen
          has inspired many authors to extend it, via modiﬁcations of itsand Williams, 2012) in a tiled-convolution conﬁguration (see
          energy function, to model other kinds of data distributions. Insection 11.2).
          particular, there has been multiple attempts to develop RBM-Another recently introduced RBM-based model with the
                                              d
          type models of real-valued data, where x ∈ R x. The mostobjective of having the hidden units encode both the mean
          straightforward approach to modeling real-valued observationsand covariance information is the spike-and-slab Restricted
          within the RBM framework is the so-called Gaussian RBMBoltzmann Machine (ssRBM) (Courville et al., 2011a,b).
          (GRBM) where the only change in the RBM energy functionThe ssRBM is deﬁned as having both a real-valued “slab”
          is to the visible units biases, by adding a bias term that isvariable and a binary “spike” variable associated with each
          quadratic in the visible units x. While it probably remainsunit in the hidden layer. The ssRBM has been demonstrated
          the most popular way to model real-valued data within theas a feature learning and extraction scheme in the context
          RBMframework, Ranzato and Hinton (2010) suggest that theof CIFAR-10 object classiﬁcation (Krizhevsky and Hinton,
          GRBMhas proved to be a somewhat unsatisfactory model of2009) from natural images and has performed well in the
          natural images. The trained features typically do not representrole (Courville et al., 2011a,b). When trained convolutionally
          sharp edges that occur at object boundaries and lead to latent(see Section 11.2) on full CIFAR-10 natural images, the model
          representations that are not particularly useful features fordemonstrated the ability to generate natural image samples
          classiﬁcation tasks. Ranzato and Hinton (2010) argue thatthat seem to capture the broad statistical structure of natural
          the failure of the GRBM to adequately capture the statisticalimages better than previous parametric generative models, as
          structure of natural images stems from the exclusive use of theillustrated with the samples of Figure 2.
          model capacity to capture the conditional mean at the expenseThe mcRBM, mPoT and ssRBM each set out to model
          of the conditional covariance. Natural images, they argue, arereal-valued data such that the hidden units encode not only
          chieﬂy characterized by the covariance of the pixel values,the conditional mean of the data but also its conditional
          not by their absolute values. This point is supported by thecovariance. Other than differences in the training schemes, the
          common use of preprocessing methods that standardize themost signiﬁcant difference between these models is how they
          global scaling of the pixel values across images in a datasetencode their conditional covariance. While the mcRBM and
          or across the pixel values within each image.   the mPoT use the activation of the hidden units to enforce con-
           These kinds of concerns about the ability of the GRBMstraints on the covariance of x, the ssRBM uses the hidden unit
          to model natural image data has lead to the development ofto pinch the precision matrix along the direction speciﬁed by
          alternative RBM-based models that each attempt to take on thisthe corresponding weight vector. These two ways of modeling
                                                                                                                                                     11
                                                                                    Boltzmann machines, is given by:
                                                                                           T                       T                             
                                                                                       ∂ X           (t)          X                ∂   BM (t)
                                                                                              logp(x    )  = −        E      (t)      E   (x   , h)
                                                                                      ∂θ                                p(h|x  )  ∂θ θ
                                                                                         i t=1                     t=1              i
                                                                                                                   T                         
                                                                                                                +XE             ∂ EBM(x,h) , (15)
                                                                                                                        p(x,h) ∂θ θ
                                                                                                                   t=1            i
                                                                                                                                            (t)    (t)
                                                                                    where we have the expectations with respect to p(h          | x  )
                                                                                    in the “clamped” condition (also called the positive phase),
                                                                                    and over the full joint p(x,h) in the “unclamped” condition
                                                                                    (also called the negative phase). Intuitively, the gradient acts
                                                                                    to locally move the model distribution (the negative phase
                                                                                    distribution) toward the data distribution (positive phase dis-
                                                                                                                                        (t)
                                                                                    tribution), by pushing down the energy of (h,x        ) pairs (for
                                                                                    h ∼ P(h|x(t))) while pushing up the energy of (h,x) pairs
                                                                                    (for (h,x) ∼ P(h,x)) until the two forces are in equilibrium,
                                                                                    at which point the sufﬁcient statistics (gradient of the energy
                                                                                    function) have equal expectations with x sampled from the
                                                                                    training distribution or with x sampled from the model.
                                                                                      The RBM conditional independence properties imply that
              Fig. 2.   (Top) Samples from convolutionally trained µ-ssRBM          the expectation in the positive phase of Eq. 15 is tractable.
              fromCourvilleetal.(2011b).(Bottom)ImagesinCIFAR-10train-              The negative phase term – arising from the partition func-
              ing set closest (L2 distance with contrast normalized training        tion’s contribution to the log-likelihood gradient – is more
              images) to corresponding model samples on top. The model              problematic because the computation of the expectation over
              does not appear to be overﬁtting particular training examples.        the joint is not tractable. The various ways of dealing with the
              conditional covariance diverge when the dimensionality of the         partition function’s contribution to the gradient have brought
              hidden layer is signiﬁcantly different from that of the input.        about a number of different training algorithms, many trying
              In the overcomplete setting, sparse activation with the ssRBM         to approximate the log-likelihood gradient.
              parametrization permits variance only in the select directions          To approximate the expectation of the joint distribution in
              of the sparsely activated hidden units. This is a property the        the negative phase contribution to the gradient, it is natural to
              ssRBM shares with sparse coding models (Olshausen and                 again consider exploiting the conditional independence of the
              Field, 1996; Grosse et al., 2007). On the other hand, in              RBMinorder to specify a Monte Carlo approximation of the
                                                                                    expectation over the joint:
              the case of the mPoT or mcRBM, an overcomplete set of                                                   L
                                                                                                 ∂   RBM            1 X ∂ RBM (l) ˜(l)
              constraints on the covariance implies that capturing arbitrary          Ep(x,h)      E     (x,h) ≈              E      (x˜ , h  ),  (16)
                                                                                                ∂θ θ                L      ∂θ θ
              covariance along a particular direction of the input requires                       i                    l=1   i
                                                                                                         (l) ˜(l)
              decreasing potentially all constraints with positive projection       with the samples (x˜    , h  ) drawn by a block Gibbs MCMC
              in that direction. This perspective would suggest that the mPoT       (Markov chain Monte Carlo) sampling procedure:
              andmcRBMdonotappeartobewellsuitedtoprovideasparse                                           (l)             ˜(l−1)
                                                                                                         x˜    ∼ P(x|h          )
                                                                                                         ˜(l)              (l)
              representation in the overcomplete setting.                                                h     ∼ P(h|x˜ ).
              6.4   RBMparameterestimation                                          Naively, for each gradient update step, one would start a
              Many of the RBM training methods we discuss here are ap-              Gibbs sampling chain, wait until the chain converges to the
              plicable to more general undirected graphical models, but are         equilibrium distribution and then draw a sufﬁcient number of
              particularly practical in the RBM setting. Freund and Haussler        samples to approximate the expected gradient with respect
              (1994) proposed a learning algorithm for harmoniums (RBMs)            to the model (joint) distribution in Eq. 16. Then restart the
              based on projection pursuit. Contrastive Divergence (Hinton,          process for the next step of approximate gradient ascent on
              1999; Hinton et al., 2006) has been used most often to train          the log-likelihood. This procedure has the obvious ﬂaw that
              RBMs, and many recent papers use Stochastic Maximum                   waiting for the Gibbs chain to “burn-in” and reach equilibrium
              Likelihood (Younes, 1999; Tieleman, 2008).                            anew for each gradient update cannot form the basis of a prac-
                                                                                    tical training algorithm. Contrastive Divergence (Hinton, 1999;
                As discussed in Sec. 6.1, in training probabilistic models          Hinton et al., 2006), Stochastic Maximum Likelihood (Younes,
              parameters are typically adapted in order to maximize the like-       1999; Tieleman, 2008) and fast-weights persistent contrastive
              lihood of the training data (or equivalently the log-likelihood,      divergence or FPCD (Tieleman and Hinton, 2009) are all ways
              or its penalized version, which adds a regularization term).          to avoid or reduce the need for burn-in.
              With T training examples, the log likelihood is given by:             6.4.1   Contrastive Divergence
                     T                   T
                    X          (t)      X X (t)
                        logP(x ;θ) =        log           P(x ,h;θ).        (14)    Contrastive divergence (CD) estimation (Hinton, 1999; Hinton
                    t=1                 t=1            d
                                                h∈{0,1} h                           et al., 2006) estimates the negative phase expectation (Eq. 15)
              Gradient-based optimization requires its gradient, which for          with a very short Gibbs chain (often just one step) initialized
                                                                                                                                                   12
             at the training data used in the positive phase. This reduces         less likely to be resampled and therefore making it more
             the variance of the gradient estimator and still moves in a           likely that the samples will move somewhere else (typically
             direction that pulls the negative chain samples towards the as-       going near another mode). Rather than drawing samples from
             sociated positive chain samples. Much has been written about          the distribution of the current model (with parameters θ),
             the properties and alternative interpretations of CD and its          FPCDexaggerates this effect by drawing samples from a local
                                                                       ˜                                                          ∗
             similarity to auto-encoder training, e.g. Carreira-Perpinan and       perturbation of the model with parameters θ and an update
                                                                                                                            T             !
             Hinton (2005); Yuille (2005); Bengio and Delalleau (2009);                ∗                       ∗    ∗ ∂     X          (t)
                                                                                      θ    =(1−η)θ        +ηθ +ǫ                logp(x   )  ,    (17)
                                                                                       t+1            t+1      t      ∂θ
             Sutskever and Tieleman (2010).                                                                             i   t=1
                                                                                            ∗
             6.4.2    Stochastic Maximum Likelihood                                where ǫ     is the relatively large fast-weight learning rate
                                                                                     ∗
             The Stochastic Maximum Likelihood (SML) algorithm (also               (ǫ  > ǫ) and 0 < η < 1 (but near 1) is a forgetting factor
             known as persistent contrastive divergence or PCD) (Younes,           that keeps the perturbed model close to the current model.
             1999; Tieleman, 2008) is an alternative way to sidestep an            Unlike tempering, FPCD does not converge to the model
                                                                                                          ∗
             extended burn-in of the negative phase Gibbs sampler. At each         distribution as ǫ and ǫ go to 0, and further work is necessary
             gradient update, rather than initializing the Gibbs chain at the      to characterize the nature of its approximation to the model
             positive phase sample as in CD, SML initializes the chain at          distribution. Nevertheless, FPCD is a popular and apparently
             the last state of the chain used for the previous update. In          effective means of drawing approximate samples from the
             other words, SML uses a continually running Gibbs chain (or           model distribution that faithfully represent its diversity, at the
             often a number of Gibbs chains run in parallel) from which            price of sometimes generating spurious samples in between
             samples are drawn to estimate the negative phase expectation.         two modes (because the fast weights roughly correspond to a
             Despite the model parameters changing between updates, these          smoothed view of the current model’s energy function). It has
             changes should be small enough that only a few steps of Gibbs         been applied in a variety of applications (Tieleman and Hinton,
             (in practice, often one step is used) are required to maintain        2009; Ranzato et al., 2011; Kivinen and Williams, 2012) and
             samples from the equilibrium distribution of the Gibbs chain,         it has been transformed into a sampling algorithm (Breuleux
             i.e. the model distribution.                                          et al., 2011) that also shares this fast mixing property with
                Atroublesome aspect of SML is that it relies on the Gibbs          herding (Welling, 2009), for the same reason, i.e., introducing
             chain to mix well (especially between modes) for learning to          negative correlations between consecutive samples of the
             succeed. Typically, as learning progresses and the weights of         chain in order to promote faster mixing.
             the RBM grow, the ergodicity of the Gibbs sample begins to            6.4.3   Pseudolikelihood, Ratio-matching and More
             break down12. If the learning rate ǫ associated with gradient         While CD, SML and FPCD are by far the most popular meth-
                                                    ∂logp (x)
             ascent θ ← θ + ǫgˆ (with E[gˆ] ≈             θ   ) is not reduced     ods for training RBMs and RBM-based models, all of these
                                                       ∂θ
             to compensate, then the Gibbs sampler will diverge from the           methods are perhaps most naturally described as offering dif-
             model distribution and learning will fail. Desjardins et al.          ferent approximations to maximum likelihood training. There
             (2010); Cho et al. (2010); Salakhutdinov (2010b,a) have all           exist other inductive principles that are alternatives to maxi-
             considered various forms of tempered transitions to address           mumlikelihood that can also be used to train RBMs. In partic-
             the failure of Gibbs chain mixing, and convincing solutions           ular, these include pseudo-likelihood (Besag, 1975) and ratio-
             have not yet been clearly demonstrated. A recently introduced                        ¨
                                                                                   matching (Hyvarinen, 2007). Both of these inductive principles
             promising avenue relies on depth itself, showing that mixing          attempt to avoid explicitly dealing with the partition function,
             between modes is much easier on deeper layers (Bengio et al.,         and their asymptotic efﬁciency has been analyzed (Marlin and
             2013) (Sec.9.4).                                                      de Freitas, 2011). Pseudo-likelihood seeks to maximize the
                Tieleman and Hinton (2009) have proposed quite a dif-              product of all one-dimensional conditional distributions of the
             ferent approach to addressing potential mixing problems of            form P(x |x ), while ratio-matching can be interpreted as
             SML with their fast-weights persistent contrastive divergence                   d   \d
                                                                                                                           ¨
                                                                                   an extension of score matching (Hyvarinen, 2005) to discrete
             (FPCD), and it has also been exploited to train Deep Boltz-           data types. Both methods amount to weighted differences of
             mann Machines (Salakhutdinov, 2010a) and construct a pure             the gradient of the RBM free energy13 evaluated at a data
             sampling algorithm for RBMs (Breuleux et al., 2011). FPCD             point and at neighboring points. One potential drawback of
             builds on the surprising but robust tendency of Gibbs chains          these methods is that depending on the parametrization of
             to mix better during SML learning than when the model                 the energy function, their computational requirements may
             parameters are ﬁxed. The phenomenon is rooted in the form of          scale up to O(nd) worse than CD, SML, FPCD, or denoising
             the likelihood gradient itself (Eq. 15). The samples drawn from       score matching (Kingma and LeCun, 2010; Vincent, 2011),
             the SML Gibbs chain are used in the negative phase of the             discussed below. Marlin et al. (2010) empirically compared all
             gradient, which implies that the learning update will slightly        of these methods (except denoising score matching) on a range
             increase the energy (decrease the probability) of those samples,      of classiﬁcation, reconstruction and density modeling tasks and
             making the region in the neighborhood of those samples                found that, in general, SML provided the best combination of
               12. When weights become large, the estimated distribution is more peaky,overall performance and computational tractability. However,
             and the chain takes very long time to mix, to move from mode to mode, soin a later study, the same authors (Swersky et al., 2011)
             that practically the gradient estimator can be very poor. This is a serious
             chicken-and-egg problem because if sampling is not effective, nor is the13. The free energy F(x;θ) is the energy associated with the data marginal
             training procedure, which may seem to stall, and yields even larger weights.probability, F(x;θ) = −logP(x)−logZ and is tractable for the RBM.
                                                                                                                        θ
                                                                                                                             13
           found denoising score matching to be a competitive inductive7.1  Auto-Encoders
           principle both in terms of classiﬁcation performance (with In the auto-encoder framework (LeCun, 1987; Bourlard and
           respect to SML) and in terms of computational efﬁciency (withKamp, 1988; Hinton and Zemel, 1994), one starts by ex-
           respect to analytically obtained score matching). Denoisingplicitly deﬁning a feature-extracting function in a speciﬁc
           score matching is a special case of the denoising auto-encoderparametrized closed form. This function, that we will denote
           training criterion (Section 7.2.2) when the reconstruction errorfθ, is called the encoder and will allow the straightforward
           residual equals a gradient, i.e., the score function associatedand efﬁcient computation of a feature vector h = fθ(x)
           with an energy function, as shown in (Vincent, 2011).      from an input x. For each example x(t) from a data set
              In the spirit of the Boltzmann machine gradient (Eq. 15){x(1),...,x(T)}, we deﬁne
                                                                                             (t)      (t)
           several approaches have been proposed to train energy-based                      h   =fθ(x )                    (18)
           models. One is noise-contrastive estimation (Gutmann and Hy-where h(t) is the feature-vector or representation or code com-
           varinen, 2010), in which the training criterion is transformedputed from x(t). Another closed form parametrized function
           into a probabilistic classiﬁcation problem: distinguish betweengθ, called the decoder, maps from feature space back into
           (positive) training examples and (negative) noise samples  input space, producing a reconstruction r = g (h). Whereas
                                                                                                                 θ
           generated by a broad distribution (such as the Gaussian).  probabilistic models are deﬁned from an explicit probability
           Another family of approaches, more in the spirit of Contrastivefunction and are trained to maximize (often approximately) the
           Divergence, relies on distinguishing positive examples (of data likelihood (or a proxy), auto-encoders are parametrized
           the training distribution) and negative examples obtained bythrough their encoder and decoder and are trained using a
           perturbations of the positive examples (Collobert and Weston,different training principle. The set of parameters θ of the
           2008; Bordes et al., 2012; Weston et al., 2010).           encoder and decoder are learned simultaneously on the task
                                                                      of reconstructing as well as possible the original input, i.e.
            7 DIRECTLY LEARNING A PARAMETRIC MAP attempting to incur the lowest possible reconstruction error
            FROM INPUT TO REPRESENTATION                              L(x,r) – a measure of the discrepancy between x and its
                                                                      reconstruction r – over training examples. Good generalization
           Within the framework of probabilistic models adopted in    means low reconstruction error at test examples, while having
           Section 6, the learned representation is always associated withhigh reconstruction error for most other x conﬁgurations. To
           latent variables, speciﬁcally with their posterior distributioncapture the structure of the data-generating distribution, it
           given an observed input x. Unfortunately, this posterior dis-is therefore important that something in the training crite-
           tribution tends to become very complicated and intractable ifrion or the parametrization prevents the auto-encoder from
           the model has more than a couple of interconnected layers, learning the identity function, which has zero reconstruction
           whether in the directed or undirected graphical model frame-error everywhere. This is achieved through various means in
           works. It then becomes necessary to resort to sampling or  the different forms of auto-encoders, as described below in
           approximate inference techniques, and to pay the associatedmore detail, and we call these regularized auto-encoders. A
           computational and approximation error price. If the true pos-particular form of regularization consists in constraining the
           terior has a large number of modes that matter then currentcode to have a low dimension, and this is what the classical
           inference techniques may face an unsurmountable challenge orauto-encoder or PCA do.
           endure a potentially serious approximation. This is in additionIn summary, basic auto-encoder training consists in ﬁnding
           to the difﬁculties raised by the intractable partition function ina value of parameter vector θ minimizing reconstruction error
                                                                                              X (t)           (t)
           undirected graphical models. Moreover a posterior distribution         JAE(θ)  =      L(x ,gθ(fθ(x )))          (19)
           over latent variables is not yet a simple usable feature vector                     t
                                                                              (t)
           that can for example be fed to a classiﬁer. So actual featurewhere x  is a training example. This minimization is usually
           values are typically derived from that distribution, taking thecarried out by stochastic gradient descent as in the training
           latent variable’s expectation (as is typically done with RBMs),of Multi-Layer-Perceptrons (MLPs). Since auto-encoders were
           their marginal probability, or ﬁnding their most likely valueprimarily developed as MLPs predicting their input, the most
           (as in sparse coding). If we are to extract stable deterministiccommonly used forms for the encoder and decoder are afﬁne
           numerical feature values in the end anyway, an alternative mappings, optionally followed by a non-linearity:
                                                                                       f (x)  = s (b+Wx)                   (20)
           (apparently) non-probabilistic feature learning paradigm that                θ          f       ′
                                                                                       g (h)  = s (d+W h)                  (21)
           focuses on carrying out this part of the computation, very efﬁ-              θ          g
                                                                      where s    and s   are the encoder and decoder activation
           ciently, is that of auto-encoders and other directly parametrized  f       g
           feature or representation functions. The commonality betweenfunctions (typically the element-wise sigmoid or hyperbolic
           these methods is that they learn a direct encoding, i.e., atangent non-linearity, or the identity function if staying linear).
           parametric map from inputs to their representation.        The set of parameters of such a model is θ = {W,b,W′,d}
              Regularized auto-encoders, discussed next, also involve where b and d are called encoder and decoder bias vectors,
           learning a decoding function that maps back from represen- and W and W′ are the encoder and decoder weight matrices.
                                                                         Thechoice of s and L depends largely on the input domain
           tation to input space. Sections 8.1 and 11.3 discuss direct                g
           encoding methods that do not require a decoder, such as semi-range and nature, and are usually chosen so that L returns a
           supervised embedding (Weston et al., 2008) and slow featurenegative log-likelihood for the observed value of x. A natural
           analysis (Wiskott and Sejnowski, 2002).                    choice for an unbounded domain is a linear decoder with a
                                                                                                                                                    14
             squared reconstruction error, i.e. s (a) = a and L(x,r) =             the volume of hidden conﬁgurations easily accessible by the
                                                    g
                      2
             kx − rk . If inputs are bounded between 0 and 1 however,              learner) is that it acts in spirit like the partition function of
             ensuring a similarly-bounded reconstruction can be achieved           RBMs, by making sure that only few input conﬁgurations can
             byusing sg = sigmoid. In addition if the inputs are of a binary       have a low reconstruction error.
             nature, a binary cross-entropy loss14 is sometimes used.                 Alternatively, one can view the objective of the regulariza-
                If both encoder and decoder use a sigmoid non-linearity,           tion applied to an auto-encoder as making the representation
             then f (x) and g (h) have the exact same form as the                  as “constant” (insensitive) as possible with respect to changes
                     θ            θ
             conditionals P(h | v) and P(v | h) of binary RBMs (see                in input. This view immediately justiﬁes two variants of
             Section 6.2). This similarity motivated an initial study (Bengio      regularized auto-encoders described below: contractive auto-
             et al., 2007) of the possibility of replacing RBMs with auto-         encoders reduce the number of effective degrees of freedom of
             encoders as the basic pre-training strategy for building deep         the representation (around each point) by making the encoder
             networks, as well as the comparative analysis of auto-encoder         contractive, i.e., making the derivative of the encoder small
             reconstruction error gradient and contrastive divergence up-          (thus making the hidden units saturate), while the denoising
             dates (Bengio and Delalleau, 2009).                                   auto-encoder makes the whole mapping “robust”, i.e., insen-
                One notable difference in the parametrization is that RBMs         sitive to small random perturbations, or contractive, making
             use a single weight matrix, which follows naturally from their        sure that the reconstruction cannot stay good when moving in
             energy function, whereas the auto-encoder framework allows            most directions around a training example.
             for a different matrix in the encoder and decoder. In practice        7.2.1    Sparse Auto-Encoders
             however, weight-tying in which one deﬁnes W′ = WT may                 The earliest uses of single-layer auto-encoders for building
             be (and is most often) used, rendering the parametrizations           deep architectures by stacking them (Bengio et al., 2007)
             identical. The usual training procedures however differ greatly       considered the idea of tying the encoder weights and decoder
             between the two approaches. A practical advantage of training         weights to restrict capacity as well as the idea of introducing a
             auto-encoder variants is that they deﬁne a simple tractable           form of sparsity regularization (Ranzato et al., 2007). Sparsity
             optimization objective that can be used to monitor progress.          in the representation can be achieved by penalizing the hidden
                In the case of a linear auto-encoder (linear encoder and           unit biases (making these additive offset parameters more
             decoder) with squared reconstruction error, minimizing Eq. 19         negative) (Ranzato et al., 2007; Lee et al., 2008; Goodfellow
             learns the same subspace15 as PCA. This is also true when             et al., 2009; Larochelle and Bengio, 2008) or by directly
             using a sigmoid nonlinearity in the encoder (Bourlard and             penalizing the output of the hidden unit activations (making
             Kamp, 1988), but not if the weights W and W′ are tied                 them closer to their saturating value at 0) (Ranzato et al.,
             (W′ = WT), because W cannot be forced into being small                2008; Le et al., 2011a; Zou et al., 2011). Penalizing the bias
             and W′ large to achieve a linear encoder.                             runs the danger that the weights could compensate for the
                Similarly, Le et al. (2011b) recently showed that adding a         bias, which could hurt numerical optimization. When directly
                                                PP               (t)
             regularization term of the form            s (W x ) to a linear
                                                  t    j 3    j                    penalizing the hidden unit outputs, several variants can be
             auto-encoder with tied weights, where s is a nonlinear convex
                                                        3                          found in the literature, but a clear comparative analysis is
             function, yields an efﬁcient algorithm for learning linear ICA.       still lacking. Although the L1 penalty (i.e., simply the sum
                                                                                   of output elements h in the case of sigmoid non-linearity)
             7.2    Regularized Auto-Encoders                                                             j
             Like PCA, auto-encoders were originally seen as a dimen-              would seem the most natural (because of its use in sparse cod-
             sionality reduction technique and thus used a bottleneck, i.e.        ing), it is used in few papers involving sparse auto-encoders.
             d    < d . On the other hand, successful uses of sparse               A close cousin of the L1 penalty is the Student-t penalty
               h        x                                                          (log(1+h2)), originally proposed for sparse coding (Olshausen
             coding and RBM approaches tend to favour overcomplete                           j
             representations, i.e. d     > d . This can allow the auto-            and Field, 1996). Several papers penalize the average output
                                      h        x                                   ¯
             encoder to simply duplicate the input in the features, with           hj (e.g. over a minibatch), and instead of pushing it to 0,
             perfect reconstruction without having extracted more mean-            encourage it to approach a ﬁxed target, either through a mean-
                                                                                   square error penalty, or maybe more sensibly (because h
             ingful features. Recent research has demonstrated very suc-                                                                            j
             cessful alternative ways, called regulrized auto-encoders, to         behaves like a probability), a Kullback-Liebler divergence
             “constrain” the representation, even when it is overcomplete.         with respect to the binomial distribution with probability ρ:
                                                                                           ¯                     ¯
                                                                                   −ρlogh −(1−ρ)log(1−h )+constant, e.g., with ρ = 0.05.
             The effect of a bottleneck or of this regularization is that the               j                     j
             auto-encoder cannot reconstruct well everything, it is trained        7.2.2    Denoising Auto-Encoders
             to reconstruct well the training examples and generalization          Vincent et al. (2008, 2010) proposed altering the training ob-
             means that reconstruction error is also small on test examples.       jective in Eq. 19 from mere reconstruction to that of denoising
             An interesting justiﬁcation (Ranzato et al., 2008) for the            an artiﬁcially corrupted input, i.e. learning to reconstruct the
             sparsity penalty (or any penalty that restricts in a soft way         clean input from a corrupted version. Learning the identity
                               P                                                   is no longer enough: the learner must capture the structure
                                 d
                14. L(x,r) = −    x x log(r )+(1−x )log(1−r )                      of the input distribution in order to optimally undo the effect
                                 i=1 i      i         i          i
                15. Contrary to traditional PCA loading factors, but similarly to theof the corruption process, with the reconstruction essentially
             parameters learned by probabilistic PCA, the weight vectors learned by abeing a nearby but higher density point than the corrupted
             linear auto-encoder are not constrained to form an orthonormal basis, nor to
             have a meaningful ordering. They will however span the same subspace. input. Figure 3 illustrates that the Denoising Auto-Encoder
                                                                                                                                                   15
             (DAE) is learning a reconstruction function that corresponds          7.2.3   Contractive Auto-Encoders
             to a vector ﬁeld pointing towards high-density regions (the           Contractive Auto-Encoders (CAE), proposed by Rifai et al.
             manifold where examples concentrate).                                 (2011a), follow up on Denoising Auto-Encoders (DAE) and
                                                   Corrupted input                 share a similar motivation of learning robust representations.
                                                                                   CAEs achieve this by adding an analytic contractive penalty
                     prior:&examples&concentrate&                                  to Eq. 19: the Frobenius norm of the encoder’s Jacobian,
                     near&a&lower&dimensional&                                     and results in penalizing the sensitivity of learned features to
                     “manifold”&&
                                                                                                                                ∂f
                                                                                   inﬁnitesimal input variations. Let J(x) =      θ (x) the Jacobian
                                                                                                                                ∂x
                               Corrupted input                                     matrix of the encoder at x. The CAE’s training objective is
                                                                                                     X                                    2
                                                                                                             (t)        (t)           (t) 
                                                                                         J       =       L(x ,g (f (x )))+λ J(x )                (23)
                               original                                                    CAE        t           θ  θ                    F
                                input                                              where λ is a hyper-parameter controlling the strength of the
                                                                                   regularization. For an afﬁne sigmoid encoder, the contractive
             Fig. 3.     When data concentrate near a lower-dimensional            penalty term is easy to compute:
                                                                                               J (x)   = f (x) (1−f (x) )W
             manifold, the corruption vector is typically almost orthogonal to                  j            θ   j      θ    j   j
             the manifold, and the reconstruction function learns to denoise,                      2       X                        2      2
                                                                                           kJ(x)k      =       (f (x) (1 −f (x) )) kW k          (24)
                                                                                                   F             θ   j       θ   j       j
             map from low-probability conﬁgurations (corrupted inputs) to                                    j
             high-probability ones (original inputs), creating a vector ﬁeld       There are at least three notable differences with DAEs, which
             aligned with the score (derivative of the estimated density).
             .                                                                     maybepartly responsible for the better performance that CAE
                Formally, the objective optimized by a DAE is:                     features seem to empirically demonstrate: (a) the sensitivity of
                                   X             h                  i                                        16
                                                      (t)                          the features is penalized    rather than that of the reconstruc-
                        JDAE   =       Eq(x˜|x(t))L(x ,gθ(fθ(x˜)))         (22)    tion; (b) the penalty is analytic rather than stochastic: an efﬁ-
                                     t                                             ciently computable expression replaces what might otherwise
             where E       (t) [·] averages over corrupted examples x˜ drawn
                      q(x˜|x )                                                     require d  corrupted samples to size up (i.e. the sensitivity in
             from corruption process q(x˜|x(t)). In practice this is optimized              x
             by stochastic gradient descent, where the stochastic gradient is      dx directions); (c) a hyper-parameter λ allows a ﬁne control of
                                                                                   the trade-off between reconstruction and robustness (while the
             estimated by drawing one or a few corrupted versions of x(t)          two are mingled in a DAE). Note however that there is a tight
             each time x(t) is considered. Corruptions considered in Vin-          connection between the DAE and the CAE: as shown in (Alain
             cent et al. (2010) include additive isotropic Gaussian noise,         and Bengio, 2012) a DAE with small corruption noise can be
             salt and pepper noise for gray-scale images, and masking              seen (through a Taylor expansion) as a type of contractive
             noise (salt or pepper only), e.g., setting some randomly chosen       auto-encoder where the contractive penalty is on the whole
             inputs to 0 (independently per example). Masking noise has            reconstruction function rather than just on the encoder17.
             been used in most of the simulations. Qualitatively better              A potential disadvantage of the CAE’s analytic penalty is
             features are reported with denoising, resulting in improved           that it amounts to only encouraging robustness to inﬁnitesimal
             classiﬁcation, and DAE features performed similarly or better         input variations. This is remedied in Rifai et al. (2011b) with
             than RBM features. Chen et al. (2012) show that a simpler             the CAE+H, that penalizes all higher order derivatives, in an
             alternative with a closed form solution can be obtained when          efﬁcient stochastic manner, by adding a term that encourages
             restricting to a linear auto-encoder and have successfully            J(x) and J(x+ǫ) to be close:
                                                                                                        X                               
             applied it to domain adaptation.                                                                                              2
                                                                                                                (t)     (t)          (t) 
                                                                                           J       =        L(x ,g (x ))+λ J(x )
                                                                                            CAE+H                   θ                   
                Vincent (2011) relates DAEs to energy-based probabilistic                                t                               F
                                                                                                                                  2
             models: DAEs basically learn in r(x˜) − x˜ a vector pointing                               +γEǫ kJ(x)−J(x+ǫ)k                       (25)
                                                                                                                                  F
             in the direction of the estimated score ∂logp(x˜) (Figure 3). In
                                                          ∂x˜                      where ǫ ∼ N(0,σ2I), and γ is the associated regularization
             the special case of linear reconstruction and squared error,          strength hyper-parameter.
             Vincent (2011) shows that training an afﬁne-sigmoid-afﬁne               The DAE and CAE have been successfully used to win
             DAE amounts to learning an energy-based model, whose                  the ﬁnal phase of the Unsupervised and Transfer Learning
             energy function is very close to that of a GRBM. Training uses        Challenge (Mesnil et al., 2011). The representation learned
             a regularized variant of the score matching parameter estima-         by the CAE tends to be saturated rather than sparse, i.e.,
                                    ¨                   ¨
             tion technique (Hyvarinen, 2005; Hyvarinen, 2008; Kingma              most hidden units are near the extremes of their range (e.g. 0
             and LeCun, 2010) termed denoising score matching (Vincent,            or 1), and their derivative ∂hi(x) is near 0. The non-saturated
             2011). Swersky (2010) had shown that training GRBMs with                                            ∂x
             score matching is equivalent to training a regular auto-encoder       units are few and sensitive to the inputs, with their associated
             with an additional regularization term, while, following up on        ﬁlters (weight vectors) together forming a basis explaining
             the theoretical results in Vincent (2011), Swersky et al. (2011)      the local changes around x, as discussed in Section 8.2.
             showed the practical advantage of denoising to implement              Another way to get saturated (nearly binary) units is semantic
             score matching efﬁciently. Finally Alain and Bengio (2012)            hashing (Salakhutdinov and Hinton, 2007).
             generalize Vincent (2011) and prove that DAEs of arbitrary              16. i.e., the robustness of the representation is encouraged.
                                                                                     17. but note that in the CAE, the decoder weights are tied to the encoder
             parametrization with small Gaussian corruption noise are              weights, to avoid degenerate solutions, and this should also make the decoder
             general estimators of the score.                                      contractive.
                                                                                                                                                      16
              7.2.4   Predictive Sparse Decomposition                               dimensionality dM, embedded in high dimensional input space
              Sparse coding (Olshausen and Field, 1996) may be viewed as a          Rdx. This prior seems particularly well suited for AI tasks
              kind of auto-encoder that uses a linear decoder with a squared        such as those involving images, sounds or text, for which
              reconstruction error, but whose non-parametric encoder fθ             mostuniformlysampledinputconﬁgurations are unlike natural
              performs the comparatively non-trivial and relatively costly          stimuli. As soon as there is a notion of “representation”
              iterative minimization of Eq. 2. A practically successful variant     then one can think of a manifold by considering the vari-
              of sparse coding and auto-encoders, named Predictive Sparse           ations in input space, which are captured by or reﬂected
              Decomposition or PSD (Kavukcuoglu et al., 2008) replaces              (by corresponding changes) in the learned representation.
              that costly and highly non-linear encoding step by a fast             To ﬁrst approximation, some directions are well preserved
              non-iterative approximation during recognition (computing the         (the tangent directions of the manifold) while others aren’t
              learned features). PSD has been applied to object recognition         (directions orthogonal to the manifolds). With this perspec-
              in images and video (Kavukcuoglu et al., 2009, 2010; Jarrett          tive, the primary unsupervised learning task is then seen as
              et al., 2009), but also to audio (Henaff et al., 2011), mostly        modeling the structure of the data-supporting manifold18. The
              within the framework of multi-stage convolutional deep archi-         associated representation being learned can be associated with
              tectures (Section 11.2). The main idea can be summarized              an intrinsic coordinate system on the embedded manifold. The
              by the following equation for the training criterion, which           archetypal manifold modeling algorithm is, not surprisingly,
              is simultaneously optimized with respect to hidden codes              also the archetypal low dimensional representation learning
                                 (t)                                                algorithm: Principal Component Analysis, which models a
              (representation) h     and with respect to parameters (W,α):
                      X (t)              (t)      (t) 2     (t)       (t)  2        linear manifold. It was initially devised with the objective
               J =        λkh k +kx −Wh k +kh −f (x )k (26)
                 PSD              1                  2            α        2        of ﬁnding the closest linear manifold to a cloud of data
                        t
              where x(t) is the input vector for example t, h(t) is the             points. The principal components, i.e. the representation f (x)
              optimized hidden code for that example, and f (·) is the                                                                             θ
                                                                     α              that PCA yields for an input point x, uniquely locates its
              encoding function, the simplest variant being                         projection on that manifold: it corresponds to intrinsic co-
                                     (t)                T (t)
                                fα(x )=tanh(b+W x )                          (27)   ordinates on the manifold. Data manifold for complex real
              where encoding weights are the transpose of decoding                  world domains are however expected to be strongly non-
              weights. Many variants have been proposed, including the              linear. Their modeling is sometimes approached as patchworks
              use of a shrinkage operation instead of the hyperbolic tan-           of locally linear tangent spaces (Vincent and Bengio, 2003;
              gent (Kavukcuoglu et al., 2010). Note how the L1 penalty on           Brand, 2003). The large majority of algorithms built on
              htendstomakethemsparse,andhowthisisthesamecriterion                   this geometric perspective adopt a non-parametric approach,
                                                                                                                                                 ¨
              as sparse coding with dictionary learning (Eq. 3) except for the      based on a training set nearest neighbor graph (Scholkopf
              additional constraint that one should be able to approximate          et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000;
              the sparse codes h with a parametrized encoder fα(x). One can         Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes,
              thus view PSD as an approximation to sparse coding, where we          2003; Weinberger and Saul, 2004; Hinton and Roweis, 2003;
              obtain a fast approximate encoder. Once PSD is trained, object        van der Maaten and Hinton, 2008). In these non-parametric
              representations f (x) are used to feed a classiﬁer. They are          approaches, each high-dimensional training point has its own
                                α                                                   set of free low-dimensional embedding coordinates, which are
              computed quickly and can be further ﬁne-tuned: the encoder
              can be viewed as one stage or one layer of a trainable multi-         optimized so that certain properties of the neighborhood graph
              stage system such as a feedforward neural network.                    computed in original high dimensional input space are best
                PSD can also be seen as a kind of auto-encoder where                preserved. These methods however do not directly learn a
                                                                                    parametrized feature extraction function f (x) applicable to
              the codes h are given some freedom that can help to further                                                          θ
                                                                                                     19
              improve reconstruction. One can also view the encoding                new test points , which seriously limits their use as feature
              penalty added on top of sparse coding as a kind of regularizer        extractors, except in a transductive setting. Comparatively few
              that forces the sparse codes to be nearly computable by a             non-linear manifold learning methods have been proposed,
              smooth and efﬁcient encoder. This is in contrast with the codes       that learn a parametric map that can directly compute a
              obtained by complete optimization of the sparse coding crite-         representation for new points; we will focus on these.
              rion, which are highly non-smooth or even non-differentiable,         8.1   Learning a parametric mapping based on a
              a problem that motivated other approaches to smooth the               neighborhoodgraph
              inferred codes of sparse coding (Bagnell and Bradley, 2009),          Some of the above non-parametric manifold learning al-
              so a sparse coding stage could be jointly optimized along with        gorithms can be modiﬁed to learn a parametric mapping
              following stages of a deep architecture.                              f , i.e., applicable to new points: instead of having free
              8 REPRESENTATION LEARNING AS MANI-                                     θ
              FOLD LEARNING                                                           18. Actually, data points need not strictly lie on the “manifold”, but the
              Another important perspective on representation learning is           probability density is expected to fall off sharply as one moves away from it,
                                                                                    and it may actually be constituted of several possibly disconnected manifolds
              based on the geometric notion of manifold. Its premise is             with different intrinsic dimensionality.
              the manifold hypothesis, according to which real-world data             19. For several of these techniques, representations for new points can
                                                                                                               ¨
              presented in high dimensional spaces are expected to con-             be computed using the Nystrom approximation as has been proposed as
                                                                                    an extension in (Bengio et al., 2004), but this remains cumbersome and
              centrate in the vicinity of a manifold M of much lower                computationally expensive.
                                                                                                                                                                                                          17
                  low-dimensional embedding coordinate “parameters” for each                                         The Local Coordinate Coding (LCC) algorithm (Yu et al.,
                  training point, these coordinates are obtained through an                                       2009) is very similar to sparse coding, but is explicitly derived
                  explicitly parametrized function, as with the parametric vari-                                  from a manifold perspective. Using the same notation as that
                  ant (van der Maaten, 2009) of t-SNE (van der Maaten and                                         of sparse coding in Eq. 2, LCC replaces regularization term
                  Hinton, 2008).                                                                                  kh(t)k = P |h(t)| yielding objective
                                                                                                                           1         j    j                                                             !
                      Instead, Semi-Supervised Embedding (Weston et al., 2008)                                                X  (t)                   (t) 2        X (t)                     (t) 1+p
                  learns a direct encoding while taking into account the manifold                                   JLCC =            kx      −Wh k2+λ                   |hj |kW:,j −x           k
                  hypothesis through a neighborhood graph. A parametrized                                                       t                                     j
                                                                                                                                                                                                       (28)
                  neural network architecture simultaneously learns a manifold                                    This is identical to sparse coding when p = −1, but with
                                                                                                                                                                                                  (t)
                  embedding and a classiﬁer. The training criterion encourages                                    larger p it encourages the active anchor points for x                                (i.e.
                                                                                                                                                                                                 (t)
                  training set neigbhors to have similar representations.                                         the codebook vectors W:,j with non-negligible |h                                  |  that
                                                                                                                                                                                                 j
                      The reduced and tightly controlled number of free param-                                    are combined to reconstruct x(t)) to be not too far from
                  eters in such parametric methods, compared to their pure                                        x(t), hence the local aspect of the algorithm. An important
                  non-parametric counterparts, forces models to generalize the                                    theoretical contribution of Yu et al. (2009) is to show that
                  manifold shape non-locally (Bengio et al., 2006b), which can                                    that any Lipschitz-smooth function φ : M → R deﬁned on a
                                                                                                                                                                                       d
                  translate into better features and ﬁnal performance (van der                                    smooth nonlinear manifold M embedded in R x can be well
                  Maaten and Hinton, 2008). However, basing the modeling of                                       approximated by a globally linear function with respect to the
                  manifolds on training set neighborhood relationships might                                      resulting coding scheme (i.e. linear in h), where the accuracy
                  be risky statistically in high dimensional spaces (sparsely                                     of the approximation and required number d of anchor points
                                                                                                                                                                                  h
                  populated due to the curse of dimensionality) as e.g. most                                      depend on d             rather than d . This result has been further
                  Euclidean nearest neighbors risk having too little in common                                                       M                        x
                                                                                                                  extended with the use of local tangent directions (Yu and
                  semantically. The nearest neighbor graph is simply not enough                                   Zhang, 2010), as well as to multiple layers (Lin et al., 2010).
                  densely populated to map out satisfyingly the wrinkles of                                          Let us now consider the efﬁcient non-iterative “feed-
                  the target manifold (Bengio and Monperrus, 2005; Bengio                                         forward” encoders fθ, used by PSD and the auto-encoders
                  et al., 2006b; Bengio and LeCun, 2007). It can also become                                      reviewed in Section 7.2, that are in the form of Eq. 20 or
                  problematic computationally to consider all pairs of data                                       27.The computed representation for x will be only signiﬁ-
                  points20, which scales quadratically with training set size.                                    cantly sensitive to input space directions associated with non-
                                                                                                                  saturated hidden units (see e.g. Eq. 24 for the Jacobian of a
                  8.2      Learning to represent non-linear manifolds                                             sigmoid layer). These directions to which the representation
                  Can we learn a manifold without requiring nearest neighbor                                      is signiﬁcantly sensitive, like in the case of PCA or sparse
                  searches? Yes, for example, with regularized auto-encoders or                                   coding, may be viewed as spanning the tangent space of the
                  PCA. In PCA, the sensitivity of the extracted components (the                                   manifold at training point x.
                  code) to input changes is the same regardless of position x.                                                  Input"Point"                     Tangents"
                  The tangent space is the same everywhere along the linear
                  manifold. By contrast, for a non-linear manifold, the tangent
                  of the manifold changes as we move on the manifold, as
                  illustrated in Figure 6. In non-linear representation-learning
                  algorithms it is convenient to think about the local variations
                  in the representation as the input x is varied on the manifold,                                 Fig. 4.      The tangent vectors to the high-density manifold as
                  i.e., as we move among high-probability conﬁgurations. As                                       estimated by a Contractive Auto-Encoder (Rifai et al., 2011a).
                                                                                                                                                            MNIST"
                  we discuss below, the ﬁrst derivative of the encoder therefore                                  The original input is shown on the top left. Each tangent vector
                                                                                                                  (images on right side of ﬁrst row) corresponds to a plausible
                  speciﬁes the shape of the manifold (its tangent plane) around                                                                                                  1"
                  an example x lying on it. If the density was really concentrated                                additive deformation of the original input, as illustrated on the
                  on the manifold, and the encoder had captured that, we                                          second row, where a bit of the 3rd singular vector is added to
                                                                                                                  the original, to form a translated and deformed image. Unlike
                  would ﬁnd the encoder derivatives to be non-zero only in the                                    in PCA, the tangent vectors are different for different inputs,
                  directions spanned by the tangent plane.                                                        because the estimated manifold is highly non-linear.
                                                                                                                  .  Rifai et al. (2011a) empirically analyze in this light the
                      Let us consider sparse coding in this light: parameter matrix                               singular value spectrum of the Jacobian (derivatives of rep-
                  Wmaybeinterpreted as a dictionary of input directions from                                      resentation vector with respect to input vector) of a trained
                  which a different subset will be picked to model the local                                      CAE. Here the SVD provides an ordered orthonormal basis of
                  tangent space at an x on the manifold. That subset corresponds                                  most sensitive directions. The spectrum is sharply decreasing,
                  to the active, i.e. non-zero, features for input x. Non-zero                                    indicating a relatively small number of signiﬁcantly sensi-
                  component hi will be sensitive to small changes of the input                                    tive directions. This is taken as empirical evidence that the
                  in the direction of the associated weight vector W , whereas
                                                                                            :,i                   CAE indeed modeled the tangent space of a low-dimensional
                  inactive features are more likely to be stuck at 0 until a                                      manifold. The leading singular vectors form a basis for the
                  signiﬁcant displacement has taken place in input space.                                         tangent plane of the estimated manifold, as illustrated in
                     20. Even if pairs are picked stochastically, many must be considered before                  Figure 4. The CAE criterion is believed to achieve this thanks
                  obtaining one that weighs signiﬁcantly on the optimization objective.                           to its two opposing terms: the isotropic contractive penalty,
                                                                                                         18
          that encourages the representation to be equally insensitive toguess P(h|x) and initialize a MAP iterative inference (where
          changes in any input directions, and the reconstruction term,the sparse prior P(h) is taken into account). However, in
          that pushes different training points (in particular neighbors) toPSD, the encoder is trained jointly with the decoder, rather
          have a different representation (so they may be reconstructedthan simply taking the end result of iterative inference as a
          accurately), thus counteracting the isotropic contractive pres-target to approximate. An interesting view22 to reconcile these
          sure only in directions tangent to the manifold. facts is that the encoder is a parametric approximation for
            Analyzing learned representations through the lens of thethe MAP solution of a variational lower bound on the joint
          spectrum of the Jacobian and relating it to the notion of tangentlog-likelihood. When MAP learning is viewed as a special
          space of a manifold is feasible, whenever the mapping iscase of variational learning (where the approximation of the
          differentiable, and regardless of how it was learned, whetherjoint log-likelihood is with a dirac distribution located at the
          as direct encoding (as in auto-encoder variants), or derivedMAPsolution), the variational recipe tells us to simultaneously
          from latent variable inference (as in sparse coding or RBMs).improve the likelihood (reduce reconstruction error) and im-
          Exact low dimensional manifold models (like PCA) wouldprove the variational approximation (reduce the discrepancy
          yield non-zero singular values associated to directions alongbetween the encoder output and the latent variable value).
          the manifold, and exact zeros for directions orthogonal to theHence PSD sits at the intersection of probabilistic models
          manifold. But in smooth models like the CAE or the RBM we(with latent variables) and direct encoding methods (which
          will instead have large versus relatively small singular valuesdirectly parametrize the mapping from input to representation).
          (as opposed to non-zero versus exactly zero).    RBMs also sit at the intersection because their particular
          8.3 Leveraging the modeled tangent spaces        parametrization includes an explicit mapping from input to
          The local tangent space, at a point along the manifold, canrepresentation, thanks to the restricted connectivity between
          be thought of capturing locally valid transformations thathidden units. However, this nice property does not extend
          were prominent in the training data. For example Rifai et al.to their natural deep generalizations, i.e., Deep Boltzmann
          (2011c) examine the tangent directions extracted with an SVDMachines, discussed in Section 10.2.
          of the Jacobian of CAEs trained on digits, images, or text-9.2RegularizedAuto-EncodersCaptureLocal
          document data: they appear to correspond to small transla-Structure of the Density
          tions or rotations for images or digits, and to substitutionsCan we also say something about the probabilistic interpreta-
          of words within a same theme for documents. Such verytion of regularized auto-encoders? Their training criterion does
          local transformations along a data manifold are not expectednot ﬁt the standard likelihood framework because this would
          to change class identity. To build their Manifold Tangentinvolve a data-dependent “prior”. An interesting hypothesis
          Classiﬁer (MTC), Rifai et al. (2011c) then apply techniquesemerges to answer that question, out of recent theoretical
          such as tangent distance (Simard et al., 1993) and tangentresults (Vincent, 2011; Alain and Bengio, 2012): the training
          propagation (Simard et al., 1992), that were initially developedcriterion of regularized auto-encoders, instead of being a form
          to build classiﬁers that are insensitive to input deformationsof maximum likelihood, corresponds to a different inductive
          provided as prior domain knowledge. Now these techniquesprinciple, such as score matching. The score matching con-
          are applied using the local leading tangent directions extractednection is discussed in Section 7.2.2 and has been shown for
          by a CAE, i.e. not using any prior domain knowledge (excepta particular parametrization of DAE and equivalent Gaussian
          the broad prior about the existence of a manifold). ThisRBM(Vincent, 2011). The work in Alain and Bengio (2012)
          approach set a new record for MNIST digit classiﬁcationgeneralizes this idea to a broader class of parametrizations (ar-
          among prior-knowledge free approaches21.         bitrary encoders and decoders), and shows that by regularizing
          9 CONNECTIONS BETWEEN PROBABILISTIC theauto-encodersothatit be contractive, one obtains that the
          AND DIRECT ENCODING MODELS                       reconstruction function and its derivative estimate ﬁrst and
          The standard likelihood framework for probabilistic mod-second derivatives of the underlying data-generative density.
          els decomposes the training criterion for models with pa-This view can be exploited to successfully sample from auto-
          rameters θ in two parts: the log-likelihood logP(x|θ) (orencoders, as shown in Rifai et al. (2012); Bengio et al. (2012).
          logP(x|h,θ) with latent variables h), and the prior logP(θ)The proposed sampling algorithms are MCMCs similar to
          (or logP(h|θ)+logP(θ) with latent variables).    Langevin MCMC, using not just the estimated ﬁrst derivative
                                                           of the density but also the estimated manifold tangents so as
          9.1 PSD:aprobabilistic interpretation            to stay close to manifolds of high density.
          In the case of the PSD algorithm, a connection can be madeThis interpretation connects well with the geometric per-
          between the above standard probabilistic view and the directspective introduced in Section 8. The regularization effects
          encoding computation graph. The probabilistic model of PSD(e.g., due to a sparsity regularizer, a contractive regularizer,
          is the same directed generative model P(x|h) of sparse codingor the denoising criterion) asks the learned representation to
          (Section 6.1.1), which only accounts for the decoder. Thebe as insensitive as possible to the input, while minimiz-
          encoder is viewed as an approximate inference mechanism toing reconstruction error on the training examples forces the
                                                           representation to contain just enough information to distin-
           21. It yielded 0.81% error rate using the full MNIST training set, with no
          prior deformations, and no convolution.            22. suggested by Ian Goodfellow, personal communication
                                                                                                                                                                                                   19
                          r(x)"                                                                               reconstruction) and (b) adds noise in the directions of the
                                                                                                              leading singular vectors of the reconstruction (or encoder)
                                                                                                              Jacobian, corresponding to those associated with smallest
                                                                                                              second derivative of the log-density.
                                                                                                              9.3     Learning Approximate Inference
                                                                                                              Let us now consider from closer how a representation is
                                                                                                              computed in probabilistic models with latent variables, when
                                                                                                              iterative inference is required. There is a computation graph
                                                                                                              (possibly with random number generation in some of the
                                              x           x             x                                     nodes, in the case of MCMC) that maps inputs to repre-
                                                1"          2"            3"                    x"            sentation, and in the case of deterministic inference (e.g.,
                  Fig. 5.       Reconstruction function r(x) (green) learned by a                             MAP inference or variational inference), that function could
                  high-capacity autoencoder on 1-dimensional input, minimizing                                be optimized directly. This is a way to generalize PSD that has
                                                                                (t)       (t)
                  reconstruction error at training examples x                       (r(x     ) in red)        been explored in recent work on probabilistic models at the
                  while trying to be as constant as possible otherwise. The dotted                            intersection of inference and learning (Bagnell and Bradley,
                  line is the identity reconstruction (which might be obtained                                2009; Gregor and LeCun, 2010b; Grubb and Bagnell, 2010;
                  without the regularizer). The blue arrows shows the vector ﬁeld                             Salakhutdinov and Larochelle, 2010; Stoyanov et al., 2011;
                  of r(x)−xpointing towards high density peaks estimated by the                               Eisner, 2012), where a central idea is that instead of using a
                  model, and estimating the score (log-density derivative).
                  .                                                                                           generic inference mechanism, one can use one that is learned
                  guish them. The solution is that variations along the high-                                 and is more efﬁcient, taking advantage of the speciﬁcs of the
                  density manifolds are preserved while other variations are                                  type of data on which it is applied.
                  compressed: the reconstruction function should be as constant                               9.4     SamplingChallenges
                  as possible while reproducing training examples, i.e., points                               A troubling challenge with many probabilistic models with
                  near a training example should be mapped to that training                                   latent variables like most Boltzmann machine variants is that
                  example (Figure 5). The reconstruction function should map                                  good MCMC sampling is required as part of the learning
                  an input towards the nearest point manifold, i.e., the difference                           procedure, but that sampling becomes extremely inefﬁcient (or
                  between reconstruction and input is a vector aligned with                                   unreliable) as training progresses because the modes of the
                  the estimated score (the derivative of the log-density with                                 learned distribution become sharper, making mixing between
                  respect to the input). The score can be zero on the manifold                                modesveryslow.Whereasinitially during training a learner as-
                  (where reconstruction error is also zero), at local maxima of                               signs mass almost uniformly, as training progresses, its entropy
                  the log-density, but it can also be zero at local minima. It                                decreases, approaching the entropy of the target distribution as
                  means that we cannot equate low reconstruction error with                                   moreexamplesandmorecomputationareprovided.According
                  high estimated probability. The second derivatives of the log-                              to our Manifold and Natural Clustering priors of Section 3.1,
                  density corresponds to the ﬁrst derivatives of the reconstruction                           the target distribution has sharp modes (manifolds) separated
                  function, and on the manifold (where the ﬁrst derivative is 0),                             by extremely low density areas. Mixing then becomes more
                  they indicate the tangent directions of the manifold (where the                             difﬁcult because MCMC methods, by their very nature, tend
                  ﬁrst derivative remains near 0).                                                            to make small steps to nearby high-probability conﬁgurations.
                                                                                                              This is illustrated in Figure 7.
                  Fig. 6. Sampling from regularized auto-encoders (Rifai et al.,                              Fig. 7. Top: early during training, MCMC mixes easily between
                                                                                                                                                                                              1"
                                                                                                              modes because the estimated distribution has high entropy
                  2012; Bengio et al., 2012): each MCMC step adds to current                                  and puts enough mass everywhere for small-steps movements
                  state x the noise δ mostly in the directions of the estimated man-                          (MCMC) to go from mode to mode. Bottom: later on, training
                  ifold tangent plane H and projects back towards the manifold                                relying on good mixing can stall because estimated modes are
                  (high-density regions) by performing a reconstruction step.                                 separated by wide low-density deserts.
                  .                                                                                           .
                     Asillustrated in Figure 6, the basic idea of the auto-encoder                               Bengio et al. (2013) suggest that deep representations could
                  sampling algorithms in Rifai et al. (2012); Bengio et al. (2012)                            help mixing between such well separated modes, based on
                  is to make MCMC moves where one (a) moves toward the                                        both theoretical arguments and on empirical evidence. The
                  manifold by following the density gradient (i.e., applying a                                idea is that if higher-level representations disentangle better
                                                                                                           20
          the underlying abstract factors, then small steps in this abstractbe useful for early stopping and reducing the cost of ordinary
          space (e.g., swapping from one category to another) can easilyAIS. For toy RBMs (e.g., 25 hidden units or less, or 25
          be done by MCMC. The high-level representations can theninputs or less), the exact log-likelihood can also be computed
          be mapped back to the input space in order to obtain input-analytically, and this can be a good way to debug and verify
          level samples, as in the Deep Belief Networks (DBN) samplingsome properties of interest.
          algorithm (Hinton et al., 2006). This has been demonstrated10 GLOBALTRAINING OF DEEP MODELS
          both with DBNs and with the newly proposed algorithm forOne of the most interesting challenges raised by deep archi-
          sampling from contracting and denoising auto-encoders (Rifaitectures is: how should we jointly train all the levels? In the
          et al., 2012; Bengio et al., 2012). This observation alone doesprevious section and in Section 4 we have only discussed
          not sufﬁce to solve the problem of training a DBN or a DBM,how single-layer models could be combined to form a deep
          but it may provide a crucial ingredient, and it makes it possiblemodel. Here we consider joint training of all the levels and
          to consider successfully sampling from deep models trainedthe difﬁculties that may arise.
          by procedures that do not require an MCMC, like the stacked
          regularized auto-encoders used in Rifai et al. (2012).10.1TheChallengeofTraining Deep Architectures
          9.5 Evaluating and Monitoring Performance         Higher-level abstraction means more non-linearity. It means
          It is always possible to evaluate a feature learning algorithmthat two nearby input conﬁgurations may be interpreted very
          in terms of its usefulness with respect to a particular task (e.g.differently because a few surface details change the underlying
          object classiﬁcation), with a predictor that is fed or initializedsemantics, whereas most other changes in the surface details
          with the learned features. In practice, we do this by savingwould not change the underlying semantics. The representa-
          the features learned (e.g. at regular intervals during training,tions associated with input manifolds may be complex because
          to perform early stopping) and training a cheap classiﬁer onthe mapping from input to representation may have to unfold
          top (such as a linear classiﬁer). However, training the ﬁnaland distort input manifolds that generally have complicated
          classiﬁer can be a substantial computational overhead (e.g.,shapes into spaces where distributions are much simpler, where
          supervised ﬁne-tuning a deep neural network takes usuallyrelations between factors are simpler, maybe even linear or
          more training iterations than the feature learning itself), soinvolving many (conditional) independencies. Our expectation
          we may want to avoid having to train a classiﬁer for ev-is that modeling the joint distribution between high-level
          ery training iteration of the unsupervised learner and everyabstractions and concepts should be much easier in the sense
          hyper-parameter setting. More importantly this may give anof requiring much less data to learn. The hard part is learning a
          incomplete evaluation of the features (what would happen forgood representation that does this unfolding and disentangling.
          other tasks?). All these issues motivate the use of methods toThis may be at the price of a more difﬁcult training problem,
          monitor and evaluate purely unsupervised performance. Thispossibly involving ill-conditioning and local minima.
          is rather easy with all the auto-encoder variants (with someIt is only since 2006 that researchers have seriously inves-
          caution outlined below) and rather difﬁcult with the undirectedtigated ways to train deep architectures, to the exception of
          graphical models such as the RBM and Boltzmann machines.the convolutional networks (LeCun et al., 1998b). The ﬁrst
            For auto-encoder and sparse coding variants, test set re-realization (Section 4) was that unsupervised or supervised
          construction error can readily be computed, but by itself maylayer-wise training was easier, and that this could be taken
          be misleading because larger capacity (e.g., more features,advantage of by stacking single-layer models into deeper ones.
          more training time) tends to systematically lead to lowerIt is interesting to ask why does the layerwise unsuper-
          reconstruction error, even on the test set. Hence it cannot bevised pre-training procedure sometimes help a supervised
          used reliably for selecting most hyper-parameters. On the otherlearner (Erhan et al., 2010b). There seems to be a more
                                                            general principle at play 23 of guiding the training of inter-
          hand, denoising reconstruction error is clearly immune to thismediate representations, which may be easier than trying to
          problem, so that solves the problem for DAEs. Based on thelearn it all in one go. This is nicely related to the curriculum
          connection between DAEs and CAEs uncovered in Bengiolearning idea (Bengio et al., 2009), that it may be much easier
          et al. (2012); Alain and Bengio (2012), this immunity can beto learn simpler concepts ﬁrst and then build higher-level
          extended to DAEs, but not to the hyper-parameter controllingones on top of simpler ones. This is also coherent with the
          the amount of noise or of contraction.            success of several deep learning algorithms that provide some
            For RBMs and some (not too deep) Boltzmann machines,such guidance for intermediate representations, like Semi-
          one option is the use of Annealed Importance Sampling (Mur-Supervised Embedding (Weston et al., 2008).
          ray and Salakhutdinov, 2009) in order to estimate the partitionThe question of why unsupervised pre-training could be
          function (and thus the test log-likelihood). Note that this esti-helpful was extensively studied (Erhan et al., 2010b), trying
          mator can have high variance and that it becomes less reliableto dissect the answer into a regularization effect and an
          (variance becomes too large) as the model becomes moreoptimization effect. The regularization effect is clear from
          interesting, with larger weights, more non-linearity, sharperthe experiments where the stacked RBMs or denoising auto-
          modes and a sharper probability density function (see ourencoders are used to initialize a supervised classiﬁcation neural
          previous discussion in Section 9.4). Another interesting andnetwork (Erhan et al., 2010b). It may simply come from the
          recently proposed option for RBMs is to track the partition
          function during training (Desjardins et al., 2011), which could23. First suggested to us by Leon Bottou
                                                                                21
        use of unsupervised learning to bias the learning dynamicsparticularly interesting results obtained with sparse rectifying
        and initialize it in the basin of attraction of a “good” localunits (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot
        minimum (of the training criterion), where “good” is in termset al., 2011a; Krizhevsky et al., 2012). An old idea regarding
        of generalization error. The underlying hypothesis exploitedthe ill-conditioning issue with neural networks is that of
        by this procedure is that some of the features or latent factorssymmetry breaking: part of the slowness of convergence may
        that are good at capturing the leading variations in the inputbe due to many units moving together (like sheep) and all
        distribution are also good at capturing the variations in thetrying to reduce the output error for the same examples.
        target output random variables of interest (e.g., classes). TheBy initializing with sparse weights (Martens, 2010) or by
        optimization effect is more difﬁcult to tease out because theusing often saturated non-linearities (such as rectiﬁers as max-
        top two layers of a deep neural net can just overﬁt the trainingpooling units), gradients only ﬂow along a few paths, which
        set whether the lower layers compute useful features or not,may help hidden units to specialize more quickly. Another
        but there are several indications that optimizing the lowerpromising idea to improve the conditioning of neural network
        levels with respect to a supervised training criterion can betraining is to nullify the average value and slope of each
        challenging.                         hidden unit output (Raiko et al., 2012), and possibly locally
         One such indication is that changing the numerical con-normalize magnitude as well (Jarrett et al., 2009). The debate
        ditions of the optimization procedure can have a profoundstill rages between using online methods such as stochastic
        impact on the joint training of a deep architecture, for ex-gradient descent and using second-order methods on large
        ample by changing the initialization range and changing theminibatches (of several thousand examples) (Martens, 2010;
        type of non-linearity used (Glorot and Bengio, 2010), muchLe et al., 2011a), with a variant of stochastic gradient descent
        more so than with shallow architectures. One hypothesis torecently winning an optimization challenge 24.
        explain some of the difﬁculty in the optimization of deepFinally, several recent results exploiting large quantities
        architectures is centered on the singular values of the Jacobianoflabeled data suggest that with proper initialization
        matrix associated with the transformation from the featuresand choice of non-linearity, very deep purely supervised
        at one level into the features at the next level (Glorot andnetworks can be trained successfully without any layerwise
        Bengio, 2010). If these singular values are all small (less thanpre-training (Ciresan et al., 2010; Glorot et al., 2011a; Seide
        1), then the mapping is contractive in every direction andet al., 2011a; Krizhevsky et al., 2012). Researchers report
        gradients would vanish when propagated backwards throughthan in such conditions, layerwise unsupervised pre-training
        many layers. This is a problem already discussed for recurrentbrought little or no improvement over pure supervised
        neural networks (Bengio et al., 1994), which can be seen aslearning from scratch when training for long enough. This
        very deep networks with shared parameters at each layer, whenreinforcesthehypothesis that unsupervised pre-training
        unfolded in time. This optimization difﬁculty has motivatedacts as a prior, which may be less necessary when very
        the exploration of second-order methods for deep architectureslarge quantities of labeled data are available, but begs the
        and recurrent networks, in particular Hessian-free second-question of why this had not been discovered earlier. The
        order methods (Martens, 2010; Martens and Sutskever, 2011).latest results reported in this respect (Krizhevsky et al.,
        Unsupervised pre-training has also been proposed to help2012) are particularly interesting because they allowed to
        training recurrent networks and temporal RBMs (Sutskeverdrastically reduce the error rate of object recognition on a
        et al., 2009), i.e., at each time step there is a local signal tobenchmark (the 1000-class ImageNet task) where many more
        guide the discovery of good features to capture in the statetraditional computer vision approaches had been evaluated
        variables: model with the current state (as hidden units) the(http://www.image-net.org/challenges/LSVRC/2012/results.html).
        joint distribution of the previous state and the current input.The main techniques that allowed this success include the
        Natural gradient (Amari, 1998) methods that can be applied tofollowing: efﬁcient GPU training allowing one to train
        networks with millions of parameters (i.e. with good scalinglonger (more than 100 million visits of examples), an aspect
        properties) have also been proposed (Le Roux et al., 2008b;ﬁrst reported by Lee et al. (2009a); Ciresan et al. (2010),
        Pascanu and Bengio, 2013). Cho et al. (2011) proposes to uselarge number of labeled examples, artiﬁcially transformed
        adaptive learning rates for RBM training, along with a novelexamples (see Section 11.1), a large number of tasks (1000
        and interesting idea for a gradient estimator that takes intoor 10000 classes for ImageNet), convolutional architecture
        account the invariance of the model to ﬂipping hidden unit bitswith max-pooling (see section 11 for these latter two
        and inverting signs of corresponding weight vectors. At leasttechniques),rectifyingnon-linearities(discussedabove),
        one study indicates that the choice of initialization (to makecareful initialization (discussed above), careful parameter
        the Jacobian of each layer closer to 1 across all its singularupdate and adaptive learning rate heuristics, layerwise
        values) could substantially reduce the training difﬁculty offeature normalization (across features), and a new dropout
        deep networks (Glorot and Bengio, 2010) and this is coherenttrick based on injecting strong binary multiplicative noise on
        with the success of the initialization procedure of Echo Statehidden units. This trick is similar to the binary noise injection
        Networks (Jaeger, 2007), as recently studied by Sutskeverused at each layer of a stack of denoising auto-encoders.
        (2012). There are also several experimental results (Glorot andFuture work is hopefully going to help identify which of
        Bengio, 2010; Glorot et al., 2011a; Nair and Hinton, 2010)these elements matter most, how to generalize them across
        showing that the choice of hidden units non-linearity could
        inﬂuence both training and generalization performance, with24. https://sites.google.com/site/nips2011workshop/optimization-challenges
                                                                                                                                                      22
              a large variety of tasks and architectures, and in particular            Note how the above equations ostensibly look like a ﬁxed
              contexts where most examples are unlabeled, i.e., including           point recurrent neural network, i.e., with constant input. In
              an unsupervised component in the training criterion.                  the same way that an RBM can be associated with a simple
              10.2   Joint Training of Deep Boltzmann Machines                      auto-encoder, the above mean-ﬁeld update equations for the
              Wenowconsider the problem of joint training of all layers of          DBMcanbeassociated with a recurrent auto-encoder. In that
              a speciﬁc unsupervised model, the Deep Boltzmann Machine              case the training criterion involves the reconstruction error at
              (DBM). Whereas much progress (albeit with many unan-                  the last or at consecutive time steps. This type of model has
              swered questions) has been made on jointly training all the           been explored by Savard (2011) and Seung (1998) and shown
              layers of deep architectures using back-propagated gradients          to do a better job at denoising than ordinary auto-encoders.
              (i.e., mostly in the supervised setting), much less work has             Iterating Eq. (31-32) until convergence yields the Q param-
                                                                                    eters of the “variational positive phase” of Eq. 33:
              been done on their purely unsupervised counterpart, e.g. with                           h           (1)  (2)             (1)  (2) i
                     25                                                                 L(Q ) =E        logP(v,h    , h   ) −logQ (h     , h   )
                                                                                             v     Q                               v
              DBMs .Notehoweverthatonecouldhopethatthesuccessful                                     v h                                          i
              techniques described in the previous section could be applied                     =E      −EDBM(v,h(1),h(2))−logQ (h(1),h(2))
                                                                                                   Q       θ                          v
              to unsupervised learning algorithms.                                                   v
                Like the RBM, the DBM is another particular subset of                    ∂L(Q ) −logZθ ∂EDBM(v,h(1),h(2))
                                                                                               v  =−E           θ
                                                                                                        Q
              the Boltzmann machine family of models where the units                        ∂θ            v           ∂θ
                                                                                                           ∂EDBM(v,h(1),h(2))
              are again arranged in layers. However unlike the RBM, the                              +E         θ                                  (33)
              DBMpossesses multiple layers of hidden units, with units in                                P            ∂θ
              odd-numbered layers being conditionally independent given                This variational learning procedure leaves the “negative
              even-numbered layers, and vice-versa. With respect to the             phase” untouched, which can thus be estimated through SML
              Boltzmann energy function of Eq. 7, the DBM corresponds               or Contrastive Divergence (Hinton, 2000) as in the RBM case.
              to setting U = 0 and a sparse connectivity structure in both V        10.2.2    Training Deep Boltzmann Machines
              and W. We can make the structure of the DBM more explicit             The major difference between training a DBM and an RBM
              by specifying its energy function. For the model with two             is that instead of maximizing the likelihood directly, we
              hidden layers it is given as:                                         instead choose parameters to maximize the lower-bound on
                  DBM       (1)  (2)          T    (1)     (1) T   (2)
                 E     (v,h   , h   ; θ) = − v Wh      −h       Vh −                the likelihood given in Eq. 30. The SML-based algorithm for
                  θ
                                           (1) T  (1)    (2) T (2)    T
                                          d     h    −d       h    −b v,     (29)   maximizing this lower-bound is as follows:
                                   (1)  (2)                                            1) Clamp the visible units to a training example.
              with θ = {W,V,d         , d  , b}. The DBM can also be char-             2) Iterate over Eq. (31-32) until convergence.
              acterized as a bipartite graph between two sets of vertices,                                                        −    (1)−        (2)−
              formed by odd and even-numbered layers (with v := h(0)).                 3) Generate negative phase samples v , h             and h
                                                                                           through SML.
              10.2.1    Mean-ﬁeld approximate inference                                4) Compute∂L(Q )/∂θ usingthevaluesobtained in steps
              A key point of departure from the RBM is that the pos-                                        v
                                                                                           2-3.
              terior distribution over the hidden units (given the visibles)           5) Finally, update the model parameters with a step of
              is no longer tractable, due to the interactions between the                  approximate stochastic gradient ascent.
              hidden units. Salakhutdinov and Hinton (2009) resort to a                While the above procedure appears to be a simple extension
              mean-ﬁeld approximation to the posterior. Speciﬁcally, in             of the highly effective SML scheme for training RBMs, as we
              the case of a model with two hidden layers, we wish to                demonstrate in Desjardins et al. (2012), this procedure seems
                                 (1)   (2)    
              approximate P h       , h    | v  with the factored distribution      vulnerable to falling in poor local minima which leave many
                                   QN          (1) QN            (2)
                    (1)  (2)            1                   2                       hidden units effectively dead (not signiﬁcantly different from
              Q (h ,h ) =                 Q h                 Q h , such
                v                     j=1   v    j        i=1   v    i
                                                    (1)   (2)            1  2     its random initialization with small norm).
              that the KL divergence KL P h            , h    | v kQ (h ,h )
                                                                      v                The failure of the SML joint training strategy was noted
              is minimized or equivalently, that a lower bound to the log           by Salakhutdinov and Hinton (2009). As an alternative, they
              likelihood is maximized:                                          proposed a greedy layer-wise training strategy. This procedure
                                    XX                           P(v,h(1),h(2))
                                                  (1)  (2)
              logP(v) > L(Q ) ≡             Q (h ,h )log                            consists in pre-training the layers of the DBM, in much the
                               v              v                        (1)  (2)
                                                                 Q (h ,h )
                                     (1) (2)                       v
                                    h   h                                    (30)   same way as the Deep Belief Network: i.e. by stacking RBMs
                Maximizing this lower-bound with respect to the mean-ﬁeld           and training each layer to independently model the output of
                                 1   2                                              the previous layer. A ﬁnal joint “ﬁne-tuning” is done following
              distribution Q (h ,h ) (by setting derivatives to zero) yields
                             v                                                      the above SML-based procedure.
              the following mean ﬁeld update equations:
                                                                     !
                   ˆ(1)               X            X ˆ(2)          (1)              11 BUILDING-IN INVARIANCE
                   hi   ←sigmoid          Wjivj +      Vikh    +di           (31)
                                                            k                       It is well understood that incorporating prior domain knowl-
                                      j             k   !                           edge helps machine learning. Exploring good strategies for
                   ˆ(2)               X ˆ(1)         (2)
                   h    ←sigmoid          Vikh    +d                         (32)
                     k                        i      k                              doing so is a very important research avenue. However, if we
                                       i                                            are to advance our understanding of core machine learning
                25. Joint training of all the layers of a Deep Belief Net is much moreprinciples, it is important that we keep comparisons between
              challenging because of the much harder inference problem involved.    predictors fair and maintain a clear awareness of the prior
                                                                                23
        domain knowledge used by different learning algorithms,feature map. Equivalently to sweeping, this may be seen as
        especially when comparing their performance on benchmarkstatic but differently positioned replicated feature extractors
        problems. We have so far only presented algorithms thatthat all share the same parameters. This is at the heart of
        exploited only generic inductive biases for high dimensionalconvolutional networks (LeCun et al., 1989, 1998b) which
        problems, thus making them potentially applicable to anyhave been applied both to object recognition and to image
        high dimensional problem. The most prevalent approach tosegmentation (Turaga et al., 2010). Another hallmark of the
        incorporating prior knowledge is to hand-design better featuresconvolutional architecture is that values computed by the same
        to feed a generic classiﬁer, and has been used extensively infeature detector applied at several neighboring input locations
        computer vision (e.g. (Lowe, 1999)). Here, we rather focusare then summarized through a pooling operation, typically
        on how basic domain knowledge of the input, in particulartaking their max or their sum. This confers the resulting pooled
        its topological structure (e.g. bitmap images having a 2Dfeature layer some degree of invariance to input translations,
        structure), may be used to learn better features.and this style of architecture (alternating selective feature
                                             extraction and invariance-creating pooling) has been the ba-
        11.1Generating transformed examples  sis of convolutional networks, the Neocognitron (Fukushima,
        Generalization performance is usually improved by providing1980) and HMAX (Riesenhuber and Poggio, 1999) models,
        a larger quantity of representative data. This can be achievedand argued to be the architecture used by mammalian brains
        by generating new examples by applying small random defor-for object recognition (Riesenhuber and Poggio, 1999; Serre
        mations to the original training examples, using deformationset al., 2007; DiCarlo et al., 2012). The output of a pooling
        that are known not to change the target variables of interest,unit will be the same irrespective of where a speciﬁc feature
        e.g., an object class is invariant to small transformations ofis located inside its pooling region. Empirically the use of
        images such as translations, rotations, scaling, or shearing.pooling seems to contribute signiﬁcantly to improved classi-
        This old approach (Baird, 1990) has been recently applied withﬁcation accuracy in object classiﬁcation tasks (LeCun et al.,
        great success in the work of Ciresan et al. (2010) who used an1998b; Boureau et al., 2010, 2011). A successful variant of
                                                                              ¨
        efﬁcient GPU implementation (40× speedup) to train a stan-pooling connected to sparse coding is L2 pooling (Hyvarinen
        dard but large deep multilayer Perceptron on deformed MNISTet al., 2009; Kavukcuoglu et al., 2009; Le et al., 2010),
        digits. Using both afﬁne and elastic deformations (Simardfor which the pool output is the square root of the possibly
        et al., 2003), with plain old stochastic gradient descent, theyweighted sum of squares of ﬁlter outputs. Ideally, we would
        reach a record 0.32% classiﬁcation error rate.like to generalize feature-pooling so as to learn what features
                                             should be pooled together, e.g. as successfully done in several
        11.2Convolution and pooling                 ¨
                                             papers (Hyvarinen and Hoyer, 2000; Kavukcuoglu et al., 2009;
        Another powerful approach is based on even more basicLe et al., 2010; Ranzato and Hinton, 2010; Courville et al.,
        knowledge of merely the topological structure of the input2011b; Coates and Ng, 2011b; Gregor et al., 2011). In this
        dimensions. By this we mean e.g., the 2D layout of pixelsway, the pool output learns to be invariant to the variations
        in images or audio spectrograms, the 3D structure of videos,captured by the span of the features pooled.
        the 1D sequential structure of text or of temporal sequences
        in general. Based on such structure, one can deﬁne localPatch-based training
        receptive ﬁelds (Hubel and Wiesel, 1959), so that each low-The simplest approach for learning a convolutional layer in an
        level feature will be computed from only a subset of the input:unsupervised fashion is patch-based training: simply feeding
        a neighborhood in the topology (e.g. a sub-image at a givena generic unsupervised feature learning algorithm with local
        position). This topological locality constraint corresponds to apatches extracted at random positions of the inputs. The
        layer having a very sparse weight matrix with non-zeros onlyresulting feature extractor can then be swiped over the input to
        allowed for topologically local connections. Computing theproduce the convolutional feature maps. That map may be used
        associated matrix products can of course be made much moreas a new input for the next layer, and the operation repeated
        efﬁcient than having to handle a dense matrix, in additionto thus learn and stack several layers. Such an approach
        to the statistical gain from a much smaller number of freewas recently used with Independent Subspace Analysis (Le
        parameters. In domains with such topological structure, similaret al., 2011c) on 3D video blocks, reaching the state-of-the-art
        input patterns are likely to appear at different positions, andon Hollywood2, UCF, KTH and YouTube action recognition
        nearby values (e.g. consecutive frames or nearby pixels) aredatasets. Similarly (Coates and Ng, 2011a) compared several
        likely to have stronger dependencies that are also important tofeature learners with patch-based training and reached state-
        model the data. In fact these dependencies can be exploitedof-the-art results on several classiﬁcation benchmarks. Inter-
        to discover the topology (Le Roux et al., 2008a), i.e. recoverestingly, in this work performance was almost as good with
        a regular grid of pixels out of a set of vectors without anyvery simple k-means clustering as with more sophisticated
        order information, e.g. after the elements have been arbitrarilyfeature learners. We however conjecture that this is the case
        shufﬂed in the same way for all examples. Thus a same localonly because patches are rather low dimensional (compared
        feature computation is likely to be relevant at all translated po-to the dimension of a whole image). A large dataset might
        sitions of the receptive ﬁeld. Hence the idea of sweeping suchprovide sufﬁcient coverage of the space of e.g. edges prevalent
        a local feature extractor over the topology: this corresponds toin 6 × 6 patches, so that a distributed representation is not
        a convolution, and transforms an input into a similarly shapedabsolutely necessary. Another plausible explanation for this
                                                                                24
        success is that the clusters identiﬁed in each image patch areparticular this idea has been applied to image sequences and as
        then pooled into a histogram of cluster counts associated withan explanation for why V1 simple and complex cells behave
        a larger sub-image. Whereas the output of a regular clusteringthe way they do. A good overview can be found in Hurri and
                                                ¨
        is a one-hot non-distributed code, this histogram is itself aHyvarinen (2003); Berkes and Wiskott (2005).
        distributed representation, and the “soft” k-means (Coates andMore recently, temporal coherence has been successfully
        Ng, 2011a) representation allows not only the nearest ﬁlter butexploited in deep architectures to model video (Mobahi et al.,
        also its neighbors to be active.     2009). It was also found that temporal coherence discov-
                                             ered visual features similar to those obtained by ordinary
        Convolutional and tiled-convolutional trainingunsupervised feature learning (Bergstra and Bengio, 2009),
        It is possible to directly train large convolutional layers usingand a temporal coherence penalty has been combined with a
        an unsupervised criterion. An early approach (Jain and Seung,training criterion for unsupervised feature learning (Zou et al.,
        2008) trained a standard but deep convolutional MLP on2011), sparse auto-encoders with L1 regularization, in this
        the task of denoising images, i.e. as a deep, convolutional,case, yielding improved classiﬁcation performance.
        denoising auto-encoder. Convolutional versions of the RBMThe temporal coherence prior can be expressed in several
        or its extensions have also been developed (Desjardins andways, the simplest being the squared difference between
        Bengio, 2008; Lee et al., 2009a; Taylor et al., 2010) as well asfeature values at times t and t + 1. Other plausible tempo-
        a probabilistic max-pooling operation built into Convolutionalral coherence priors include the following. First, instead of
        Deep Networks (Lee et al., 2009a,b; Krizhevsky, 2010). Otherpenalizing the squared change, penalizing the absolute value
        unsupervised feature learning approaches that were adapted to(or a similar sparsity penalty) would state that most of the
        the convolutional setting include PSD (Kavukcuoglu et al.,time the change should be exactly 0, which would intuitively
        2009, 2010; Jarrett et al., 2009; Henaff et al., 2011), amake sense for the real-life factors that surround us. Second,
        convolutional version of sparse coding called deconvolutionalone would expect that instead of just being slowly changing,
        networks (Zeiler et al., 2010), Topographic ICA (Le et al.,different factors could be associated with their own different
        2010), and mPoT that Kivinen and Williams (2012) appliedtime scale. The speciﬁcity of their time scale could thus
        to modeling natural textures. Gregor and LeCun (2010a);become a hint to disentangle explanatory factors. Third, one
        Le et al. (2010) also demonstrated the technique of tiled-would expect that some factors should really be represented by
        convolution, where parameters are shared only between featurea group of numbers (such as x, y, and z position of some object
        extractors whose receptive ﬁelds are k steps away (so thein space and the pose parameters ofHinton et al. (2011))
        ones looking at immediate neighbor locations are not shared).rather than by a single scalar, and that these groups tend
        This allows pooling units to be invariant to more than justto move together. Structured sparsity penalties (Kavukcuoglu
        translations, and is a hybrid between convolutional networkset al., 2009; Jenatton et al., 2009; Bach et al., 2011; Gregor
        and earlier neural networks with local connections but noet al., 2011) could be used for this purpose.
        weight sharing (LeCun, 1986, 1989).  11.4 Algorithms to Disentangle Factors of Variation
                                             The goal of building invariant features is to remove sensitivity
        Alternatives to pooling              of the representation to directions of variance in the data that
        Alternatively, one can also use explicit knowledge of theare uninformative to the task at hand. However it is often the
        expected invariants expressed mathematically to deﬁne trans-case that the goal of feature extraction is the disentangling or
        formations that are robust to a known family of input defor-separation of many distinct but informative factors in the data,
        mations, using so-called scattering operators (Mallat, 2012;e.g., in a video of people: subject identity, action performed,
        Bruna and Mallat, 2011), which can be computed in a waysubject pose relative to the camera, etc. In this situation,
        interestingly analogous to deep convolutional networks andthe methods of generating invariant features, such as feature-
        wavelets. Like convolutional networks, the scattering operatorspooling, may be inadequate.
        alternate two types of operations: convolution and poolingThe process of building invariant features can be seen as
        (as a norm). Unlike convolutional networks, the proposedconsisting of two steps. First, low-level features are recovered
        approach keeps at each level all of the information about thethat account for the data. Second, subsets of these low level
        input (in a way that can be inverted), and automatically yieldsfeatures are pooled together to form higher-level invariant
        a very sparse (but very high-dimensional) representation. An-features, exempliﬁed by the pooling and subsampling layers
        other difference is that the ﬁlters are not learned but insteadof convolutional neural networks. The invariant representation
        set so as to guarantee that a priori speciﬁed invariances areformed by the pooling features offers an incomplete window
        robustly achieved. Just a few levels were sufﬁcient to achieveon the data as the detailed representation of the lower-level
        impressive results on several benchmark datasets.features is abstracted away in the pooling procedure. While
                                             we would like higher-level features to be more abstract and
        11.3Temporal coherence and slow featuresexhibit greater invariance, we have little control over what
        Theprinciple of identifying slowly moving/changing factors ininformation is lost through pooling. What we really would like
        temporal/spatial data has been investigated by many (Beckeris for a particular feature set to be invariant to the irrelevant
        and Hinton, 1992; Wiskott and Sejnowski, 2002; Hurri andfeatures and disentangle the relevant features. Unfortunately,
          ¨        ¨
        Hyvarinen, 2003; Kording et al., 2004; Cadieu and Olshausen,it is often difﬁcult to determine a priori which set of features
        2009) as a principle for ﬁnding useful representations. Inwill ultimately be relevant to the task at hand.
                                                                                25
         An interesting approach to taking advantage of some ofgenerative interpretation (or a way to reconstruct inputs from
        the factors of variation known to exist in the data is thetheir high-level representation), the generative perspective can
        transforming auto-encoder (Hinton et al., 2011): instead of aprovide insight into how to think about disentangling fac-
        scalar pattern detector (e.g,. corresponding to the probabilitytors. The majority of the models currently used to construct
        of presence of a particular form in the input) one can thinkinvariant features have the interpretation that their low-level
        of the features as organized in groups that include both a        27
                                             features linearly combine to construct the data.This is a
        pattern detector and pose parameters that specify attributesfairly rudimentary form of feature composition with signiﬁcant
        of the detected pattern. In (Hinton et al., 2011), what islimitations. For example, it is not possible to linearly combine
        assumedapriori is that pairs of examples (or consecutive ones)a feature with a generic transformation (such as translation) to
        are observed with an associated value for the correspondinggenerate a transformed version of the feature. Nor can we even
        change in the pose parameters. For example, an animal thatconsider a generic color feature being linearly combined with
        controls its eyes knows what changes to its ocular motora gray-scale stimulus pattern to generate a colored pattern. It
        system were applied when going from one image on its retinawould seem that if we are to take the notion of disentangling
        to the next. In that work, it is also assumed that the poseseriously we require a richer interaction of features than that
        changes are the same for all the pattern detectors, and thisoffered by simple linear combinations.
        makes sense for global changes such as image translation and12 CONCLUSION
        camera geometry changes. Instead, we would like to discoverThis review of representation learning and deep learning has
        the pose parameters and attributes that should be associatedcovered three major and apparently disconnected approaches:
        with each feature detector, without having to specify ahead ofthe probabilistic models (both the directed kind such as
        time what they should be, force them to be the same for allsparse coding and the undirected kind such as Boltzmann
        features, and having to necessarily observe the changes in allmachines), the reconstruction-based algorithms related to auto-
        of the pose parameters or attributes.encoders, and the geometrically motivated manifold-learning
         The approach taken recently in the Manifold Tangent Clas-approaches. Drawing connections between these approaches
        siﬁer, discussed in section 8.3, is interesting in this respect.is currently a very active area of research and is likely to
        Without any supervision or prior knowledge, it ﬁnds prominentcontinue to produce models and methods that take advantage
        local factors of variation (tangent vectors to the manifold,of the relative strengths of each paradigm.
        extracted from a CAE, interpreted as locally valid input ”defor-Practical Concerns and Guidelines. One of the criticisms
        mations”). Higher-level features are subsequently encouragedaddressed to artiﬁcial neural networks and deep learning algo-
        to be invariant to these factors of variation, so that they mustrithms is that they have many hyper-parameters and variants
        depend on other characteristics. In a sense this approach isand that exploring their conﬁgurations and architectures is an
        disentangling valid local deformations along the data manifoldart. This has motivated an earlier book on the “Tricks of the
        from other, more drastic changes, associated to other factorsTrade” (Orr and Muller, 1998) of which LeCun et al. (1998a)
                                     26
        of variation such as those that affect class identity.is still relevant for training deep architectures, in particular
         One solution to the problem of information loss that wouldwhat concerns initialization, ill-conditioning and stochastic
        ﬁt within the feature-pooling paradigm, is to consider manygradient descent. A good and more modern compendium of
        overlapping pools of features based on the same low-levelgood training practice, particularly adapted to training RBMs,
        feature set. Such a structure would have the potential tois provided in Hinton (2010), while a similar guide oriented
        learn a redundant set of invariant features that may not causemore towards deep neural networks can be found in Bengio
        signiﬁcant loss of information. However it is not obvious(2013), both of which are part of a novel version of the
        what learning principle could be applied that can ensureabove book. Recent work on automating hyper-parameter
        that the features are invariant while maintaining as muchsearch (Bergstra and Bengio, 2012; Bergstra et al., 2011;
        information as possible. While a Deep Belief Network or aSnoek et al., 2012) is also making it more convenient, efﬁcient
        Deep Boltzmann Machine (as discussed in sections 4 and 10.2and reproducible.
        respectively) with two hidden layers would, in principle, beIncorporating Generic AI-level Priors. We have covered
        able to preserve information into the “pooling” second hiddenmany high-level generic priors that we believe could bring
        layer, there is no guarantee that the second layer featuresmachine learning closer to AI by improving representation
        are more invariant than the “low-level” ﬁrst layer features.learning. Many of these priors relate to the assumed existence
        However, there is some empirical evidence that the secondof multiple underlying factors of variation, whose variations
        layer of the DBN tends to display more invariance than theare in some sense orthogonal to each other. They are expected
        ﬁrst layer (Erhan et al., 2010a).    to be organized at multiple levels of abstraction, hence the
         A more principled approach, from the perspective of en-need for deep architectures, which also have statistical advan-
        suring a more robust compact feature representation, cantages because they allow to re-use parameters in a combi-
        be conceived by reconsidering the disentangling of featuresnatorially efﬁcient way. Only a few of these factors would
        through the lens of its generative equivalent – feature com-
        position. Since many unsupervised learning algorithms have a27. As an aside, if we are given only the values of the higher-level pooling
                                             features, we cannot accurately recover the data because we do not know how to
         26. The changes that affect class identity might, in input space, actually beapportion credit for the pooling feature values to the lower-level features. This
        of similar magnitude to local deformations, but not follow along the manifold,is simply the generative version of the consequences of the loss of information
        i.e. cross zones of low density.     caused by pooling.
                                                                                26
        typically be relevant for any particular example, justifyingremain unanswered and deserve much more study.
        sparsity of representation. These factors are expected to beAcknowledgments
        related to simple (e.g., linear) dependencies, with subsets of
        these explaining different random variables of interest (inputs,The author would like to thank David Warde-Farley, Razvan
        tasks) and varying in structured ways in time and spacePascanu and Ian Goodfellow for useful feedback, as well as
        (temporal and spatial coherence). We expect future successfulNSERC, CIFAR and the Canada Research Chairs for funding.
        applications of representation learning to reﬁne and increaseREFERENCES
        that list of priors, and to incorporate most of them instead of
        focusing on only one. Research in training criteria that betterAlain, G. and Bengio, Y. (2012). What regularized auto-encoders
        take these priors into account are likely to move us closer tolearn from the data generating distribution. Technical Report Arxiv
                                                             ´     ´
        the long-term objective of discovering learning algorithms thatreport 1211.4246, Universite de Montreal.
        can disentangle the underlying explanatory factors.Amari, S. (1998).Natural gradient works efﬁciently in learning.
                                               Neural Computation, 10(2), 251–276.
         Inference. We anticipate that methods based on directlyBach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011). Struc-
        parametrizing a representation function will incorporate moretured sparsity through convex optimization. CoRR, abs/1109.2397.
        and more of the iterative type of computation one ﬁnds in theBagnell, J. A. and Bradley, D. M. (2009).Differentiable sparse
        inference procedures of probabilistic latent-variable models.coding. In NIPS’2009, pages 113–120.
        There is already movement in the other direction, with prob-Baird, H. (1990). Document image defect models. In IAPR Workshop,
                                               Syntactic & Structural Patt. Rec., pages 38–46.
        abilistic latent-variable models exploiting approximate infer-Becker, S. and Hinton, G. (1992). A self-organizing neural network
        ence mechanisms that are themselves learned (i.e., producing athat discovers surfaces in random-dot stereograms. Nature, 355,
        parametric description of the representation function). A major161–163.
        appeal of probabilistic models is that the semantics of theBelkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimen-
        latent variables are clear and this allows a clean separationsionality reduction and data representation. Neural Computation,
                                               15(6), 1373–1396.
        of the problems of modeling (choose the energy function),Bell, A. and Sejnowski, T. J. (1997). The independent components
        inference (estimating P(h|x)), and learning (optimizing theof natural scenes are edge ﬁlters. Vision Research, 37, 3327–3338.
        parameters), using generic tools in each case. On the otherBengio, Y. (1993). A connectionist approach to speech recognition.
        hand, doing approximate inference and not taking that approxi-International Journal on Pattern Recognition and Artiﬁcial Intel-
        mation into account explicitly in the approximate optimizationligence, 7(4), 647–668.
                                             Bengio, Y. (2008). Neural net language models. Scholarpedia, 3(1).
        for learning could have detrimental effects, hence the appealBengio, Y. (2009). Learning deep architectures for AI. Foundations
        of learning approximate inference. More fundamentally, thereand Trends in Machine Learning, 2(1), 1–127. Also published as
        is the question of the multimodality of the posterior P(h|x).a book. Now Publishers, 2009.
        If there are exponentially many probable conﬁgurations ofBengio, Y. (2011). Deep learning of representations for unsupervised
        values of the factors h that can explain x, then we seemand transfer learning. In JMLR W&CP: Proc. Unsupervised and
                      i                        Transfer Learning.
        to be stuck with very poor inference, either focusing on aBengio, Y. (2013).Practical recommendations for gradient-based
                                                                     ¨
        single mode (MAP inference), assuming some kind of strongtraining of deep architectures. In K.-R. Muller, G. Montavon, and
        factorization (as in variational inference) or using an MCMCG. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer.
        that cannot visit enough modes of P(h|x). What we proposeBengio, Y. and Delalleau, O. (2009).Justifying and generalizing
        as food for thought is the idea of dropping the requirementcontrastive divergence. Neural Computation, 21(6), 1601–1621.
                                             Bengio, Y. and Delalleau, O. (2011). On the expressive power of
        of an explicit representation of the posterior and settle fordeep architectures. In ALT’2011.
        an implicit representation that exploits potential structure inBengio, Y. and LeCun, Y. (2007). Scaling learning algorithms towards
        P(h|x) in order to represent it compactly: even though P(h|x)AI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors,
        mayhave an exponential number of modes, it may be possibleLarge Scale Kernel Machines. MIT Press.
        to represent it with a small set of numbers. For example,Bengio, Y. and Monperrus, M. (2005). Non-local manifold tangent
                                               learning. In NIPS’2004, pages 129–136. MIT Press.
        consider computing a deterministic feature representation f(x)Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term
        that implicitly captures the information about a highly multi-dependencies with gradient descent is difﬁcult. IEEE Transactions
        modal P(h|x), in the sense that all the questions (e.g. makingon Neural Networks, 5(2), 157–166.
        some prediction about some target concept) that can be askedBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A
        from P(h|x) can also be answered from f(x).neural probabilistic language model. JMLR, 3, 1137–1155.
                                             Bengio, Y., Paiement, J.-F., Vincent, P., Delalleau, O., Le Roux,
         Optimization. Much remains to be done to better under-N., and Ouimet, M. (2004). Out-of-sample extensions for LLE,
        stand the successes and failures of training deep architectures,Isomap, MDS, Eigenmaps, and Spectral Clustering. In NIPS’2003.
        both in the supervised case (with many recent successes) andBengio, Y., Delalleau, O., and Le Roux, N. (2006a). The curse of
        the unsupervised case (where much more work needs to behighly variable functions for local kernel machines. In NIPS’2005.
        done). Although regularization effects can be important onBengio, Y., Larochelle, H., and Vincent, P. (2006b).Non-local
                                               manifold Parzen windows. In NIPS’2005. MIT Press.
        small datasets, the effects that persist on very large datasetsBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007).
        suggest some optimization issues are involved. Are they moreGreedy layer-wise training of deep networks. In NIPS’2006.
        due to local minima (we now know there are huge numbersBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).
        of them) and the dynamics of the training procedure? OrCurriculum learning. In ICML’09.
        are they due mostly to ill-conditioning and may be handledBengio, Y., Delalleau, O., and Simard, C. (2010). Decision trees
                                               do not generalize to new variations. Computational Intelligence,
        by approximate second-order methods? These basic questions26(4), 449–467.
                                                                                27
        Bengio, Y., Alain, G., and Rifai, S. (2012). Implicit density esti-models of images by spike-and-slab RBMs. In ICML’2011.
         mation by local moment matching to sample from auto-encoders.Dahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E. (2010).
         Technical report, arXiv:1207.0057.    Phone recognition with the mean-covariance restricted Boltzmann
        Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Bettermachine. In NIPS’2010.
         mixing via deep representations. In ICML’2013.Dahl, G. E., Yu, D., Deng, L., and Acero, A. (2012).Context-
        Bergstra, J. and Bengio, Y. (2009). Slow, decorrelated features fordependent pre-trained deep neural networks for large vocabulary
         pretraining complex cell-like networks. In NIPS’2009.speech recognition.IEEE Transactions on Audio, Speech, and
        Bergstra, J. and Bengio, Y. (2012).Random search for hyper-Language Processing, 20(1), 33–42.
         parameter optimization. J. Machine Learning Res., 13, 281–305.Deng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton,
                                ´
        Bergstra, J., Bardenet, R., Bengio, Y., and Kegl, B. (2011). Algo-G. (2010). Binary coding of speech spectrograms using a deep
         rithms for hyper-parameter optimization. In NIPS’2011.auto-encoder. In Interspeech 2010, Makuhari, Chiba, Japan.
        Berkes, P. and Wiskott, L. (2005). Slow feature analysis yields aDesjardins, G. and Bengio, Y. (2008).Empirical evaluation of
         rich repertoire of complex cell properties. Journal of Vision, 5(6),convolutional RBMs for vision.Technical Report 1327, Dept.
                                                       ´
         579–602.                              IRO, U. Montreal.
        Besag, J. (1975).Statistical analysis of non-lattice data.TheDesjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau,
         Statistician, 24(3), 179–195.         O. (2010). Tempered Markov chain Monte Carlo for training of
        Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012).Jointrestricted Boltzmann machine. In AISTATS’2010, volume 9, pages
         learning of words and meaning representations for open-text145–152.
         semantic parsing. AISTATS’2012.     Desjardins, G., Courville, A., and Bengio, Y. (2011). On tracking the
        Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012).partition function. In NIPS’2011.
         Modeling temporal dependencies in high-dimensional sequences:Desjardins, G., Courville, A., and Bengio, Y. (2012). On training
         Application to polyphonic music generation and transcription. Indeep Boltzmann machines. Technical Report arXiv:1203.4416v1,
                                                    ´     ´
         ICML’2012.                            Universite de Montreal.
        Boureau, Y., Ponce, J., and LeCun, Y. (2010). A theoretical analysisDiCarlo, J., Zoccolan, D., and Rust, N. (2012). How does the brain
         of feature pooling in vision algorithms. In ICML’10.solve visual object recognition? Neuron.
        Boureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y. (2011).Donoho, D. L. and Grimes, C. (2003). Hessian eigenmaps: new
         Ask the locals: multi-way local pooling for image recognition. Inlocally linear embedding techniques for high-dimensional data.
         ICCV’11.                              Technical Report 2003-08, Dept. Statistics, Stanford University.
        Bourlard, H. and Kamp, Y. (1988). Auto-association by multilayerEisner, J. (2012).Learning approximate inference policies for
         perceptrons and singular value decomposition. Biological Cyber-fast prediction.Keynote talk at ICML Workshop on Inferning:
         netics, 59, 291–294.                  Interactions Between Search and Learning.
        Brand, M. (2003). Charting a manifold. In NIPS’2002, pages 961–Erhan, D., Courville, A., and Bengio, Y. (2010a).Understanding
         968. MIT Press.                       representations learned in deep architectures.Technical Report
        Breuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating´´
                                               1355, Universite de Montreal/DIRO.
         representative samples from an RBM-derived process.NeuralErhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P.,
         Computation, 23(8), 2053–2073.        and Bengio, S. (2010b). Why does unsupervised pre-training help
        Bruna, J. and Mallat, S. (2011).Classiﬁcation with scatteringdeep learning? Journal of Machine Learning Research, 11, 625–
         operators. In ICPR’2011.              660.
        Cadieu, C. and Olshausen, B. (2009).Learning transformationalFreund, Y. and Haussler, D. (1994).Unsupervised learning of
         invariants from natural movies. In NIPS’2009, pages 209–216.distributions on binary vectors using two layer networks. Technical
         MIT Press.                            Report UCSC-CRL-94-25, University of California, Santa Cruz.
               ˜
        Carreira-Perpinan, M. A. and Hinton, G. E. (2005). On contrastiveFukushima, K. (1980).Neocognitron: A self-organizing neural
         divergence learning. In AISTATS’2005, pages 33–40.network model for a mechanism of pattern recognition unaffected
        Chen, M., Xu, Z., Winberger, K. Q., and Sha, F. (2012). Marginalizedby shift in position. Biological Cybernetics, 36, 193–202.
         denoising autoencoders for domain adaptation. In ICML’2012.Glorot, X. and Bengio, Y. (2010). Understanding the difﬁculty of
        Cho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is efﬁcienttraining deep feedforward neural networks. In AISTATS’2010.
         for learning restricted Boltzmann machines. In IJCNN’2010.Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectiﬁer
        Cho, K., Raiko, T., and Ilin, A. (2011).Enhanced gradient andneural networks. In AISTATS’2011.
         adaptive learning rate for training restricted Boltzmann machines.Glorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation
         In ICML’2011, pages 105–112.          for large-scale sentiment classiﬁcation: A deep learning approach.
        Ciresan, D., Meier, U., and Schmidhuber, J. (2012). Multi-columnIn ICML’2011.
         deep neural networks for image classiﬁcation. Technical report,Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009).Measuring
         arXiv:1202.2745.                      invariances in deep networks. In NIPS’2009, pages 646–654.
        Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber,Goodfellow, I., Courville, A., and Bengio, Y. (2011).Spike-and-
         J. (2010).Deep big simple neural nets for handwritten digitslab sparse coding for unsupervised feature discovery. In NIPS
         recognition. Neural Computation, 22, 1–14.Workshop on Challenges in Learning Hierarchical Models.
        Coates, A. and Ng, A. Y. (2011a). The importance of encoding versusGoodfellow, I. J., Courville, A., and Bengio, Y. (2012).Spike-
         training with sparse coding and vector quantization. In ICML’2011.and-slab sparse coding for unsupervised feature discovery.
        Coates, A. and Ng, A. Y. (2011b). Selecting receptive ﬁelds in deeparXiv:1201.3382.
         networks. In NIPS’2011.             Gregor, K. and LeCun, Y. (2010a). Emergence of complex-like cells
        Collobert, R. and Weston, J. (2008).A uniﬁed architecture forin a temporal product network with local receptive ﬁelds. Technical
         natural language processing: Deep neural networks with multitaskreport, arXiv:1006.0448.
         learning. In ICML’2008.             Gregor, K. and LeCun, Y. (2010b). Learning fast approximations of
        Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K.,sparse coding. In ICML’2010.
         and Kuksa, P. (2011). Natural language processing (almost) fromGregor, K., Szlam, A., and LeCun, Y. (2011).Structured sparse
         scratch. Journal of Machine Learning Research, 12, 2493–2537.coding via lateral inhibition. In NIPS’2011.
        Courville, A., Bergstra, J., and Bengio, Y. (2011a). A spike and slabGribonval, R. (2011). Should penalized least squares regression be
         restricted Boltzmann machine. In AISTATS’2011.interpreted as Maximum A Posteriori estimation? IEEE Transac-
        Courville, A., Bergstra, J., and Bengio, Y. (2011b). Unsupervisedtions on Signal Processing, 59(5), 2405–2410.
                                                                                28
        Grosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007). Shift-Jenatton, R., Audibert, J.-Y., and Bach, F. (2009).Structured
         invariant sparse coding for audio classiﬁcation. In UAI’2007.variable selection with sparsity-inducing norms. Technical report,
        Grubb, A. and Bagnell, J. A. D. (2010). Boosted backpropagationarXiv:0904.3523.
         learning for training deep modular networks. In ICML’2010.Jutten, C. and Herault, J. (1991). Blind separation of sources, part I:
        Gutmann,M.andHyvarinen,A.(2010). Noise-contrastiveestimation:an adaptive algorithm based on neuromimetic architecture. Signal
         Anewestimation principle for unnormalized statistical models. InProcessing, 24, 1–10.
         AISTATS’2010.                       Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference
        Hamel, P., Lemieux, S., Bengio, Y., and Eck, D. (2011). Temporalin sparse coding algorithms with applications to object recognition.
         pooling and multiscale learning for automatic annotation andCBLL-TR-2008-12-01, NYU.
         ranking of music audio. In ISMIR.   Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009).
         ˚
        Hastad, J. (1986).Almost optimal lower bounds for small depthLearning invariant features through topographic ﬁlter maps. In
         circuits. In STOC’86, pages 6–20.     CVPR’2009.
         ˚
        Hastad, J. and Goldmann, M. (1991). On the power of small-depthKavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Math-
         threshold circuits. Computational Complexity, 1, 113–129.ieu, M., and LeCun, Y. (2010). Learning convolutional feature
        Henaff, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y. (2011).hierarchies for visual recognition. In NIPS’2010.
         Unsupervised learning of sparse features for scalable audio clas-Kingma, D. and LeCun, Y. (2010). Regularized estimation of image
         siﬁcation. In ISMIR’11.               statistics by score matching. In NIPS’2010.
        Hinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-Kivinen, J. J. and Williams, C. K. I. (2012).Multiple texture
         encoders. In ICANN’2011.              Boltzmann machines. In AISTATS’2012.
                                              ¨                 ¨        ¨
        Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A.,Kording, K. P., Kayser, C., Einhauser, W., and Konig, P. (2004).
         Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012).Howarecomplexcell properties adapted to the statistics of natural
         Deepneural networks for acoustic modeling in speech recognition.stimuli? J. Neurophysiology, 91.
         IEEE Signal Processing Magazine, 29(6), 82–97.Krizhevsky, A. (2010).Convolutional deep belief networks on
        Hinton, G. E. (1986). Learning distributed representations of con-CIFAR-10. Technical report, U. Toronto.
         cepts. In Proc. 8th Conf. Cog. Sc. Society, pages 1–12.Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of
        Hinton, G. E. (1999). Products of experts. In ICANN’1999.features from tiny images. Technical report, U. Toronto.
        Hinton, G. E. (2000). Training products of experts by minimizingKrizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet clas-
         contrastive divergence.Technical Report GCNU TR 2000-004,siﬁcation with deep convolutional neural networks. In NIPS’2012.
         Gatsby Unit, University College London.Larochelle, H. and Bengio, Y. (2008). Classiﬁcation using discrimi-
        Hinton, G. E. (2010).A practical guide to training restrictednative restricted Boltzmann machines. In ICML’2008.
         Boltzmann machines.Technical Report UTML TR 2010-003,Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009).
         Department of Computer Science, University of Toronto.Exploring strategies for training deep neural networks. Journal of
        Hinton, G. E. and Roweis, S. (2003). Stochastic neighbor embedding.Machine Learning Research, 10, 1–40.
         In NIPS’2002.                       Lazebnik, S., Schmid, C., and Ponce, J. (2006). Beyond bags of
        Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimension-features: Spatial pyramid matching for recognizing natural scene
         ality of data with neural networks. Science, 313(5786), 504–507.categories. In CVPR’2006.
        Hinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimumLe, H.-S., Oparin, I., Allauzen, A., Gauvin, J.-L., and Yvon, F. (2013).
         description length, and helmholtz free energy. In NIPS’1993.Structured output layer neural network language models for speech
        Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learningrecognition. IEEE Trans. Audio, Speech & Language Processing.
         algorithm for deep belief nets. Neural Computation, 18, 1527–Le, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A.
         1554.                                 (2010). Tiled convolutional neural networks. In NIPS’2010.
        Hubel, D. H. and Wiesel, T. N. (1959). Receptive ﬁelds of singleLe, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng,
         neurons in the cat’s striate cortex. Journal of Physiology, 148,A. (2011a).On optimization methods for deep learning.In
         574–591.                              ICML’2011.
                 ¨
        Hurri, J. and Hyvarinen, A. (2003).Temporal coherence, naturalLe, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y. (2011b). ICA with
         image sequences, and the visual cortex. In NIPS’2002.reconstruction cost for efﬁcient overcomplete feature learning. In
          ¨
        Hyvarinen, A. (2005).Estimation of non-normalized statisticalNIPS’2011.
         models using score matching. J. Machine Learning Res., 6.Le, Q. V., Zou, W. Y., Yeung, S. Y., and Ng, A. Y. (2011c). Learning
          ¨
        Hyvarinen, A. (2007). Some extensions of score matching. Compu-hierarchical spatio-temporal features for action recognition with
         tational Statistics and Data Analysis, 51, 2499–2512.independent subspace analysis. In CVPR’2011.
          ¨
        Hyvarinen, A. (2008). Optimal approximation of signal priors. NeuralLe Roux, N., Bengio, Y., Lamblin, P., Joliveau, M., and Kegl, B.
         Computation, 20(12), 3087–3110.       (2008a). Learning the 2-D topology of images. In NIPS’07.
          ¨
        Hyvarinen, A. and Hoyer, P. (2000).Emergence of phase andLeRoux,N.,Manzagol,P.-A.,andBengio,Y.(2008b). Topmoumoute
         shift invariant features by decomposition of natural images intoonline natural gradient algorithm. In NIPS’07.
         independent feature subspaces. Neural Computation, 12(7).LeCun, Y. (1986). Learning processes in an asymmetric threshold
          ¨
        Hyvarinen, A., Karhunen, J., and Oja, E. (2001a).Independentnetwork.In Disordered Systems and Biological Organization,
         Component Analysis. Wiley-Interscience.pages 233–240. Springer-Verlag.
          ¨
        Hyvarinen, A., Hoyer, P. O., and Inki, M. (2001b).TopographicLeCun, Y. (1987). Modeles connexionistes de l’apprentissage. Ph.D.
                                                          `
                                                       ´
         independent component analysis.Neural Computation, 13(7),thesis, Universite de Paris VI.
         1527–1558.                          LeCun, Y. (1989). Generalization and network design strategies. In
          ¨
        Hyvarinen, A., Hurri, J., and Hoyer, P. O. (2009). Natural ImageConnectionism in Perspective. Elsevier Publishers.
         Statistics: A probabilistic approach to early computational vision.LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E.,
         Springer-Verlag.                      Hubbard, W., and Jackel, L. D. (1989). Backpropagation applied
        Jaeger, H. (2007). Echo state network. Scholarpedia, 2(9), 2330.to handwritten zip code recognition. Neural Computation.
                                                                   ¨
        Jain, V. and Seung, S. H. (2008).Natural image denoising withLeCun, Y., Bottou, L., Orr, G. B., and Muller, K. (1998a). Efﬁcient
         convolutional networks. In NIPS’2008. backprop. In Neural Networks, Tricks of the Trade.
        Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009).LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b). Gradient
         What is the best multi-stage architecture for object recognition?based learning applied to document recognition. Proc. IEEE.
         In ICCV’09.                         Lee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net
                                                                                       29
          model for visual area V2. In NIPS’07.  Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007). Efﬁcient
        Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009a). Convolu-learning of sparse representations with an energy-based model. In
          tional deep belief networks for scalable unsupervised learning ofNIPS’2006.
          hierarchical representations. In ICML’2009.Ranzato, M., Boureau, Y., and LeCun, Y. (2008). Sparse feature
        Lee, H., Pham, P., Largman, Y., and Ng, A. (2009b). Unsupervisedlearning for deep belief networks. In NIPS’2007.
          feature learning for audio classiﬁcation using convolutional deepRanzato, M., Krizhevsky, A., and Hinton, G. (2010a). Factored 3-
          belief networks. In NIPS’2009.          way restricted Boltzmann machines for modeling natural images.
        Lin, Y., Tong, Z., Zhu, S., and Yu, K. (2010). Deep coding network.In AISTATS’2010, pages 621–628.
          In NIPS’2010.                          Ranzato, M., Mnih, V., and Hinton, G. (2010b). Generating more
        Lowe, D. (1999).Object recognition from local scale invariantrealistic images using gated MRF’s. In NIPS’2010.
          features. In ICCV’99.                  Ranzato, M., Susskind, J., Mnih, V., and Hinton, G. (2011). On deep
        Mallat, S. (2012). Group invariant scattering. Communications ongenerative models with applications to recognition. In CVPR’2011.
          Pure and Applied Mathematics.          Riesenhuber, M. and Poggio, T. (1999). Hierarchical models of object
        Marlin, B. and de Freitas, N. (2011).Asymptotic efﬁciency ofrecognition in cortex. Nature Neuroscience.
          deterministic estimators for discrete energy-based models: RatioRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a).
          matching and pseudolikelihood. In UAI’2011.Contractive auto-encoders: Explicit invariance during feature ex-
        Marlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010).traction. In ICML’2011.
          Inductive principles for restricted Boltzmann machine learning. InRifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin,
          AISTATS’2010, pages 509–516.            Y., and Glorot, X. (2011b). Higher order contractive auto-encoder.
        Martens, J. (2010). Deep learning via Hessian-free optimization. InIn ECML PKDD.
          ICML’2010, pages 735–742.              Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011c).
        Martens, J. and Sutskever, I. (2011).Learning recurrent neuralThe manifold tangent classiﬁer. In NIPS’2011.
          networks with Hessian-free optimization. In ICML’2011.Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012).A
        Memisevic, R. and Hinton, G. E. (2010). Learning to represent spatialgenerative process for sampling contractive auto-encoders.In
          transformations with factored higher-order Boltzmann machines.ICML’2012.
          Neural Comp., 22(6).                   Roweis, S. (1997). EM algorithms for PCA and sensible PCA. CNS
        Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow,Technical Report CNS-TR-97-02, Caltech.
          I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vin-Roweis, S. and Saul, L. K. (2000).Nonlinear dimensionality
          cent, P., Courville, A., and Bergstra, J. (2011). Unsupervised andreduction by locally linear embedding. Science, 290(5500).
          transfer learning challenge: a deep learning approach. In JMLRSalakhutdinov, R. (2010a). Learning deep Boltzmann machines using
          W&CP: Proc. Unsupervised and Transfer Learning, volume 7.adaptive MCMC. In ICML’2010.
        Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky,Salakhutdinov, R. (2010b). Learning in Markov random ﬁelds using
          J. (2011).Empirical evaluation and combination of advancedtempered transitions. In NIPS’2010.
          language modeling techniques. In INTERSPEECH’2011.Salakhutdinov, R. and Hinton, G. E. (2007). Semantic hashing. In
        Mobahi, H., Collobert, R., and Weston, J. (2009). Deep learning fromSIGIR’2007.
          temporal coherence in video. In ICML’2009.Salakhutdinov, R. and Hinton, G. E. (2009).Deep Boltzmann
        Mohamed, A., Dahl, G., and Hinton, G. (2012). Acoustic modelingmachines. In AISTATS’2009, pages 448–455.
          using deep belief networks. IEEE Trans. on Audio, Speech andSalakhutdinov, R. and Larochelle, H. (2010). Efﬁcient learning of
          Language Processing, 20(1), 14–22.      deep Boltzmann machines. In AISTATS’2010.
        Montufar, G. F. and Morton, J. (2012).When does a mixtureSalakhutdinov, R., Mnih, A., and Hinton, G. E. (2007). Restricted
          of products contain a product of mixtures?Technical report,Boltzmann machines for collaborative ﬁltering. In ICML 2007.
          arXiv:1206.0387.                       Savard, F. (2011). Reseaux de neurones a relaxation entraınes par
                                                             ´           `         ˆ ´
        Murray, I. and Salakhutdinov, R. (2009). Evaluating probabilities            ´
                                                  critere d’autoencodeur debruitant. Master’s thesis, U. Montreal.
                                                    `            ´
          under high-dimensional latent variable models.In NIPS’2008,Schmah, T., Hinton, G. E., Zemel, R., Small, S. L., and Strother,
          pages 1137–1144.                        S. (2009). Generative versus discriminative training of RBMs for
        Nair, V. and Hinton, G. E. (2010). Rectiﬁed linear units improveclassiﬁcation of fMRI images. In NIPS’2008, pages 1409–1416.
                                                   ¨                 ¨
          restricted Boltzmann machines. In ICML’10.Scholkopf, B., Smola, A., and Muller, K.-R. (1998).Nonlinear
        Neal, R. M. (1992).Connectionist learning of belief networks.component analysis as a kernel eigenvalue problem.Neural
          Artiﬁcial Intelligence, 56, 71–113.     Computation, 10, 1299–1319.
        Neal, R. M. (1993).Probabilistic inference using Markov chainSchwenk, H., Rousseau, A., and Attik, M. (2012). Large, pruned or
          Monte-Carlo methods. Technical Report CRG-TR-93-1, Dept. ofcontinuous space language models on a gpu for statistical machine
          Computer Science, University of Toronto.translation. In Workshop on the future of language modeling for
        Ngiam, J., Chen, Z., Koh, P., and Ng, A. (2011). Learning deepHLT.
          energy models. In Proc. ICML’2011. ACM.Seide, F., Li, G., and Yu, D. (2011a).Conversational speech
        Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-transcription using context-dependent deep neural networks. In
          cell receptive ﬁeld properties by learning a sparse code for naturalInterspeech 2011, pages 437–440.
          images. Nature, 381, 607–609.          Seide, F., Li, G., and Yu, D. (2011b).Feature engineering in
        Orr, G. and Muller, K.-R., editors (1998). Neural networks: tricks ofcontext-dependent deep neural networks for conversational speech
          the trade. Lect. Notes Comp. Sc. Springer-Verlag.transcription. In ASRU’2011.
        Pascanu, R. and Bengio, Y. (2013).Natural gradient revisited.Serre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007). Robust
          Technical report, arXiv:1301.3584.      object recognition with cortex-like mechanisms.IEEE Trans.
        Raiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning madePattern Anal. Mach. Intell., 29(3), 411–426.
          easier by linear transformations in perceptrons. In AISTATS’2012.Seung, S. H. (1998).Learning continuous attractors in recurrent
        Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007).networks. In NIPS’1997.
          Self-taught learning: transfer learning from unlabeled data.InSimard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices
          ICML’2007.                              for convolutional neural networks. In ICDAR’2003.
        Ranzato, M. and Hinton, G. H. (2010). Modeling pixel means andSimard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent
          covariances using factorized third-order Boltzmann machines. Inprop - A formalism for specifying selected invariances in an
          CVPR’2010, pages 2551–2558.             adaptive network. In NIPS’1991.
                                                                                30
        Simard, P. Y., LeCun, Y., and Denker, J. (1993). Efﬁcient patternrandom ﬁeld models. In UAI’2009.
         recognition using a new transformation distance. In NIPS’92.Welling, M., Hinton, G. E., and Osindero, S. (2003). Learning sparse
        Smolensky, P. (1986). Information processing in dynamical systems:topographic representations with products of Student-t distribu-
         Foundations of harmony theory. In D. E. Rumelhart and J. L.tions. In NIPS’2002.
         McClelland, editors, Parallel Distributed Processing, volume 1,Weston, J., Ratle, F., and Collobert, R. (2008). Deep learning via
         chapter 6, pages 194–281. MIT Press, Cambridge.semi-supervised embedding. In ICML 2008.
        Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical bayesianWeston, J., Bengio, S., and Usunier, N. (2010). Large scale image
         optimization of machine learning algorithms. In NIPS’2012.annotation: learning to rank with joint word-image embeddings.
        Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning,Machine Learning, 81(1), 21–35.
         C. D. (2011a). Dynamic pooling and unfolding recursive autoen-Wiskott, L. and Sejnowski, T. (2002). Slow feature analysis: Un-
         coders for paraphrase detection. In NIPS’2011.supervised learning of invariances. Neural Computation, 14(4),
        Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning,715–770.
         C. D. (2011b). Semi-supervised recursive autoencoders for pre-Younes, L. (1999).On the convergence of Markovian stochastic
         dicting sentiment distributions. In EMNLP’2011.algorithms with rapidly decreasing ergodicity rates. Stochastics
        Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learningand Stochastic Reports, 65(3), 177–228.
         with deep boltzmann machines. In NIPS’2012.Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using
        Stoyanov, V., Ropson, A., and Eisner, J. (2011).Empirical riskdeep-structured conditional random ﬁelds. IEEE Journal of Se-
         minimization of graphical model parameters given approximatelected Topics in Signal Processing.
         inference, decoding, and model structure. In AISTATS’2011.Yu, K. and Zhang, T. (2010). Improved local coordinate coding using
        Sutskever, I. (2012).Training Recurrent Neural Networks. Ph.D.local tangents. In ICML’2010.
         thesis, Departement of computer science, University of Toronto.Yu, K., Zhang, T., and Gong, Y. (2009). Nonlinear learning using
        Sutskever, I. and Tieleman, T. (2010). On the Convergence Propertieslocal coordinate coding. In NIPS’2009.
         of Contrastive Divergence. In AISTATS’2010.Yu, K., Lin, Y., and Lafferty, J. (2011). Learning image representa-
        Sutskever, I., Hinton, G., and Taylor, G. (2009).The recurrenttions from the pixel level via hierarchical sparse coding. In CVPR.
         temporal restricted Boltzmann machine. In NIPS’2008.Yuille, A. L. (2005). The convergence of contrastive divergences. In
        Swersky, K. (2010). Inductive Principles for Learning RestrictedNIPS’2004, pages 1593–1600.
         Boltzmann Machines.Master’s thesis, University of BritishZeiler, M., Krishnan, D., Taylor, G., and Fergus, R. (2010). Decon-
         Columbia.                             volutional networks. In CVPR’2010.
        Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas,Zou, W. Y., Ng, A. Y., and Yu, K. (2011). Unsupervised learning
         N. (2011). On score matching for energy based models: Gen-of visual invariance with temporal coherence.In NIPS 2011
         eralizing autoencoders and simplifying deep learning. In Proc.Workshop on Deep Learning and Unsupervised Feature Learning.
         ICML’2011. ACM.
        Taylor, G. and Hinton, G. (2009). Factored conditional restricted
         Boltzmann machines for modeling motion style. In ICML’2009.
        Taylor, G., Fergus, R., LeCun, Y., and Bregler, C. (2010). Convolu-
         tional learning of spatio-temporal features. In ECCV’10.
        Tenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global
         geometric framework for nonlinear dimensionality reduction. Sci-
         ence, 290(5500), 2319–2323.
        Tieleman, T. (2008). Training restricted Boltzmann machines using
         approximations to the likelihood gradient. In ICML’2008, pages
         1064–1071.
        Tieleman, T. and Hinton, G. (2009). Using fast weights to improve
         persistent contrastive divergence. In ICML’2009.
        Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal
         components analysis. J. Roy. Stat. Soc. B, (3).
        Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M.,
         Briggman, K., Denk, W., and Seung, H. S. (2010). Convolutional
         networks can learn to generate afﬁnity graphs for image segmen-
         tation. Neural Computation, 22, 511–538.
        van der Maaten, L. (2009). Learning a parametric embedding by
         preserving local structure. In AISTATS’2009.
        van der Maaten, L. and Hinton, G. E. (2008). Visualizing data using
         t-SNE. J. Machine Learning Res., 9.
        Vincent, P. (2011).A connection between score matching and
         denoising autoencoders. Neural Computation, 23(7).
        Vincent, P. and Bengio, Y. (2003). Manifold Parzen windows. In
         NIPS’2002. MIT Press.
        Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008).
         Extracting and composing robust features with denoising autoen-
         coders. In ICML 2008.
        Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol,
         P.-A. (2010).Stacked denoising autoencoders: Learning useful
         representations in a deep network with a local denoising criterion.
         J. Machine Learning Res., 11.
        Weinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of
         image manifolds by semideﬁnite programming. In CVPR’2004,
         pages 988–995.
        Welling, M. (2009). Herding dynamic weights for partially observed
