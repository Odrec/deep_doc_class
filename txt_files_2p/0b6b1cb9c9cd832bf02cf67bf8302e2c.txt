IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

2109

Improved Quasi-Newton Adaptive-Filtering

Algorithm

Md Zulﬁquar Ali Bhotto, Student Member, IEEE, and Andreas Antoniou, Life Fellow, IEEE

Abstract—An improved quasi-Newton (QN) algorithm that per-
forms data-selective adaptation is proposed whereby the weight
vector and the inverse of the input-signal autocorrelation matrix
are updated only when the a priori error exceeds a prespeciﬁed
error bound. The proposed algorithm also incorporates an im-
proved estimator of the inverse of the autocorrelation matrix. With
these modiﬁcations, the proposed QN algorithm takes signiﬁcantly
fewer updates to converge and yields a reduced steady-state mis-
alignment relative to a known QN algorithm proposed recently.
These features of the proposed QN algorithm are demonstrated
through extensive simulations. Simulations also show that the
proposed QN algorithm, like the known QN algorithm, is quite
robust with respect to roundoff errors introduced in ﬁxed-point
implementations.

Index Terms—Adaptation algorithms, adaptive ﬁlters, conver-
gence speed in adaptation algorithms, quasi-Newton algorithms,
steady-state misalignment.

I. INTRODUCTION

T HE least-mean-squares (LMS) algorithm minimizes the

Weiner-Hopf function iteratively by using the instanta-
neous values of the autocorrelation function of the input signal
and the crosscorrelation function between the input and desired
signals [1]. Due to its simplicity, the LMS algorithm is fre-
quently used in current practice. However, when the input signal
is highly colored or bandlimited, the LMS algorithm as well as
other algorithms of the steepest-descent family converge slowly
and the capability of such algorithms in tracking nonstationar-
ities deteriorates. In such situations, more sophisticated algo-
rithms that belong to the Newton family are preferred. However,
the computational complexity of these algorithms is usually pro-
hibitively large especially in real-time applications where low-
cost digital hardware must be employed. Numerical instability
is also a major issue in these algorithms. The conventional re-
cursive least-squares (CRLS) algorithm converges much faster
than algorithms of the steepest-descent family [1]. However, it
can become unstable and if a large forgetting factor is chosen
it can actually lose its tracking capability. The known quasi-
Newton (KQN) algorithm reported in [2], [3] offers better nu-
merical robustness whereas the LMS-Newton (LMSN) algo-

Manuscript received April 14, 2009; revised September 26, 2009; accepted
November 11, 2009. Date of publication February 05, 2010; date of current
version August 11, 2010. This work was supported by the Natural Sciences and
Engineering Research Council of Canada. This paper was recommended by As-
sociate Editor H. Johansson.

The authors are with the Department of Electrical and Computer Engineering,
University of Victoria, Victoria, BC V8W 3P6, Canada (e-mail: zbhotto@ece.
uvic.ca, aantoniou@ieee.org).

Digital Object Identiﬁer 10.1109/TCSI.2009.2038567

rithms reported in [4] offer better convergence performance than
the CRLS algorithm.

Two methods are available for the development of Newton-
type adaptation algorithms: methods based on the direct solu-
tion of the normal equations of a least-squares problem (see
Section II) and methods based on the orthogonal decomposition
of the input-signal matrix. The CRLS, KQN, and the LMSN
algorithms are based on the normal equations and the QR de-
composition algorithm (QRD) is based on the orthogonal de-
composition of the input-signal matrix [5]. The computational
designated as
complexity of these algorithms is of order
. The fast QRD (FQRD) algorithms in [6]–[12] are ac-
tually efﬁcient implementations of the QRD algorithm whose
. Fast RLS (FRLS) algo-
computational complexity is of
are also avail-
rithms with computational complexities of
able in the literature for FIR adaptive ﬁltering and autoregres-
sive (AR) prediction [5], [13]–[16]. FRLS algorithms exploit
the Toeplitz structure of the input-signal autocorrelation ma-
trix. The fast QN (FQN) algorithm reported in [17], which has a
, also exploits the Toeplitz
computational complexity of
structure of the autocorrelation matrix to reduce the computa-
tional complexity. The FRLS and FQRD algorithms suffer from
numerical instability problems that are inherited from the nu-
merical instability problems of the CRLS and QRD algorithms,
respectively, and also the simpliﬁcations used to obtain these
algorithms [8]. However, the FQRD algorithm reported in [8]
offers numerically stable operation in low-precision implemen-
tations and in the absence of persistent excitation. The numer-
ical instability problems associated with the CRLS algorithm
are discussed in [18] where an upper bound on the relative pre-
cision to assure the BIBO stability of the CRLS algorithm in
stationary and nonstationary environments is derived. Formulas
for choosing the forgetting factor to avoid explosive divergence
for a given precision in the CRLS algorithm are also given in
[18]. However, these formulas were derived on the assumption
that the input signal is persistently exciting. Furthermore, the
input-signal statistics must be known a priori in order to use
these formulas. Consequently, a prudent strategy for the deriva-
tion of fast Newton-type algorithms would be to start with a
parent algorithm that is inherently stable. The numerical robust-
ness of the quasi-Newton (QN) algorithm reported in [2], [3] is
achieved by using a biased estimate of the autocorrelation ma-
trix, which can reduce the tracking capability of the algorithm
relative to that of the CRLS algorithm (see [19, p. 678]).

In this paper, we propose an improved version of the QN algo-
rithm reported in [2], [3] that incorporates data-selective adap-
tation. The proposed QN (PQN) algorithm takes fewer weight
updates to converge and yields a reduced steady-state misalign-

1549-8328/$26.00 © 2010 IEEE

2110

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

is the step size and

is an estimate of the inverse
where
of the autocorrelation matrix at iteration . The a posteriori error
at iteration

is deﬁned as

(4)

is determined by minimizing

In the KQN algorithm,
with respect to
selective weight adaptation is performed whereby
only when the magnitude of the a priori error
prespeciﬁed error bound . In other words, whenever

in every iteration. In the PQN algorithm, data-
is updated
is greater than a
,
. The associated

is chosen to force the equality

optimization problem can be stated as

Fig. 1. Adaptive ﬁlter.

Straightforward analysis leads to

if
otherwise

(5)

(6)

ment relative to the KQN algorithm in [2], [3]. These features of
the new algorithm are demonstrated through MATLAB simula-
tions in stationary and nonstationary environments. Simulations
also show that the PQN algorithm, like the KQN algorithm, is
quite robust with respect to roundoff errors in ﬁxed-precision
implementations.

The paper is organized as follows. In Section II, the pro-
posed QN algorithm is described. Performance analysis is car-
ried out in Section III. Simulation results for stationary and non-
stationary environments are given in Section IV. Finally, con-
clusions are drawn in Section V.

II. PROPOSED QUASI-NEWTON ALGORITHM

The simplest and most commonly used conﬁguration
the tapped delay-line structure
for adaptive ﬁltering is
illustrated in Fig. 1, which is essentially an FIR dig-
and
ital ﬁlter. Vectors
represent the weight and
input-signal sequences at iteration , respectively. The a priori
error signal at iteration

is given by

where
weight vector,
optimization problem

is the desired signal. Adaptation algorithms update the
, in such a way as to obtain the solution of the

(1)

where
is the expectation operator. This is usually referred
to as the Wiener solution. Adaptation algorithms use either the
steepest-descent or the Newton direction. LMS-type algorithms
use the steepest-descent direction and, therefore, their conver-
gence rate depends on the nature of the objective function and
spread of the eigenvalues of the Hessian matrix of the objective
function. Most algorithms of the Newton family use the update
formula

holds true during each update, we use

and

(2)

(8)

(9)

(10)

where

and

if
otherwise

As in the KQN algorithm in [3], the PQN algorithm also uses the
rank-one quasi-Newton updating formula given in Eq. (7.20) in
[20] to deduce the update of the inverse of the autocorrelation
matrix as

(7)

The estimator in (7) satisﬁes the classical QN hereditary con-
dition and, therefore, the associated algorithm belongs to the
QN family according to Fletcher’s classiﬁcation in [21]. For the
adaptation of
along
the Newton direction

is chosen to be in proportion to

. Since the relation

,

Unfortunately, in the context of adaptive ﬁltering, gradient
not available at iteration since it requires future data

is
and

. However, parameter

in (10) can be approximated as

(3)

(11)

