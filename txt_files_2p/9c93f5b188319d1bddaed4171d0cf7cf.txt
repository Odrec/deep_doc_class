Information Sciences 180 (2010) 3434–3443

Contents lists available at ScienceDirect

Information Sciences

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / i n s

Covariance intersection based image fusion technique with
application to pansharpening in remote sensing
Qing Guo a,b, Siyue Chen b, Henry Leung b, Shutian Liu a,*
a Harbin Institute of Technology, Department of Physics, Harbin 150001, PR China
b Department of Electrical and Computer Engineering, University of Calgary, Calgary, Alberta, Canada T2N 1N4

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 14 November 2009
Received in revised form 29 April 2010
Accepted 6 May 2010

Keywords:
Covariance intersection
Image fusion
Expectation maximization
Multi-spectral image
Panchromatic image
Pansharpening

1. Introduction

Image fusion of multi-spectral images and panchromatic images has been widely applied
to imaging sensors. Multi-spectral images are rich in spectral information whereas pan-
chromatic images have relatively higher spatial resolution. In this paper, we consider the
image fusion as an estimation problem, that is to estimate the ideal scene of multi-spectral
images at the resolution of panchromatic images. We propose a method of combining the
covariance intersection (CI) principle with the expectation maximization (EM) algorithm to
develop a novel image fusion approach. In contrast to other fusion methods, the proposed
scheme takes cross-correlation among data sources into account, and thus provides consis-
tent and accurate estimates through convex combinations. Since the covariance informa-
tion is usually unknown in practice, the EM method is employed to provide a maximum
likelihood estimate (MLE) of the covariance matrix. Real multi-spectral and panchromatic
images are used to evaluate the effectiveness of the proposed EM–CI method. The proposed
algorithm is found to preserve both the spectral information of the multi-spectral image
and the high spatial resolution information of the panchromatic image more effectively
than the conventional image fusion techniques.

Ó 2010 Elsevier Inc. All rights reserved.

With the rapid growth of the internet and other electronic sources of information, the problem of coherent merging infor-
mation from multiple sources has become an important issue [21,30,31]. As one type of information processing technique,
the image fusion is widely used in many practical applications. Images captured by multiple sensors often contain comple-
mentary and redundant information. An effective fusion process will result in an improved image, which contains more com-
plete information and should be more suitable for human visual perception and object recognition [2,20]. In this study, we
consider fusion of multi-spectral (MS) images and panchromatic (Pan) images, which are widely used in remote sensing. This
fusion scheme is often referred as pansharpening [13]. The MS images, such as acquired by IKONOS and QuickBird satellites,
usually have four bands of spectral information (i.e., red (R), green (G), blue (B), and near infrared (NIR)). Meanwhile, the Pan
image has good spatial resolution. Therefore, the goal of fusing these images is to generate a high-resolution MS image,
which can be used more effectively for applications, such as land classiﬁcation and road detection.

Image fusion can be classiﬁed as pixel-level, feature-level and symbol-level [3]. Fusion of MS and Pan images at pixel-le-
vel is traditionally handled by the intensity-hue-saturation (IHS) transform methods [4,6], the Brovey transform methods
[7], the principal component analysis (PCA) methods [6,5,24], the highpass ﬁltering (HPF) method [25] and by the wavelet

* Corresponding author. Tel./fax: +86 451 86418042.

E-mail address: stliu@hit.edu.cn (S. Liu).

0020-0255/$ - see front matter Ó 2010 Elsevier Inc. All rights reserved.
doi:10.1016/j.ins.2010.05.010

Q. Guo et al. / Information Sciences 180 (2010) 3434–3443

3435

transform (WT) method [9,14,23,33]. The IHS transform method ﬁrst transforms three MS bands from RGB color space to the
spatial (I) and spectral (H, S) information space. Fusion is then performed by replacing the intensity components in the IHS
space with the Pan image. The IHS method can preserve the spatial resolution of the Pan image, however, according to Prasad
et al. [22], it severely distorts the spectral information of the MS image. The method based on Brovey transform is a simple
color normalized method. It also usually introduces distortion to the spectral information. The same problem of spectral dis-
tortion is also found in the PCA method because fusion is performed by replacing the ﬁrst principal component of the MS
image with the Pan image, which results in a signiﬁcant loss of spectral information. The HPF method simply adds the
high-frequency components of the Pan image to the MS image. However, choosing a proper ﬁlter size is difﬁcult in this
method.

The fusion method based on WT is also popular. Zhou et al. [33] employ WT to fuse landsat TM and SPOT Pan images. The
SPOT image and each spectral band of the TM images are decomposed into an orthogonal wavelet representation at a given
resolution, which consists of an approximation image and a set of spatially-oriented detailed images. Band by band, the
approximation image from each TM band is combined with the detailed image from SPOT. Then the inverse WT is performed
to obtain the fused image. The spectral and spatial quality of the fused image based on this method is better than those based
on the IHS transform, PCA, and the Brovey transform [33].

In this paper, our motivation is to generate a high resolution MS image with more information and better quality than
those provided by individual sensor image alone. The image fusion problem is considered as a statistical estimation problem.
More speciﬁcally, the objective is to estimate the ideal high-resolution MS image from the observed MS image and the ob-
served Pan image. To this end, covariance intersection (CI), which has been widely used in radar track fusion [28], is em-
ployed. CI can produce consistent estimate for any degree of cross-correlation between the input sources through convex
combination [12,18]. In addition, CI enforces estimation consistency by means of convex combination of the inverse of
covariance matrices. When CI is applied to image fusion, it treats the ideal high-resolution MS image as a linear combination
of the ideal MS and Pan images and the fused coefﬁcients are selected through the convex combination of weight optimiza-
tion. However, many image fusion methods, such as conventional WT, choose the fused coefﬁcients simply by selecting lar-
ger or by replacement [32,19,15], without using any rigorous procedure.

To use the convex combination in CI, the covariance information of the estimation error is required. But for most imaging
applications, this information is not available. Here, we use the expectation maximization (EM) to give a maximum likeli-
hood estimate (MLE) of the covariance information. Formalized by Dempster et al. [8], the EM algorithm is an iterative pro-
cedure that estimates both the parameters and the missing or unobservable data during an iteration. The approach ﬁrst
computes the approximation to the expectation of the log-likelihood function of the complete data conditioned on the cur-
rent parameter estimate, which is referred as the expectation (E-step). In this step, the current incomplete data estimate is
calculated. Next, a new parameter estimate is computed by ﬁnding the value of the parameter that maximizes the function
found in the E-step. This is called the maximization step (M-step). In this study, the ideal MS and Pan images are treated as
the missing data, and the covariances of them are treated as the parameters to be estimated by EM.

The remainder of this paper is organized as follows. The problem of fusing the MS image and the Pan image is formu-
lated in Section 2. This formulation justiﬁes the capability of CI to give a consistent and unbiased estimation for any de-
gree of the cross-correlation, and can be used to optimally fuse the images. In Section 3, the EM is developed to help
performing CI and the fast performing CI with weight in a suboptimal linear way to give the consistent estimation through
convex combination is presented. Performance evaluation of the EM–CI method using real image sensory data is accom-
plished in Section 4 and shows that the EM–CI method preserves the spectral information very well. Section 5 concludes
this paper.

2. Problem formulation

The observed MS image and the observed Pan image are denoted as ~x1 and ~x2, respectively, which are the vectors com-
posed of pixel values. To simplify the derivation, one band of the MS image as ~x1 is used for the derivation example, the other
bands have the same expressions. Due to sensor distortion and noise in the capture process of MS and Pan images, the ob-
served images can be modeled as ‘‘linear observation plus Gaussian noise” [10,16]. That is,

i ¼ 1; 2;

~xi ¼ Hixi þ ni;

ð1Þ
where xi denotes the vector of pixel values from an ideal image, Hi represents the linear observation operator which is a
known observation matrix. Typical examples of the linear observation operator Hi include optical blur, motion blur, tomo-
graphic projections. ni is assumed to be zero-mean white Gaussian noise with covariance
Pxixi which is deﬁned in the next
paragraph.

b

From (1), it is noted that neither the ideal image xi nor its statistics are available. However, the consistent estimate ^xi can

be obtained. That is, if we deﬁne

b
Pxixi ¼ ð~xi   ^xiÞð~xi   ^xiÞT ;

i ¼ 1; 2:

and

Pxixi ¼ E½ðxi   ^xiÞðxi   ^xiÞT;

i ¼ 1; 2;

ð2Þ

ð3Þ

