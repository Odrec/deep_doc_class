Word Learning, Intentions and 

Bayes’ Rule 

Based on 

“Using Speakers’ Referential Intentions to 
Model Early 
Cross-SituationalWord Learning - Michael C. 
Frank, Noah D. Goodman, and Joshua B. 
Tenenbaum (2009)” 

By 

Arushi Garg 

Instructors 

Prof. Dr. Frank Jäkel 
Dr. Mingya Liu 

Course 

Word Meaning – From Logic to Natural 
Language – Seminar 

Term  

Winter Term 2014/15 

Overview 

• Traditional Models & Their Problems 
• Intentional Model 
• Testing the Intentional Model 
• Discussion 
• Questions 

 

Clear your head! 

Cross-situational/Associative Word 

Learning 

• मेज़ पे सेब रखा है  

MEZ PE SEB RAKHA HAI 

 
• सेब खाओ  
     SEB KHAO 
 
• देखो, सेब! 
     DEKHO, SEB! 

Cross-situational/Associative Word 

Learning 

• मेज़ पे सेब रखा है  

MEZ PE SEB RAKHA HAI 
The apple is on the table 

 
• सेब खाओ  
     SEB KHAO 
Eat apple 
 
• देखो, सेब! 
     DEKHO, SEB! 

See, apple! 

The Chicken and Egg problem 

Meaning 

Intention 

Two Strategies – Focus on single aspect 

Intention 
Known 

Social Theories 

Cross-

situational 

strategy 

Intention 
Irrelevant 

Two Strategies – Focus on single aspect 

• Awareness of 

goal and 
intentions of 
speaker 

• Mappings 

between words 
and referent 
easy to learn 
 

Social Theories 

Two Strategies – Focus on single aspect 

Cross-

situational 

strategy 

• Words refer to 
local context ( 
immediate 
environment) 

• Consistent 
association 
between words 
and referent 

The Chicken and Egg problem 

Meaning 

Intention 

Intentional Model 

Captures both aspects - simultaneously 
infers what speakers are attempting to 

communicate and learns a lexicon 

Side-Note – What is Lexicon? 

HOUSE 

• The vocabulary of a 

person, language, 
or branch of 
knowledge 

• Refers to the 

mapping between 
words and their 
meanings 

Side-Note – What is Lexicon? 

Bird 

Cow 

Duck 

Hat 

Eyes 

Book 

Side Note – Object, Referential Intention, 

Word 

Basic level object categories in the 

local context , immediate environment 

or current situation 

Intended object in the local context 

Actual words used by the speaker 

Intentional Model - Assumptions 

Intention  

Words  

Function of physical 

surroundings  

F(Objects) 

Function of Intention and 

Lexicon 

F ( Intentions & Lexicon) 

After many many situations 

corpus C  

( collection of 

situations 

encountered) 

Hugh sample space 
of possible Lexicons 

 

Intentional Model 

C 

Based on  

We need to find the 

most probable 

Lexicon L 

 

Intentional Model 

P(L|C) 

a 

P(C|L). P(L) 

Intentional Model 

P(L|C) 

a 

P(C|L) . P(L) 

P(L)     a    e –a|L| 
  
probable as more word – object 

Lexicons become exponentially less 

pairings are included 

Important points about the model that were 

coded in mathematically 

• Distinction between words used to refer to 

objects and those used non-referentially 
• All intentions are equally likely in a  given 

context 

• An utterance could be said with an empty 

intention 
 

Testing the Intentional Model - Method 

Objects 

Lexicon 

Video 
Files 

Corpus C of 
different 
situations S 

Words 

Intention 

Mathematically coded into 

model as sets for each 
Situation S in the corpus 

Inferred on the basis of 
highest probability from 

all possible values 

Testing the Intentional Model - Method 

Objects 

Lexicon 

Video 
Files 

Corpus C of 
different 
situations S 

Words 

Coded 

Intention 

Calculated 

Testing the Intentional Model - Method 

• These models include: 

– IBM Machine Translation 

Model I 

–  the statistical machine-
translation model used 
by Yu and Ballard 

• Evaluation based on 
accuracy of lexicons 
learned and inferences 
drawn regarding the 
speaker’s intent 

 

• Evaluated by Intentional 

Models and several 
other models of cross-
situational word 
learning for 
comparison. 

 

Testing the Intentional Model – Method I – 

Gold Standard Comparison 

Gold standard 

for lexicons 

Gold standard 
for intentions 
Designed by 
human coder 

Accuracy 

comparison 

against 

Lexicon 

Intention 

Calculated 

Testing the Intentional Model –  

Results I – Gold Standard Comparison 

Testing the Intentional Model –  

Results I – Gold Standard Comparison 

Testing the Intentional Model  -  

Results I – Gold Standard Comparison 

Testing the Intentional Model –  

Results  

• The best lexicons found by simple statistical 

and translation models were considerably 
larger than that by Intentional model – many 
spurious lexical items found 
 

Testing the Intentional Model - Note 

• Possible reason for great precision of 

Intentional Model: 
– distinction between referential and nonreferential 

words allowed our model to exclude from the 
lexicon words that were used without a consistent 
referent 

– the ability of the model to infer an empty 

intention allowed it to discount utterances that 
did not contain references to any object in the 
immediate context   

 

Testing the Intentional Model - Note 

• Intentional model prefers certain kinds of 

lexicons and utterance interpretations 
– Sparse lexicons 
– One to one lexicons 
– Prefer intentions about objects present in the 

local context   

 

Testing the Intentional Model -  

Method II - Prediction of Experimental 

Results 

• Many kinds of phenomenon that are observed 
with early word learning were tested with the 
all these models to judge and compare their 
predictability. These include: 
– Cross-Situational Word Learning 
– Mutual Exclusivity 
– One-Trial Learning 
– Object Individuation 
– Intention Reading 

 
 

Testing the Intentional Model -  

Method II - Prediction of Experimental 

Results 

• Many kinds of phenomenon that are observed 
with early word learning were tested with the 
all these models to judge and compare their 
predictability. These include: 
– Cross-Situational Word Learning 
– Mutual Exclusivity 
– One-Trial Learning 
– Object Individuation 
– Intention Reading 

 
 

Testing the Intentional Model  -  
Side Note – Mutual Exclusivity 

Can you 

hand 
me the 
DAX ? 

BIRD (KNOWN) 

NOVEL OBJECT 

Testing the Intentional Model  -  
Results II – Mutual Exclusivity 

• Children possess principle of Mutual 

Exclusivity 

• So it leads them to prefer lexicons with only 

one label for each object 
– Similar to the intentional model 

Testing the Intentional Model  -  
Results II – Mutual Exclusivity 

Discussion 

• Things to note 

– Operates at Marr’s computational theory level of 

explanation 

• Clear structure of learner’s a learner’s assumptions in terms 
of relationships between observed and unobserved variables 

– No claims about nature of mechanisms 
– Kind of ideal-observer analysis  

• Supports hypothesis that specialized principles 

may not be necessary to explain many of the 
smart inferences that young children are able to 
make in learning words. 
– Representation of speaker’s intentions may suffice 
 
 

Discussion 

• Merits: 

– Performs well in learning words from a natural 

corpus 

– predicts a variety of behavioral phenomena 

reported in the word-learning literature 

– First word learning model to incorporate both 

behavioral coverage and corpus data 

 
 

Discussion 

• Impetus for future work/shortcomings: 

– Partial account for word learning process ( ideal 

observer analysis) - other computational theories 
can provide further insight into different aspects 

– Does not describe in any way the nature of 

mechanisms behind this learning of words or 
intentions 

 
 

Discussion 

• Associative and Cross-situational learning 

models might very well explain the missing 
mechanisms 

Questions 

Intentional Model 

P(L|C) 

a 

P(C|L) . P(L) 

P(C|L)     =  P   P(Ws, Os, Is|L)                       
  

Words, Objects and Intentions are 

s e C 

components of the corpus C of 

Situations S 

Intentional Model 

P(C|L)     =  P   P(Ws, Os, Is|L)                       
  

s e C 

• W & O conditionally independent given I 
• P(Ws, Os, Is|L) =   P(Ws|Is, L) . P (Is| Os) 
• Sum up over all intentions  
• Isare equally likely 
• Is can be empty 

 
 

Intentional Model 

P(C|L)     =  P  S    P(Ws|Is, L) . P (Is| Os)                       
  

s e C  Is ⊆ C 

g 

Word 

 1 - g 

Referential utterance of 

the word 

• Probability of referential 
utterance (PR)= Probability 
of being chosen from 
lexicon to refer to any of 
the intended referents 
 

Nonreferential utterance 

of the word 

• Probability of 
nonreferential utterance 
(PNR)= Probability of being 
randomly chosen from 
lexicon 

Intentional Model 

P(C|L)     =  P  S    P(Ws|Is, L) . P (Is| Os)                       
  

s e C  Is ⊆ C 

 P  [g.   S      {PR(w|o, L) } / | Is| 

o e Is 

weWs 
  
           + (1-g) . PNR(w|L)]  
  

Testing the Intentional Model  -  
Side Note– Object Individuation 

• One Word Condition 

“Look! A 

toy!” 

“Look! A 

toy!” 

Testing the Intentional Model  -  
Side Note– Object Individuation 

• Two Word Condition 

“Look! A 
duck!” 

“Look! A 

ball!” 

Testing the Intentional Model  -  
Side Note– Object Individuation 

• Test Condition I 

Testing the Intentional Model  -  
Side Note– Object Individuation 

• Test Condition II 

Testing the Intentional Model  -  
Side Note– Object Individuation 

• Infants in one word condition look longer ( 

express surprise) when 2 objects are presented 
• Infants in two word condition look longer when 

1 object is presented 

• Interpretation  
• – because of one to one mappings – infants 

expect one object to show when one word is 
said and two objects when two words are said 
 

Testing the Intentional Model –  
Method II – Object Individuation 

• Created different sets of situations 
• Two construals for each set 

–  one word condition and two word condition  
• Evaluated each construals for all possible 
lexicons using surprisal ( measure linking 
model probabilities to reaction time – hence 
assessing surprise) 

Testing the Intentional Model –  
Results II – Object Individuation 

• Higher surprisal for mismatch words ( same as 

actual data) 
 

Testing the Intentional Model –  
Side Note – Intention Reading 

“TEDDY 
BEAR” 

Testing the Intentional Model –  
Side Note – Intention Reading 

Testing the Intentional Model –  
Side Note – Intention Reading 

Testing the Intentional Model –  
Side Note – Intention Reading 

• Despite the greater temporal contiguity 

between the label and the second toy, the 
children showed evidence of learning that the 
label corresponded to the first toy. 
 

Testing the Intentional Model –  
Method II – Intention Reading 

• Constructed a situation with two novel objects 

and one novel word 

• Additional information of intention of speaker 

to refer to first object provided 

Testing the Intentional Model –  
Results II – Intention Reading 

• model then highly preferred the correct 

pairing. 
– Not surprising as intent information was provided 

• Models that rely on perceptual salience fail 
here as less salient object is the correct one. 

Testing the Intentional Model  -  

Results II – Cross Situational Word 

Learning 

• No differences in precision was seen among 
different models in case of Cross Situational 
Word Learning 

• All models were equally precise in 

– Finding correct word – object pairings 
– Recalling when stimulus was presented 

