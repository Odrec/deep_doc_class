Public Opinion Quarterly, Vol. 79, No. 4, Winter 2015, pp. 952–975

COMPARING EXTREME RESPONSE STYLES BETWEEN 
AGREE-DISAGREE AND ITEM-SPECIFIC SCALES

MINGNAN LIU*
SUNGHEE LEE
FREDERICK G. CONRAD

Abstract  Although Likert scales in agree-disagree (A/D) format are 
popular  in  surveys,  the  data  quality  yielded  by  them  is  controversial 
among  researchers.  Recognizing  the  measurement  issues  involved 
with  the A/D  format,  researchers  have  developed  other  question  for-
mats to measure attitudes. In this study, we focused on an alternative 
question type, the item-specific (IS) question, which asks the respond-
ent to choose an option that best describes his or her attitude. Using 
political  efficacy  items  from  the American  National  Election  Studies 
(ANES), we compare extreme response style (ERS) between A/D and 
IS scales. Latent class factor analysis showed that ERS exists in both 
A/D  and  IS  scale  formats,  but  differs  slightly  across  the  two. Also, 
when analyzing ERS within subjects across two waves, there is only a 
single ERS for both question formats, after controlling for the correla-
tion within respondents. The last finding suggests that ERS is a stable 
characteristic.

Mingnan Liu is a survey scientist at SurveyMonkey, Palo Alto, CA, USA. Sunghee Lee is 
an assistant research scientist at Survey Research Center, University of Michigan, Ann Arbor, 
MI, USA. Frederick G. Conrad is a research professor and director of the Program in Survey 
Methodology,  Institute  for  Social  Research,  University  of  Michigan, Ann Arbor,  MI,  USA, 
and  the  Joint  Program  in  Survey  Methodology,  University  of  Maryland,  College  Park,  MD, 
USA, and a professor of psychology at the University of Michigan, Ann Arbor, MI, USA. The 
authors thank Norbert Schwarz, Yu Xie, and the editors and reviewers for their feedback on 
previous  drafts  of  this  manuscript.  This  work  was  partially  supported  by  the  Rensis  Likert 
Fund  in  Research  in  Survey  Methodology,  Program  in  Survey  Methodology,  University  of 
Michigan. American National Election Studies data were collected by Stanford University and 
the  University  of  Michigan,  supported  by  the  National  Science  Foundation  [grant  numbers 
SES-0937715  and  SES-0937727]. Any  opinions,  findings,  and  conclusions  or  recommenda-
tions expressed in this study are those of the authors and do not necessarily reflect the views of 
the funding organizations. *Address correspondence to Mingnan Liu, SurveyMonkey, 101 Lytton 
Avenue, CA, 94301, USA; e-mail: mingnanl@surveymonkey.com.

doi:10.1093/poq/nfv034 
Advance Access publication August 14, 2015
© The Author 2015. Published by Oxford University Press on behalf of the American Association for Public Opinion Research. 
All rights reserved. For permissions, please e-mail: journals.permissions@oup.com

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

953

When designing survey questions to measure opinions and attitudes, one of 
the first ideas that comes to a researcher’s mind is probably a Likert scale. As 
Bradburn, Sudman, and Wansink (2004) point out, attitudes comprise cogni-
tions,  evaluations,  and  behaviors. The  cognitive  component  is  one’s  beliefs 
about the target object. The evaluative component is one’s evaluation of the 
object.  The  behavioral  component  is  the  connection  between  attitude  and 
behavior. A Likert scale can be used to measure both the respondent’s evalua-
tion (agree vs. disagree) of the rating object and the strength of the evaluation 
(strongly  vs.  somewhat). That  is,  it  measures  the  intensity  of  one’s  attitude 
toward  an  object  (Krosnick  and Abelson  1992).  Furthermore,  it  may  seem 
easy to design Likert scales, since researchers have only to create questions 
describing  the  objects  they  want  to  measure  without  varying  the  response 
options.  These  advantages  make  Likert  scales  very  popular  among  social 
scientists  and  marketing  researchers.  For  example,  the  Marketing  Scales 
Handbook and other similar references provide numerous citations to publica-
tions using Likert scales (Bearden and Netemeyer 1999; Bruner, Hensel, and 
James 2001).

Although they are popular in many fields, the data quality yielded by Likert 
scales  in  the  agree-disagree  (A/D)  format  is  controversial  among  research-
ers (for examples, see Clark and Clark 1977; Billiet and McClendon 2000; 
Fowler and Cosenza 2008; DeVellis 2011; Revilla, Saris, and Krosnick 2013). 
Some  survey  methodologists  have  even  argued  that  “researchers  will  have 
more  reliable,  valid,  and  interpretable  data  if  they  avoid  the  agree-disagree 
question form” (Fowler 2008, 105). One of the concerns about the A/D scale 
is acquiescent response style bias, defined as the tendency to choose “agree” 
or “yes” responses more frequently than other response options (Baumgartner 
and Steenkamp 2001). Recognizing the measurement issues involved with the 
A/D  format,  researchers  have  developed  other  question  formats  to  measure 
attitudes. In this study, we will focus on one of these question types, the item-
specific  (IS)  question,  which  asks  the  respondent  to  choose  an  option  that 
best describes his or her attitude. Distinct from the A/D format, which offers 
the same response options for all questions, an IS question presents response 
options that are specific to the question contents (see appendix A for examples 
of IS questions).

Efforts have been taken to compare the quality of data from A/D and IS 
rating scales. Although some research has examined extreme response style 
(ERS) using the A/D scale, no work has yet examined ERS in the context of 
the IS scale. ERS, the tendency to select the two endpoints of a response scale 
more frequently than the intermediate ones (Paulhus 1991), produces another 
common measurement bias in rating scales. This study fills that gap by exam-
ining ERS with A/D and IS scales using experimental data in a panel study. 
We also answer two research questions. First, do respondents show different 
patterns of ERS when using an IS scale compared to an A/D scale? Since ERS 
is regarded as a type of measurement error, a scale that yields lower levels of 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

954

Liu, Lee, and Conrad

ERS bias is preferred. Second, is ERS a stable trait over time or is it a phe-
nomenon mainly driven by survey design features, such as question formats?

Literature Review
The literature presents a number of problems associated with questions in the 
A/D format.

First of all, to design an effective scale, researchers must make a series of 
decisions about the format of the question, including the number of response 
options  (Alwin  and  Krosnick  1991;  Revilla,  Saris,  and  Krosnick  2013), 
whether to label the whole scale or only the endpoints (Krosnick and Berent 
1993), whether to use numeric or verbal labels (Alwin and Krosnick 1991), 
and  whether  to  provide  middle  options  (O’Muircheartaigh,  Krosnick,  and 
Helic 2000). These decisions can potentially influence the reliability as well as 
the validity of the data. (For a more comprehensive review, see Krosnick and 
Presser [2010].) Second, the choice of the rating object is arbitrary, and differ-
ent wordings are subject to differential measurement errors (DeVellis 2011). 
The common practice is to phrase the statement such that it aligns unambigu-
ously with one end of a continuum, although the endpoint of a latent continuum 
may also be arbitrary (Fowler 1995). Third, the cognitive process of answering 
an A/D question is different from that used to answer most other survey ques-
tions. The cognitive burden is higher than if the attitude is approached more 
directly  through  IS  format.  Rather  than  the  well-known  four-step  cognitive 
process for survey response (Tourangeau, Rips, and Rasinski 2000), the A/D 
format  requires  a  more  involved  set  of  cognitive  steps  (Carpenter  and  Just 
1975; Clark and Clark 1977; Trabasso, Rollins, and Shaughnessy 1971). The 
fourth problem associated with the A/D format is that it is not clear what the 
distance is between the two adjacent options (Fowler and Cosenza 2008). In 
analysis, researchers often dichotomize Likert scale items to agree versus disa-
gree, and hence information is lost from the supposedly ordinal scale (Fowler 
2008). Last but not least, A/D format suffers from acquiescent response-style 
bias  (Schuman  and  Presser  1981;  Billiet  and  McClendon  2000;  Billiet  and 
Davidov  2008).  Such  response  style  can  artificially  increase  the  estimated 
mean and make the responses look more positive than they really are.

Knowing the measurement errors associated with the A/D scale, some sur-
vey methodologists prefer to use IS formats over A/D formats because IS for-
mats are “much simpler, direct and informative” (Fowler 1995, 57; see also 
Converse and Presser [1986]). In contrast to the A/D format, in the IS format, 
“the categories used to express the opinion are exactly those answers that we 
would like to obtain for this item” (Saris et al. 2010). Thus, it is reasonable 
to assume that the cognitive burden of answering an IS question is lower than 
that of answering an A/D question. The respondent needs only to judge and 
map his or her attitude onto the response scale without having to map both his 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

955

or her true attitude and the rating object onto the response continuum in order 
to answer an A/D question.

Empirical studies comparing the measurement quality of A/D and IS formats 
are relatively limited, and the findings are mixed. Most of the studies examine 
acquiescent  response  style  and  the  reliability  and  validity  of  the A/D  and  IS 
scales. Berkowitz and Wolkon (1964), who were among the first to examine these 
two types of rating scales, discovered that the reliability is similar across all scale 
types. Later, Ray (1979) tested a total of six A/D and IS scales, and correlated the 
responses with external validation data.1 He reported mixed findings: two types 
of the A/D scale and one IS scale had higher validity than the other scales. Patient 
satisfaction rating is another area in which A/D versus IS formats have been stud-
ied. Counte (1979) compared patient-satisfaction rating scales and found that the 
IS scale explained more variance than did the A/D measures. Similarly,  Ross, 
Steward, and Sinacore (1995) evaluated seven patient-satisfaction rating scales, 
three of which were A/D Likert scales. They detected acquiescence bias for all 
scale types.2 They also found that the internal consistency of satisfaction measures 
was higher for respondents who gave either no acquiescent responses or a mod-
erate amount, as compared to those with a high level of acquiescent responses. 
Saris  et  al.  (2010)  extended  this  line  of  study  to  an  international  setting  (i.e., 
14 European countries) using a multitrait-multimethod approach. They demon-
strated that, for two out of three questions, the item-specific format showed better 
reliability, validity, and quality than the A/D format.3 However, the quality advan-
tage for the IS question was not universal across all countries examined. More 
recently, Cibelli and Callegaro (2011) conducted a Google AdWords customer 
satisfaction survey in five languages; in each language, the A/D format elicited 
more acquiescence bias than did the item-specific format.

Previous research has also shown that the A/D Likert scales are suscepti-
ble to ERS bias (Greenleaf 1992; Weijters, Schillewaert, and Geuens 2008; 
Morren, Gelissen, and Vermunt 2011; Vaerenbergh and Thomas 2012). This is 
true regardless of the design features of the A/D scale, including scale length 
(5 to 11 points), middle response category, fully labeled versus end labeled, 
numerical labels, and agreement scale versus bipolar scale, although the mag-
nitudes  are  slightly  different  (Moors  2003;  Kieruj  and  Moors  2010,  2013; 
Moors, Kieruj, and Vermunt 2014). The presence of extreme response style 
is stable over time in longitudinal studies and consistent across items in the 
same survey, suggesting that it is a stable personality trait among respondents 

1.  The  external  validation  data  in  the  Ray  (1979)  study  are  the  peer  ratings  of  respondents’ 
achievement motivation. The validity is operationalized as the correlation between the self-reports 
and the peer ratings.
2.  The acquiescent response bias in Ross, Steward, and Sinacore (1995) was measured through 
pairs of questions for which the content is the same but the direction of the question wording was 
reversed.
3.  Validity in Saris et al. (2010) refers to the internal validity as measured by the MTMM model.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

956

Liu, Lee, and Conrad

(Weijters, Geuens, and Schillewaert 2010a, 2010b).4 It is unknown whether 
the IS scale also suffers from ERS bias and to what extent compared to the 
A/D scale. The goal of this study is to evaluate and compare the ERS bias 
between these two scale formats through experimental data.

Data and Measures
This  study  analyzes  the  experimental  data  embedded  in  the  2012 American 
National Election Studies (ANES). The questions under study measure the gen-
eral population’s political efficacy. Political efficacy refers to the “feeling that 
individual political action does have, or can have, an impact upon the political 
process” (Campbell, Gurin, and Miller 1954, 187). The ANES has measured 
the general population’s political efficacy for decades using the A/D format. 
Given the importance of this measure, it is critical to achieve a thorough under-
standing of the measurement of these questions. In 2012, the ANES includes 
four A/D items to measure individuals’ sense of efficacy. The first two items 
measure one’s internal efficacy, and the last two items measure one’s external 
efficacy. The internal efficacy questions ask about how people think about their 
ability to adequately understand and effectively participate in politics, and the 
external efficacy questions ask about people’s perceptions of the responsive-
ness of public officials and government institutions to their demands (Niemi, 
Craig, and Mattei 1991). Although there are debates about the concept of politi-
cal efficacy, these are beyond the scope of this study, and readers can find more 
details elsewhere (Balch 1974; Morrell 2003; Chamberlain 2012).

The 2012 ANES conducted the surveys in two modes, namely face-to-face 
and web, using two independent national representative samples and one iden-
tical questionnaire. The target population for the survey was US citizens aged 
18 or older by Election Day 2012. It contained a nationally representative main 
sample and two oversamples, one for African Americans and one for Hispanic 
Americans. The face-to-face respondents came from an address-based, strati-
fied, multi-stage cluster sample. The web survey was conducted using the GfK 
KnowledgePanel, a national probability web panel in the United States. Both 
address-based sampling and random-digit dialing were used to recruit the panel 
members from the general population. After a household was selected, within-
household enumeration was conducted during the recruitment stage and one 
person from each household was selected to become the panel member. For 
those households without Internet access, the survey company provided free 

4.  The ERS in Greenleaf (1992) is operationalized as the proportion of the extreme answers to 16 ques-
tions with low inter-item correlations. The ERS in Weijters, Schillewaert, and Geuens (2008) is meas-
ured through representative indicators response style means and covariance structure (RIRSMACS) 
method. The ERS in Weijters, Geuens, and Schillewaert (2010a) is estimated by the multiple-indica-
tors, multiple-covariates (MIMIC) model. The ERS in Weijters, Geuens, and Schillewaert (2010b) is 
modeled as a tau-equivalent factor complemented with a time-invariant autoregressive effect.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

957

Internet service and computer in order to remove the coverage bias of the panel. 
As such, the web panel is representative of the general US population.5

The ANES comprised two waves of data collection, including a pre-election 
study  and  a  post-election  study.  The  face-to-face  pre-election  study  was  con-
ducted between September and October 2012, and web pre-election study was 
conducted  between  October  and  November  2012. The  post-election  study  for 
both  modes  was  conducted  between  November  2012  and  January  2013.  The 
response rates for the pre-election were 38 percent and 2 percent for face-to-face 
and web, respectively (AAPOR RR1). The re-interview rates for the post-election 
studies were 94 percent and 93 percent, respectively. In total, the ANES com-
pleted 5,914 interviews in the pre-election study (2,054 in face-to-face surveys) 
and 5,510 interviews in the post-election study (1,929 in face-to-face surveys).

In order to experimentally compare the impact of A/D versus IS formats on 
data quality, the 2012 ANES included four IS items in addition to the A/D items 
for measuring political efficacy. The IS items were designed to measure the same 
internal and external efficacy as the A/D items. The two question formats were 
tested in a between- and within-subject experiment across two waves of inter-
views. In the pre-election study, the respondents were randomly assigned to one 
of the two kinds of scales. As a result, 3,023 respondents answered questions with 
an A/D scale and 2,890 respondents answered comparable questions with an IS 
scale. In the post-election study, the assignment was reversed. The between-sub-
ject design allows for examination of the ERS between the two question formats 
if we treat the pre- and post-surveys as two cross-sectional surveys. The within-
subject design permits study of the change in response style between different 
question formats for the same respondent. The experimental design and sample 
sizes are illustrated in figure 1. To rule out the possibility of differential nonre-
sponse bias between the two modes, we compared the demographic variables, 
including  gender,  age,  race/ethnicity,  education  level,  household  income,  and 
marital status, in the face-to-face and web samples. None of them was statistically 
different between the two modes (see appendix B). Also, we compared the same 
set of demographic variables between the two rating scale formats within each 
mode, and they did not differ significantly either. We applied weights to all analy-
ses throughout the paper. The weights were provided by the survey organization.6

Analytical Approaches

BETWEEN-SUBJECT EXPERIMENT ANALYSIS

We first examine the distributions of answers to each question between the 
two question formats using the pre-election interview data. This will indicate 

5.  For more information about the GfK KnowledgePanel sample design, see http://www.knowl-
edgenetworks.com/ganp/docs/KnowledgePanel(R)-Design-Summary.pdf.
6.  For more information on how the weights were created for ANES, please see http://www.elec-
tionstudies.org/studypages/anes_timeseries_2012/anes_timeseries_2012_userguidecodebook.pdf.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

958

Liu, Lee, and Conrad

Pre-election: 

Post-election: 

(1) A/D (n=3023)  

(2) IS (n=2890)  

(3) A/D (n=2778)  

(4) IS (n=2732)  

Figure 1.  Experimental Design and Sample Sizes.

whether the A/D and IS formats differ in terms of ERS and whether the differ-
ence is driven by any particular question or whether it is a consistent phenom-
enon across all four questions.

We will then perform latent class factor analysis to examine ERS between 
the two question formats. In the literature, there is no single accepted statisti-
cal procedure for measuring the ERS. We choose this method because it can 
simultaneously estimate response style and the substantive contents of the scale 
as different latent class factors. As a result, the estimate of the response-style 
factor is not confounded with the question contents. Billiet and McClendon 
(2000) initially propose using confirmatory factor analysis (CFA) to estimate 
both content factors and the acquiescent response-style factor (see also Billiet 
and Davidov [2008]). CFA treats the rating scales as ordinal variables, so it 
is  adequate  for  measuring  acquiescent  response  style  because  a  monotonic 
relationship is assumed between the response-style latent variable and rating 
scale. In other words, respondents with higher levels of acquiescent response 
style are more likely to choose a category closer to the positive end than the 
negative end of the scale. When it comes to the ERS, a U-shaped relationship 
is expected between the ERS latent class factor and response items. That is, 
respondents with higher levels of ERS are more likely to choose endpoints 
than the intermediate categories. CFA cannot capture such a relationship.

Moors (2003) extends this approach to the latent class analysis framework 
and  treats  the  rating  scales  as  nominal  variables  in  order  to  estimate  ERS. 
Morren, Gelissen, and Vermunt (2011) further extend the model, imposing an 
ordinal relationship between the rating scales and content latent class factors 
but for ERS, maintaining a nominal relationship between the same rating scales 
and ERS latent class factors in order to capture the non-monotonic (U-shaped) 
relationship. This is a more parsimonious model; for each response item, only 
one  coefficient  is  estimated  for  the  content  factors.  The  model  was  further 
simplified by imposing an equality constraint on the style factor coefficients 
(Kieruj and Moors 2013; Moors, Kieruj, and Vermunt 2014). In other words, 
only  one  set  of  coefficients  is  estimated  for  all  items.  This  is  theoretically 
meaningful, since the style factor should affect each item equally regardless 
of the question content. This constraint is similar to the equal factor loading 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

959

constraint in the CFA model by Billiet and colleagues (Billiet and McClendon 
2000; Billiet and Davidov 2008). The model can be written as follows:

log

 

(

P Y
ij
P Y
ij

(

2

F F

,

= +
1 1
|
c
i
=
|
c F F

1
i

,

2

i

,

i
E
i

,

E
i

)
) = (

,

β

0

β−

0

jc

jc

+

1

)

++

β

1

1
F
i

j
1

+

β

2

F

2

i

j
2

+

(
β

3

−

β

3

jc

jc

+

1

)

E
i

,

 

(1)

where Yij denotes the response of respondent i to item j, i=1,…,I, j=1,…,4;

F i1  denotes the internal efficacy latent class factor;
F i2  denotes the external efficacy latent class factor;
Ei denotes the extreme response style latent class factor;
β1 1j  denotes the effects on the adjacent category logits for the internal effi-

cacy latent class factor,  j1=1, 2;

β2 2j  denotes the effects on the adjacent category logits for the external effi-

cacy latent class factor,  j2=3, 4;

β

3

+ −

1

β

3

jc

jc

  denotes  the  nonmonotone  (U-shape)  relationship  between  the 

extreme response-style latent class factor and the items;

and c denotes the number of response options, c=1,2, 3, or 4.
To study ERS bias for the two types of rating scales, we fit the same meas-
urement model for the four groups (pre-election A/D, pre-election IS, post-
election A/D, and post-election IS). As illustrated in figure 2, items 1–4 refer 
to either the four A/D items or the four IS items, depending on the experimen-
tal groups under analysis. The model has two content latent class factors (F i1
, F i2 ) and one ERS latent class factor. The first content factor (F i1 ) measures 
internal efficacy, and the second measures external efficacy (F i2 ). The items 
can  only  load  on  their  corresponding  content  factors. At  the  same  time,  all 
items load on the same ERS latent factor. Previous studies suggest that, given 
our data structure, this model should fit the data well (Kieruj and Moors 2013; 
Moors, Kieruj, and Vermunt 2014). We also fit several alternative models in 
each group to test whether other measurement models can better fit the data. 
The  Bayesian  information  criterion  (BIC)  is  used  to  compare  models  and 
determine the empirically best-fitting model.

Results

BETWEEN-SUBJECT EXPERIMENT RESULTS

To examine whether ERS is a consistent phenomenon or confined to one or 
two questions, we first present the proportion of extreme responses for each 
question  from  both  question  formats  in  the  pre-election  and  post-election 
surveys  (table  1).  In  both  the  pre-  and  post-election  surveys,  respondents 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

960

Liu, Lee, and Conrad

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Figure  2.  Latent  Class  Model  with  One  Style  Factor  and Two  Content 
Factors (items 1–4 represent A/D1–A/D4 or IS1–IS4).

Table 1.  Proportions and Standard Errors of Extreme Responses for 
Agree/Disagree (A/D) and Item-Specific (IS) Scales, 2012 ANES

A/D

IS

Scale format  
effect between

(1)

S.E.

(2)

S.E.

(1)–(2)

(1)–(4)

(3)–(2)

29.00
27.79
25.77

0.83 15.43 0.67 13.57*** 15.29*** 15.42***
9.42***
0.82 14.23 0.65 13.56*** 14.91***
0.80 22.04 0.77
6.77***
3.59***

3.74***

Pre-election Complicated
Understand
Official care
Affect  

government 27.14

0.81 22.34 0.78

4.80***

6.94***

2.27***

Post-election Complicated
Understand
Official care
Affect  

(3)
30.84
23.65
25.62

(4)

S.E.

S.E.
(3)–(4)
0.88 13.70 0.66 17.14***
0.81 12.88 0.64 10.78***
6.62***
0.83 19.00 0.75

government 24.61

0.82 20.20 0.77

4.41***

***p < .001

Comparing Extreme Response Styles

961

gave more extreme responses to all four questions when using the A/D for-
mat compared to the IS format. The independent t-test shows that differences 
in pre-election (i.e., columns (1)–(2)) and post-election (i.e., (3)–(4)) are all 
statistically significant at p < .001. We also compared scale format effects 
within subject across two waves. The difference between (1) and (4) repre-
sents the difference in extreme responses among the same group of respond-
ents who responded using an A/D scale in the pre-election survey and an IS 
scale  in  the  post-election  survey.  The  paired  t-test  reveals  significant  dif-
ferences for all questions at p < .001. The comparison between (3) and (2) 
shows  that,  for  the  same  respondents,  more  extreme  answers  were  given 
using the A/D scale (post-election) than the IS scale (pre-election). The dif-
ferences are significant at p < .001 for the “complicated” and “understand” 
questions, p < .05 for the “official care” question, and marginally significant 
(p < .1) for the “affect government” question. These results offer evidence 
that the A/D format elicits more extreme answers than the IS format, and 
that the difference is not confined to a single question or the particular set 
of respondents but is consistent across all four questions for all respondents.
Although table 1 illustrates that the A/D scale has more extreme responses 
than the IS scale, it is possible that this difference is confounded with the true 
attitudinal  differences  between  these  two  question  formats.  In  other  words, 
it is possible that the difference reflects the actual attitudinal differences the 
respondent  expressed  to  the  two  sets  of  questions,  or  a  combination  of  the 
true difference and response-style difference. Table 1 does not tell us to what 
extent the pattern reflects response option preference, controlling for respond-
ents’ true attitudes toward the content of the questions. In order to control for 
this potential confounding effect, we performed latent class factor analysis to 
explicitly control for question content and to separate out the response-style 
effect from the influence of substantive attitudes. Table 2 shows all the alter-
native models that we performed and the corresponding BIC. In principle, a 
model with a small BIC and relatively fewer parameters is preferred. We fit 
seven different models within each experimental condition for both pre- and 
post-election  surveys.  Models  1  and  2  are  used  to  determine  the  appropri-
ate latent class factors for the data. Model 1 has two content latent class fac-
tors only, and model 2 includes the ERS latent class factor in addition to the 
content latent class factors. For all four experimental conditions, adding the 
style factor improves the model fit (a smaller BIC), which suggests that the 
responses,  regardless  of  the  question  formats,  reflect  something  other  than 
just the content factors. Next, we need to determine the number of classes for 
the latent class factors by comparing four alternative models, models 2, 3a, 4, 
and 5, with two, three, four, and five classes, respectively. The BICs decrease 
monotonically when the number of classes increases from two to four for all 
four conditions, but the changes are very small when moving from three to 
four classes. When it comes to five-class models, however, the model fits dete-
riorate in comparison to four-class models for all but A/D in the pre-election 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

962

Liu, Lee, and Conrad

Table 2.  Model Fit Statistics (BIC) for Agree/Disagree (A/D) and Item-
Specific (IS) Scales, 2012 ANES

Pre-election

Post-election

A/D

IS

A/D

IS

No. of 

parameters

Model 1: Content only (2-class)
Model 2: Content+ERS (2-class)
Model 3a: Content+ERS (3-class)
Model 3b: Equality on all latent  

31410
30838
30483

29185
28715
28643

28146
27468
27119

27203
26804
26757

factors (3-class)

30945

29110

27536

27118

Model 3c: Equality on style latent  

factors (3-class)

Model 4: Content+ERS (4-class)
Model 5: Content+ERS (5-class)

30434
30398
30381

28583
28639
28654

27054
27049
27069

26687
26762
26785

23
40
43

28

31
46
49

survey. Based on the BIC and ease of interpretation, we use three-class latent 
class factor models for all four conditions. The model can be further simpli-
fied by requiring the β coefficients of ERS to be equal across all items. Models 
3b and 3c test two alternative constraints. In model 3b, the equality constraint 
is  imposed  on  both  ERS  and  the  two  content  latent  class  factors.  In  model 
3c, only the ERS coefficients are constrained to be equal, but the coefficients 
associated with the two content factors are allowed to vary across items. The 
BIC indicates that model 3c fits the data best for both A/S and IS formats in 
both pre- and post-election surveys. This is theoretically meaningful because 
the ERS latent class factor is irrelevant to question content, and this is evident 
by the identical β coefficients across items.

We next report the coefficients of the style latent class factors from model 
3c for both pre- and post-election surveys. Figure 3 presents the coefficients 
for A/D and IS format from the pre-election survey. The coefficients are log 
odds of the style latent class factor effect on response items. Since model 3c 
imposes an equality constraint on the style latent class factor, there is only one 
set of coefficients for each question format. Also, because the response item is 
treated as a nominal variable, there is one coefficient for each response cate-
gory. A positive coefficient in figure 3 indicates a positive association between 
use of ERS and the likelihood of choosing the corresponding response cat-
egory. As  can  be  seen  from  figure  3,  a  common  feature  shared  by  the  two 
question formats is that the two endpoints have positive coefficients while the 
three middle response options have negative coefficients. Recall that the latent 
class factors are ordinal discrete variables with equidistant category scores. 
This suggests that a higher level of the style latent class variable represents 
tendencies toward ERS and a lower level represents a tendency to avoid ERS. 
However,  there  are  two  distinct  patterns  of  ERS  associated  with  these  two 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

963

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Figure 3.  Extreme Response Style Coefficients on Agree/Disagree (A/D) 
and Item-Specific (IS) Scales, 2012 ANES Pre-Election Survey (see appen-
dix for response categories 1–5).

Figure 4.  Extreme Response-Style Coefficients on Agree/Disagree (A/D) 
and  Item-Specific  (IS)  Scales,  2012  ANES  Post-Election  Survey  (see 
appendix for response categories 1–5).

question formats. For the A/D format, the “agree somewhat” and “disagree 
somewhat” categories fall between the two endpoints and the middle point, 
suggesting that the ERS latent class factor reflects the contrast between end-
points and midpoint. For the IS format, the three intermediate categories are 
close to each other while the endpoints are farther away. It indicates that the 
ERS coefficient for the IS format reflects the contrast between a preference for 
extreme responses and a preference for non-extreme responses. Regardless of 
these question-format specific ERS effects on response categories, the overall 
effect of ERS on response items is clearly observed from both types of ques-
tions. Figure 4 shows the ERS coefficients of A/D and IS formats from the 

964

Liu, Lee, and Conrad

post-election survey. We find exactly the same pattern as that observed in the 
pre-election survey. The consistent findings across two waves of surveys lend 
strong support to the idea that there are two different ERS patterns associated 
with A/D and IS formats.

WITHIN-SUBJECT EXPERIMENT ANALYSIS

The experimental design embedded in a longitudinal survey not only allows us 
to test the ERS of two question formats between subjects, but it also permits us 
to examine within-subject differences between the question formats. Unlike the 
between-subject analysis, the within-subject analysis must take within-subject 
correlation into consideration when building the analysis model, since the same 
respondents answer each question twice: once in A/D format and once in IS 
format. Figures 5(a) and (b) illustrate two alternative models for examining ERS 
across two waves. There are two groups of respondents for the within-subject 
analysis. The first group answered A/D formatted questions in the pre-election 
survey and IS formatted questions in the post-election survey, and the order of 
question formats is reversed for the second group. We fit latent class factor mod-
els separately for the two groups in order to measure ERS. We are interested in 
comparing the ERS latent class factors between the two question formats within 
each group rather than between the two groups. As can be seen from figure 5(a), 
the A/D questions were asked in the pre-election (or post-election) and IS were 
asked in the post-election (or pre-election) survey. F1 and F2 are the two content 
latent class factors for the A/D format, and F3 and F4 are the two content latent 
class factors for the IS format. We allow correlation between the four latent class 
factors because in theory the within-subjects political efficacy should be corre-
lated across question formats and across pre- and post-election surveys. In con-
trast, there is only one style latent class factor (ERS) for both question formats. 
This model suggests that within subjects, ERS is stable even though the question 
formats changed and the questions were administered at two points in time. The 
model in figure 5(b) contains the same response items and content latent class 
factors. The only difference is that there are two style latent class factors (ERS1 
and ERS2) for A/D and IS formats, respectively. The two style factors are cor-
related with each other. This model suggests that the two question formats elicit 
two types of ERS within subjects across two time points.

WITHIN-SUBJECT EXPERIMENT RESULTS

Figure 5 presents two theoretical measurement models measuring ERS across 
time using the panel data. However, we also test alternative models that are 
slight modifications of the ones in figure 5. The BIC and number of parameters 
in these models are presented in table 3. Model 1 contains four content latent 
class factors only, model 2 contains four content factors and one style factor 
(figure  5a),  and  model  3  contains  four  content  factors  and  two  style  factors 
(figure  5b).  For  group  1  members  who  were  asked  questions  using  the A/D 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

965

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Figure 5.  (a) Latent Class Model with One Style Factor and Four Content 
Factors; (b) Latent Class Model with Two Style Factors and Four Content 
Factors.

966

Liu, Lee, and Conrad

Table 3.  Model Fit Statistics (BIC) for Within-Subject Analysis, 2012 
ANESc

Group 1

Group 2

A/D→ISa

IS→A/Db

No. of 

parameters

Model 1: Content only
Model 2: Content+1 ERS
  Model 2a: Equality on all latent factors
  Model 2b: Equality on style latent factor
Model 3: Content+2 ERS
  Model 3a: Equality on all latent factors
  Model 3b: Equality on style latent factors

19334
19201
19619
19144
19207
19637
19151

18386
18214
18570
18091
18192
18629
18105

54
88
56
60
91
71
67

aAgree/disagree in pre-election and item specific in post-election survey.
bItem specific in pre-election and agree/disagree in post-election survey.
cAll latent class factors have 3 classes.

format in the pre-election and IS format in the post-election, model 2 has the 
smallest BIC. We then proceed to test two alternatives of this model by impos-
ing equality constraints on the all latent class factors (model 2a) or on the style 
latent class factor only (model 2b). The second approach results in the smallest 
BIC, which suggests that for group 1, the model in figure 5a fits the data best. 
Similarly, for group 2 members, who received IS questions in the pre-election 
and A/D questions in the post-election survey, the smallest BIC comes from 
model 2b as well. This means that the model in figure 5a consistently outper-
forms figure 5b, regardless of the question administration order. In other words, 
the single ERS latent class factor for group 1 indicates that once the within-
subject correlation is built in to model, the question formats do not affect ERS.
The  ERS  coefficients  for  groups  1  and  2  are  plotted  in  figures  6  and  7. 
The general conclusion from these two figures is that ERS represents a con-
trast between two extreme categories and the middle three categories. This is 
evident by the negative and relatively similar coefficients for the three non-
extreme categories and the two positive coefficients at the endpoints. The ERS 
coefficients from the two figures are very similar to each other, which indi-
cates that the single ERS model is quite reliable.

Discussion
In this study, we examined extreme response styles from two types of rating 
scales, namely agree-disagree (A/D) scales and item-specific (IS) scales, using 
data from between- and within-subject experiments. The four items we exam-
ined  come  from  the  2012 American  National  Election  Studies  and  measure 
internal and external political efficacy. The experiments were carried out in two 
waves of data collection, with respondents randomly assigned to one of the two 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

967

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Figure 6.  Extreme Response-Style Coefficients for Group 1 (A/D in the 
pre-election and IS in the post-election), Model 2b (table 3), 2012 ANES 
(see appendix for response categories 1–5).

Figure 7.  Extreme Response-Style Coefficients for Group 2 (IS in the pre-
election and A/D in the post-election), Model 2b (table 3), 2012 ANES (see 
appendix for response categories 1–5).

question formats in wave 1, and then assigned to the remaining format in wave 
2. We conducted a latent class factor analysis that yielded three main findings: 
first, ERS exists in both A/D and IS scale formats. Ideally, we would have iden-
tified a scale format that is response-style free. Unfortunately, this study shows 
that  we  cannot  achieve  this  by  using  item-specific  scales.  Second,  the  ERS 
from the two formats show slightly different shapes. Although respondents who 
exhibit higher levels of ERS are more likely to choose the two endpoints in both 
formats, those who exhibit lower levels of ERS show different patterns in these 

968

Liu, Lee, and Conrad

two formats. In A/D format, ERS reflects a contrast between the two endpoints 
and the middle category while the two intermediate categories (“agree some-
what” and “disagree somewhat”) are somewhere in between. In IS format, ERS 
is the contrast between the two endpoints and the three non-extreme categories. 
This finding is consistent in both pre-election and post-election studies. Third, 
when analyzing ERS within subjects across two waves, there is only one single 
ERS for both question formats, regardless of which question type was asked 
first, after controlling for the correlation within respondents.

Why do the ERSs differ between A/D and IS, and why do they differ in 
this particular pattern? A closer look at the response options of the two scale 
formats may shed some light on this question. The verbal labels of these two 
scale formats are different. The A/D format is a bipolar scale, while the verbal 
labels for IS seem to be more unipolar. In a bipolar scale, the two endpoints 
represent the ends of an underlying continuum while the midpoint represents 
the conceptual center of this continuum. The conceptual distance from “agree 
strongly”  to  “neither  agree  nor  disagree”  and  the  distance  from  “disagree 
strongly” to “neither agree nor disagree” are equal. Since the respondents can 
easily pinpoint the conceptual center of an A/D scale, those who tend to avoid 
extreme points will move away from the endpoints toward the midpoint, the 
one that is the furthest from the extreme points. Thus, we observe a contrast 
between the extreme points and midpoints for the A/D scale.

For the IS scale, however, the conceptual center is more difficult to establish. 
For example, consider the following IS scale: Extremely well, very well, moder-
ately well, slightly well, or not well at all. It is clear that “moderately well” is the 
closest to the conceptual center of the scale but is not necessarily the center. Also, 
the conceptual distance between “extremely well” and “moderately well” and 
the distance between “not well at all” and “moderately well” are not necessarily 
equal. Consequently, when respondents want to avoid extreme options, they will 
pick one of the middle three categories with relatively equal likelihood, since the 
verbal labels of response categories on an IS scale do not necessarily contain a 
conceptual midpoint. These findings are consistent with the results from Moors, 
Kieruj, and Vermunt (2014). In their study, the balanced bipolar A/D scale with 
numerical labels from –3 to +3 elicits an ERS similar to the one for the A/D scale 
in the present study, while a unipolar A/D scale with numerical labels from 1 to 7 
has an ERS similar to the one from the IS scale in the present study.

Another important finding is the similarity of the ERS between two question 
formats within subjects. Each respondent answered both types of scales across 
two waves of interviews. Our analysis shows that once the within-person cor-
relation is incorporated into the latent class factor analysis, the ERS differences 
between  question  formats  disappear  for  both  the  group  of  respondents  who 
answered A/D in pre-election and IS in post-election interviews, and the group 
of respondents who answered IS in pre-election and A/D in post-election inter-
views. As a result, there is only one ERS latent class factor for both question 
formats in both groups. This suggests that the response style is stable across two 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

969

question formats and across two time points (pre- and post-election). This find-
ing lends strong support to the argument that response styles have more to do 
with individual respondents than with survey design or question content. This 
finding is in line with the results reported in previous studies (Kieruj and Moors 
2013; Weijters, Geuens, and Schillewaert 2010a, 2010b). Those studies report 
stable response style for the A/D format in longitudinal surveys or demonstrate an 
association between response style and personality measures. In the 2012 ANES, 
the two waves are about four months apart, and the question formats are differ-
ent in the two interviews. The identical and similar ERS we observed in the data 
provide further support for the argument that response style is a stable behavior.
One  of  the  aims  of  this  study  is  to  identify  a  rating  scale  format  that  is 
ERS free. As we already know, A/D format suffers from both acquiescent and 
extreme  response  style  while  IS  format  reduces  acquiescent  response  bias. 
This study shows that, like A/D format, IS format is still susceptible to ERS. 
Thus,  the  goal  of  eliminating  ERS  cannot  be  achieved  by  using  IS  format. 
However, given the advantage of IS format in reducing acquiescent response 
style, we would still recommend IS over A/D rating scales, in general. On the 
bright side, although we fail to remove ERS through scale design, we can still 
control  for  ERS,  as  well  as  other  response  styles,  in  the  statistical  analysis 
stage. Latent class factor analysis is one analytic approach that measures sub-
stantive latent class factors and response style latent class factors at the same 
time. The estimates for the substantive latent construct should be free of the 
response-style bias. Of course, this analytic approach imposes requirements 
on the rating scales. At the very least, the latent construct needs to be measured 
by multi-item scales rather than by one single question. Also, reverse-worded 
questions must be included for measuring acquiescent response style.

As this study shows, more work is required to examine and ultimately elimi-
nate response styles from rating scales. In the present study, we used latent 
class factor analysis to estimate the response style. As we mentioned above, 
this is certainly not the only analytical approach one can take. There are other 
methods in the literature that have been used to examine response style, such 
as item response theory (Jong et al. 2008), multidimensional unfolding model 
(Javaras and Ripley 2007), and representative indicators response style means 
and  covariance  structure  (Thomas, Abts,  and  Weyden  2014).  However,  we 
are not aware of any studies comparing these different analytical tools in this 
domain  of  study.  Future  research  should  evaluate  and  compare  the  existing 
analytic methods and resolve any differences in the conclusions in order to 
better adjust for response styles and produce more accurate survey estimates.
Furthermore,  although  the  IS  format  fails  to  eliminate  ERS,  it  is  possible 
that other designs of the IS format may elicit different levels of ERS. Future 
research  should  explore  the  scale  length  and  the  use  of  verbal  and  numeric 
labeling in IS format. Some research has examined variations of A/D format 
and their impacts on ERS (Kieruj and Moors 2010; Kieruj and Moors 2013; 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

970

Liu, Lee, and Conrad

Moors, Kieruj, and Vermunt 2014), but this is lacking for IS format. In addition, 
would these findings change if the study had been conducted in another cul-
ture? Numerous studies have shown response-style differences between racial/
ethnic groups within country and across different countries (for a comprehen-
sive review, see Yang et al. [2010]; Liu [2015]). As international comparative 
studies become more prominent, it is critical to identify a scale format that can 
reduce response-style bias across different countries and cultural groups. So 
far, there is very little knowledge on this topic. Last but not least, the current 
study only examined scales measuring political efficacy. We hope future stud-
ies will replicate this study by examining scales measuring other topics in order 
to help ensure that the findings reported here are broadly applicable.

Appendix A. Question Wordings and Response Options for 
Agree-Disagree and Item-Specific Questions

LIKERT SCALE

A/D1: Sometimes, politics and government seem so complicated that a person 
like me can’t really understand what’s going on.
A/D2: I feel that I have a pretty good understanding of the important political 
issues facing our country.
A/D3: Public officials don’t care much what people like me think.
A/D4: People like me don’t have any say about what the government does.
Response options to A/D1–A/D4 are agree strongly (category 1), agree some-
what (category 2), neither agree nor disagree (category 3), disagree some-
what (category 4), or disagree strongly (category 5).

ITEM-SPECIFIC ITEMS

IS1:  How  often  do  politics  and  governments  seem  so  complicated  that  you 
can’t really understand what’s going on?
Always (category 1), most of the time (category 2), about half the time (cat-
egory 3), some of the time (category 4), or never (category 5)
IS2:  How  well  do  you  understand  the  important  political  issues  facing  our 
country?
Extremely well (category 1), very well (category 2), moderately well (category 
3), slightly well (category 4), or not well at all (category 5)
IS3: How much do public officials care what people like you think?
A great deal (category 1), a lot (category 2), a moderate amount (category 3), 
a little (category 4), or not at all (category 5)
IS4: How much can people like you affect what the government does?
A great deal (category 1), a lot (category 2), a moderate amount (category 3), 
a little (category 4), or not at all (category 5)

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

P

6
9

.

0

4
4

.

0

χ2

0

1
8

.

4

5
9

.

0

4
3

.

0

6
9

.

0

7
0

.

0

971

d
e
u
n
i
t
n
o
C

6
4

.

0

9
5

.

2

 

d
n
a

 
)

D
A

/

(
 
e
e
r
g
a
s
i
D
/
e
e
r
g
A
 
r
o
f
 
s
e
l
b
a
i
r
a
V
 
c
i
h
p
a
r
g
o
m
e
D

 

 
f
o
 
s
r
o
r
r
E
d
r
a
d
n
a
t
S
d
n
a
 
s
n
a
e
M

 

 
.

 

B
x
i

d
n
e
p
p
A

l
l

A

b
e
W

e
c
a
f
-
o
t
-
e
c
a
F

S
E
N
A
2
1
0
2

 

 
,

n
o
i
t
c
e
l
l
o
C
a
t
a
D

 

 
f
o

 

 
e
d
o
M
y
b
 
s
e
l
a
c
S
 
)
S
I
(
 
c
i
f
i
c
e
p
S
-
m
e
t
I

b
e
W

1

.

2
5

0

.

1
2

0

.

5
1

7

.

6
1

1

.

0
2

3

.

6
1

9

.

0
1

9

.

0
7

9

.

1
1

3

.

1
1

9

.

5

1

.

0
4

4

.

0
3

5

.

9
2

5

.

9
4

9

.

1
3

F
T
F

0

.

2
5

2

.

1
2

6

.

5
1

3

.

7
1

9

.

8
1

3

.

4
1

8

.

2
1

9

.

0
7

9

.

1
1

9

.

0
1

3

.

6

6

.

0
4

1

.

0
3

3

.

9
2

1

.

0
5

0

.

0
3

p

5
8

.

0

2
4

.

0

χ2

4
0

.

0

3
9

.

4

2
8

.

0

3
9

.

0

0
2

.

0

6
2

.

3

6
4

.

0

0
6

.

2

S
I

3

.

2
5

0

.

2
2

6

.

4
1

6

.

5
1

5

.

9
1

7

.

6
1

7

.

1
1

1

.

1
7

1

.

2
1

6

.

0
1

2

.

6

0

.

2
4

6

.

9
2

4

.

8
2

7

.

0
5

9

.

0
3

D
A

/

9

.

1
5

1

.

0
2

3

.

5
1

8

.

7
1

7

.

0
2

8

.

5
1

2

.

0
1

7

.

0
7

8

.

1
1

8

.

1
1

6

.

5

3

.

8
3

2

.

1
3

5

.

0
3

3

.

8
4

9

.

2
3

p

7
7

.

0

6
7

.

0

χ2

9
0

.

0

9
5

.

2

5
5

.

0

3
1

.

2

0
7

.

0

0
7

.

0

8
8

.

0

7
6

.

0

S
I

5

.

2
5

9

.

0
2

1

.

5
1

4

.

6
1

0

.

0
2

7

.

3
1

9

.

3
1

3

.

0
7

3

.

1
1

4

.

1
1

0

.

7

1

.

0
4

3

.

1
3

6

.

8
2

8

.

8
4

1

.

1
3

D
A

/

6

.

1
5

4

.

1
2

0

.

6
1

1

.

8
1

7

.

7
1

9

.

4
1

7

.

1
1

4

.

1
7

6

.

2
1

4

.

0
1

6

.

5

1

.

1
4

0

.

9
2

9

.

9
2

l
o
o
h
c
s
 

h
g
i
h
-
t
s
o
p

 
e
m
o
S

e
v
o
b
a
 

d
n
a
 
s
’
r
o
l
e
h
c
a
B

s
s
e
l
 
r
o

 
l
o
o
h
c
s
 

h
g
i
H

y
t
i
c
i
n
h
t
e
/
e
c
a
R

c
i
n
a
p
s
i
H

r
e
h
t
O

n
o
i
t
a
c
u
d
E

e
t
i
h
W

k
c
a
l
B

e
l
a
m
e
F

e
g
A

0
3
<

 

9
3
–
0
3

9
4
–
0
4

9
5
–
0
5

9
6
–
0
6

 

0
7
=
>

4

.

1
5

8

.

8
2

9
9
9

,

9
9
–
0
0
0

,

0
5

9
9
9

,

9
4
–
0

)
s
r
a
l
l
o
d

 

S
U

(
 
e
m
o
c
n
i
 

d
l
o
h
e
s
u
o
H

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

972

Liu, Lee, and Conrad

P

χ2

6
9

.

0

2
6

.

0

b
e
W

9

.

6

7

.

1
1

3

.

3
5

7

.

5

1

.

3
1

3

.

2

7

.

5
2

F
T
F

3

.

8

7

.

1
1

2

.

3
5

1

.

6

6

.

2
1

6

.

2

5

.

5
2

p

χ2

0
2

.

0

5
0

.

6

S
I

3

.

6

0

.

2
1

2

.

1
5

5

.

6

3

.

3
1

6

.

2

3

.

6
2

D
A

/

4

.

7

4

.

1
1

3

.

5
5

9

.

4

8

.

2
1

0

.

2

1

.

5
2

p

χ2

2
1

.

0

6
3

.

7

S
I

4

.

8

7

.

1
1

9

.

4
5

9

.

5

3

.

3
1

6

.

1

4

.

4
2

D
A

/

2

.

8

6

.

1
1

6

.

1
5

2

.

6

9

.

1
1

6

.

3

7

.

6
2

l
l

A

b
e
W

e
c
a
f
-
o
t
-
e
c
a
F

9
9
9

,

9
4
1
–
0
0
0

,

0
0
1

s
u
t
a
t
s
 
l
a
t
i
r
a
M

+
0
0
0

,

0
5
1

d
e
i
r
r
a
m

 
r
e
v
e
N

d
e
w
o
d
i
W

d
e
c
r
o
v
i
D

d
e
t
a
r
a
p
e
S

d
e
i
r
r
a

M

d
e
u
n
i
t
n
o
C

 
.

 

B
x
i

d
n
e
p
p
A

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.
o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

973

References

Alwin, Duane F., and Jon A. Krosnick. 1991. “The Reliability of Survey Attitude Measurement: 
The  Influence  of  Question  and  Respondent Attributes.”  Sociological  Methods  &  Research 
20:139–81.

Balch, George I. 1974. “Multiple Indicators in Survey Research: The Concept ‘Sense of Political 

Efficacy.’” Political Methodology 1:1–43.

Baumgartner, Hans, and Jan-Benedict E. M. Steenkamp. 2001. “Response Styles in Marketing 

Research: A Cross-National Investigation.” Journal of Marketing Research 38:143–56.

Bearden, William O., and Richard G. Netemeyer. 1999. Handbook of Marketing Scales: Multi-
Item Measures for Marketing and Consumer Behavior Research. Thousand Oaks, CA: Sage.
Berkowitz, Norman H., and George H. Wolkon. 1964. “A Forced Choice Form of the F Scale-Free 

of Acquiescent Response Set.” Sociometry 27:54–65.

Billiet, Jaak B., and Eldad Davidov. 2008. “Testing the Stability of an Acquiescence Style Factor 
Behind Two Interrelated Substantive Variables in a Panel Design.” Sociological Methods & 
Research 36:542–62.

Billiet,  Jaak  B.,  and  McKee  J.  McClendon.  2000.  “Modeling  Acquiescence  in  Measurement 
Models for Two Balanced Sets of Items.” Structural Equation Modeling: A Multidisciplinary 
Journal 7:608–28.

Bradburn,  Norman  M.,  Seymour  Sudman,  and  Brian  Wansink.  2004.  Asking  Questions:  The 
Definitive Guide to Questionnaire Design—for Market Research, Political Polls, and Social 
and Health Questionnaires. San Francisco: Jossey-Bass.

Bruner, Gordon C., Paul J. Hensel, and Karen E. James. 2001. Marketing Scales Handbook, vol. 

5. Carbondale, IL: American Marketing Association.

Campbell, Angus, Gerald Gurin, and Warren E. Miller. 1954. The Voter Decides. Evanston, IL: 

Row, Peterson.

Carpenter, Patricia A., and Marcel A. Just. 1975. “Sentence Comprehension: A Psycholinguistic 

Processing Model of Verification.” Psychological Review 82:45–73.

Chamberlain,  Adam.  2012.  “A  Time-Series  Analysis  of  External  Efficacy.”  Public  Opinion 

Quarterly 76:117–30.

Cibelli, Kristen L., and Mario Callegaro. 2011. “Assessing the Measurement Quality of Agree/
Disagree Items versus Item-Specific Answer Scales.” Paper presented at the Annual Meeting 
for the Midwest Association for Public Opinion Research, Chicago, IL, USA.

Clark, Herbert H., and Eve V. Clark. 1977. Psychology and Language. New York: Harcourt Brace 

Jovanovich.

Converse, Jean M., and Stanley Presser. 1986. Survey Questions: Handcrafting the Standardized 

Questionnaire, vol. 63. Thousand Oaks, CA: Sage.

Counte,  Michael  A.  1979.  “An  Examination  of  the  Convergent  Validity  of  Three  Measures 
of  Patient  Satisfaction  in  an  Outpatient  Treatment  Center.”  Journal  of  Chronic  Diseases 
32:583–88.

DeVellis, Robert F. 2011. Scale Development: Theory and Applications, vol. 26. Thousand Oaks, 

CA: Sage.

Fowler, Floyd J. 1995. Improving Survey Questions: Design and Evaluation, vol. 38. Thousand 

Oaks, CA: Sage.

Fowler, F. J., and Carol Cosenza. 2008. “Writing Effective Questions.” In International Handbook 
of Survey Methodology, edited by Edith D. de Leeuw, Joop J. Hox, and Don A. Dillman, 136–
59. New York: Taylor & Francis Group.

———. 2008. Survey Research Methods, vol. 1. Thousand Oaks, CA: Sage.
Greenleaf,  Eric  A.  1992.  “Measuring  Extreme  Response  Style.”  Public  Opinion  Quarterly 

56:328–51.

Javaras, Kristin N., and Brian D. Ripley. 2007. “An ‘Unfolding’ Latent Variable Model for Likert 
Attitude  Data:  Drawing  Inferences Adjusted  for  Response  Style.”  Journal  of  the  American 
Statistical Association 102:454–63.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

974

Liu, Lee, and Conrad

Jong, Martijn G. de, Jan-Benedict E. M. Steenkamp, Jean-Paul Fox, and Hans Baumgartner. 2008. 
“Using  Item  Response Theory  to  Measure  Extreme  Response  Style  in  Marketing  Research: 
A Global Investigation.” Journal of Marketing Research 45:104–15.

Kieruj, Natalia D., and Guy Moors. 2010. “Variations in Response Style Behavior by Response 
Scale  Format  in  Attitude  Research.”  International  Journal  of  Public  Opinion  Research 
22:320–42.

———.  2013.  “Response  Style  Behavior:  Question  Format  Dependent  or  Personal  Style?” 

Quality & Quantity 47:193–211.

Krosnick, Jon A., and Robert P. Abelson. 1992. “The Case for Measuring Attitude Strength in 
Surveys.” In Questions About Questions: Inquiries into the Cognitive Bases of Surveys, edited 
by Judith M. Tanur, 177–203. New York: Russell Sage Foundation.

Krosnick, Jon A., and Matthew K. Berent. 1993. “Comparisons of Party Identification and Policy 
Preferences: The Impact of Survey Question Format.” American Journal of Political Science 
37:941–64.

Krosnick, Jon A., and Stanley Presser. 2010. “Question and Questionnaire Design.” In Handbook 
of Survey Research, Second Edition, edited by Peter V. Marsden and James D. Wright, 263–
314. Bingley, UK: Emerald Publishing Group Limited.

Liu, Mingnan. 2015. “Response Style and Rating Scales: The Effects of Data Collection Mode, 

Scale Format, and Acculturation.” Dissertation, University of Michigan.

Moors,  Guy.  2003.  “Diagnosing  Response  Style  Behavior  by  Means  of  a  Latent-Class  Factor 
Approach: Socio-Demographic Correlates of Gender Role Attitudes and Perceptions of Ethnic 
Discrimination Reexamined.” Quality and Quantity 37:277–302.

Moors,  Guy,  Natalia  D.  Kieruj,  and  Jeroen  K.  Vermunt.  2014.  “The  Effect  of  Labeling  and 
Numbering of Response Scales on the Likelihood of Response Bias.” Sociological Methodology 
44:369–99.

Morrell, Michael E. 2003. “Survey and Experimental Evidence for a Reliable and Valid Measure 

of Internal Political Efficacy.” Public Opinion Quarterly 67:589–602.

Morren, Meike, John P. T. M. Gelissen, and Jeroen K. Vermunt. 2011. “Dealing with Extreme 
Response  Style  in  Cross-Cultural  Research:  A  Restricted  Latent  Class  Factor  Analysis 
Approach.” Sociological Methodology 41:13–47.

Niemi,  Richard  G.,  Stephen  C.  Craig,  and  Franco  Mattei.  1991.  “Measuring  Internal  Political 
Efficacy in the 1988 National Election Study.” American Political Science Review 85:1407–1413.
O’Muircheartaigh,  Colm,  Jon  Krosnick,  and  Armen  Helic.  2000.  Middle  Alternatives, 
Acquiescence,  and  the  Quality  of  Questionnaire  Data.  Chicago:  Harris  School  of  Public 
Policy Studies, University of Chicago.

Paulhus,  Delroy  L.  1991.  “Measurement  and  Control  of  Response  Bias.”  In  Measures  of 
Personality and Social Psychological Attitudes: Measures of Social Psychological Attitudes, 
vol. 1, edited by John P. Robinson, Phillip R. Shaver, and Lawrence S. Wrightsman, 17–59. San 
Diego, CA: Academic Press.

Ray, John J. 1979. “A Quick Measure of Achievement Motivation—Validated in Australia and 

Reliable in Britain and South Africa.” Australian Psychologist 14:337–44.

Revilla, M. A., W. E. Saris, and J. A. Krosnick. 2013. “Choosing the Number of Categories in 

Agree-Disagree Scales.” Sociological Methods & Research 43:73–97.

Ross, Caroline K., Colette A. Steward, and James M. Sinacore. 1995. “A Comparative Study of 

Seven Measures of Patient Satisfaction.” Medical Care 33:392–406.

Saris, Willem  E.,  Melanie  Revilla,  Jon A.  Krosnick,  and  Eric  M.  Shaeffer.  2010.  “Comparing 
Questions with Agree/Disagree Response Options to Questions with Item-Specific Response 
Options.” Survey Research Methods 4:61–79.

Schuman,  Howard,  and  Stanley  Presser.  1981.  Questions  and  Answers  in  Attitude  Surveys: 

Experiments on Question Form, Wording, and Context. Thousand Oaks, CA: Sage.

Thomas, Troy D., Koen Abts, and Patrick Vander Weyden. 2014. “Response Styles and the Rural–

Urban Divide.” Educational and Psychological Measurement 74:97–115.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

975

Tourangeau,  Roger,  Lance  J.  Rips,  and  Kenneth  Rasinski.  2000.  The  Psychology  of  Survey 

Response. Cambridge, UK: Cambridge University Press.

Trabasso,  Tom,  Howard  Rollins,  and  Edward  Shaughnessy.  1971.  “Storage  and  Verification 

Stages in Processing Concepts.” Cognitive Psychology 2:239–89.

Vaerenbergh,  Yves  Van,  and  Troy  D.  Thomas.  2012.  “Response  Styles  in  Survey  Research: 
A Literature Review of Antecedents, Consequences, and Remedies.” International Journal of 
Public Opinion Research 25:195–217.

Weijters,  Bert,  Maggie  Geuens,  and  Niels  Schillewaert.  2010a.  “The  Stability  of  Individual 

Response Styles.” Psychological Methods 15:96–110.

———.  2010b.  “The  Individual  Consistency  of Acquiescence  and  Extreme  Response  Style  in 

Self-Report Questionnaires.” Applied Psychological Measurement 34:105–21.

Weijters, Bert, Niels Schillewaert, and Maggie Geuens. 2008. “Assessing Response Styles across 

Modes of Data Collection.” Journal of the Academy of Marketing Science 36:409–22.

Yang, Yongwei, Janet A. Harkness, Tzu-Yun Chin, and Ana Villar. 2010. “Response Styles and 
Culture.”  In  Survey  Methods  in  Multinational,  Multiregional,  and  Multicultural  Contexts, 
edited  by  Janet  A.  Harkness,  Michael  Braun,  Brad  Edwards,  Timothy  P.  Johnson,  Lars  E. 
Lyberg, Peter Ph. Mohler, Beth-Ellen Pennell, and Tom W. Smith, 203–23. Hoboken, NJ: Wiley.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

