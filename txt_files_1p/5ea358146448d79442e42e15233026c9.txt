IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—II: EXPRESS BRIEFS, VOL. 58, NO. 8, AUGUST 2011

537

Robust Quasi-Newton Adaptive Filtering Algorithms

Md. Zulﬁquar Ali Bhotto, Student Member, IEEE, and Andreas Antoniou, Life Fellow, IEEE

Abstract—Two robust quasi-Newton (QN) adaptive ﬁltering al-
gorithms that perform well in impulsive-noise environments are
proposed. The new algorithms use an improved estimate of the in-
verse of the autocorrelation matrix and an improved weight-vector
update equation, which lead to improved speed of convergence and
steady-state misalignment relative to those achieved in the known
QN algorithms. A stability analysis shows that the proposed
algorithms are asymptotically stable. The proposed algorithms
perform data-selective adaptation, which signiﬁcantly reduces the
amount of computation required. Simulation results presented
demonstrate the attractive features of the proposed algorithms.

Index

Terms—Adaptive

in
adaptive ﬁlters, quasi-Newton algorithms, robust adaptation
algorithms.

impulsive

ﬁlters,

noise

I. INTRODUCTION

K NOWN approaches for improving the performance of

adaptive ﬁlters in impulsive-noise environments involve
the use of nonlinear clipping [1], [2], robust statistics [3]–[5],
or order statistics. The common step in the adaptive ﬁlters
reported in [1]–[5] is to detect the presence of impulsive noise
by comparing the magnitude of the error signal with the value of
a threshold parameter, which is a scalar multiple of the variance
of the error signal, and then either stop or reduce the learning
rate of the adaptive ﬁlter. In [1], the variance of the error
signal is estimated by averaging the square of its instantaneous
values, but this approach is not robust with respect to impulsive
noise. In [2]–[5], improved robustness is achieved by estimating
the variance of the error signal using the median absolute
deviation [6].

The adaptation algorithms in [1] and [3] use the Huber
mixed-norm M-estimate objective function [6], and the algo-
rithms in [4] and [5] use the Hampel three-part redescending
M-estimate objective function [6]. The Huber function uses the
L1 norm to measure the amplitude of the error signal when the
absolute error is larger than the threshold. The Hampel function,
on the other hand, assigns a constant value to the error signal
when the absolute error becomes larger than the threshold.
Algorithms based on the Huber and Hampel functions of-
fer similar performance. The nonlinear recursive least-squares
(NRLS) algorithm in [2] uses nonlinear clipping to control the
learning rate and offers better performance in impulsive-noise

Manuscript received October 1, 2010; revised December 31, 2010 and
March 16, 2011; accepted May 14, 2011. Date of publication July 22, 2011;
date of current version August 17, 2011. This work was supported by the
Natural Sciences and Engineering Research Council of Canada. This paper was
recommended by Associate Editor Z. Lin.

The authors are with the Department of Electrical and Computer Engi-
neering, University of Victoria, Victoria, BC V8W 3P6, Canada (e-mail:
zbhotto@ece.uvic.ca; aantoniou@ieee.org).

Color versions of one or more of the ﬁgures in this paper are available online

environments than the conventional RLS algorithm. The recur-
sive least-mean (RLM) algorithm reported in [4] offers faster
convergence and better robustness than the NRLS algorithm
in impulsive-noise environments. The quasi-Newton algorithm
(QN) in [7] is not robust against impulsive noise. Simulation re-
sults in [5] show that the recursive QN (RQN) algorithm offers
faster convergence and improved robustness in impulsive-noise
environments relative to the QN algorithm in [7]. Algorithms
of the Newton family such as those in [2], [4], [5], and [7]
converge much faster than algorithms of the steepest-descent
family [8]. The RLS, RLM, and RQN algorithms exponentially
forget past input signal vectors in the estimate of the autocor-
relation matrix, and therefore, the positive deﬁniteness of the
inverse of the autocorrelation matrix can be lost [9], [10]. As a
result, this type of adaptive ﬁlter can become unstable in ﬁnite-
precision implementations. This form of explosive divergence
in RLS adaptive ﬁlters is well documented in the literature, and
ways and conditions to stabilize these ﬁlters are addressed in
[9] and [10]. The QN algorithm reported in [7] is shown to be
robust in terms of explosive divergence, as compared with RLS
adaptive ﬁlters.

In this brief, we propose two new robust QN algorithms
that perform data-selective adaptation in updating the inverse
of the autocorrelation matrix and the weight vector. The new
algorithms are essentially enhancements of the algorithms we
reported in [16] for applications that entail impulsive noise. A
stability analysis shows that the proposed algorithms are as-
ymptotically stable. Furthermore, simulation results show that
the proposed algorithms offer improved performance relative
to two known QN (KQN) algorithms with respect to robust-
ness, steady-state misalignment, computational efﬁciency, and
tracking capability.

This brief is organized as follows. In Section II, the proposed
robust QN algorithms are described. In Section III, stability
issues of the algorithms are discussed, and in Section IV, some
practical issues concerning the implementation of the proposed
algorithms are examined. Simulation results are presented in
Section V, and conclusions are drawn in Section VI.

II. PROPOSED ROBUST QN ALGORITHMS

Two slightly different robust QN algorithms are possible, one
using a ﬁxed threshold and the other using a variable threshold,
as will now be demonstrated.

A. RQN Algorithm with a Fixed Threshold

The objective of the proposed adaptation algorithm is to
generate a series of weight vectors that would eventually solve
the optimization problem

(dk − wT xk)2

(1)

at http://ieeexplore.ieee.org.

Digital Object Identiﬁer 10.1109/TCSII.2011.2158722

minimize

w

E

1549-7747/$26.00 © 2011 IEEE

(cid:2)

(cid:3)

