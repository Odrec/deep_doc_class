 

Functionalism 

The objection that made the identity theory in its original version disappear as quickly as it had 

risen to prominence in the 1950s was, as we saw last week, the multiple realizability argument, 

either in Putnam’s “empirical likelihood version” or in Kripke’s “imaginability version.” One 

response to this argument was to modify the identity theory in such a way as to relativize it to 

species (or subsets thereof) and to bet one’s philosophical money on something like “species-

specific identities.” The major problem with this move, however, was that if we restrict mental 

properties and their physical “correlates” to, say, species, then we can no longer hold that 

members of different species who are, say, in pain, literally share the same property. They don’t. 

One of them has S1-pain, the other S2-pain etc. But since the general property being in pain 

doesn’t correspond to a single physical property across all species, it does not occur in our list 

of physicalistically acceptable or “kosher” properties. There are then only three options: (1) 

accept being in pain as a non-physical addition to the ontology of our world (property dualism), 

(2) explain how being in pain can be a fundamentally physical property even though it is not 

straightforwardly reducible to physical properties (anti-reductionism), (3) give up the general 

property of being in pain as a property in its own right (eliminativism). Note that what is at issue 

is that the advocate of the species-specific identity theory can do to respond to the objection at 

hand. If you are an advocate of the species-specific identity theory, (1) and (2) are obviously 

not really an option. For you became an advocate of the species-specific identity theory in the 

first place because you were convinced of a reductive version of physicalism. (1) would mean giving 

up physicalism; (2) would mean giving up reductionism. Hence, only (3) is left. 

Some advocates of the species-specific identity theory (Kim, for instance) have indeed 

bitten the bullet and have denied that there is such a general property like being in pain. They 

argue that not every predicate that we use in our everyday language corresponds to a property. 

We use the predicate “is in pain” for pragmatic reasons to pick out a group of individuals, viz., 

those who are in some form of pain or other, although there is no single pain property that 

they and only they share—this property has been eliminated. For those who don’t find that 

attractive, and who find property dualism equally attractive, the only way out seems to be (2), 

i.e., to hold that, to stick to the example, being in pain is a fundamentally physical property that 

is, however, not straightforwardly reducible to physical properties. It is an irreducible physical 

property. One of the most important way to spell out this position is, of course, functionalism. 

According to Putnam, his multiple realizability argument shows that being in pain is not a 

physical property of the brain (or even the whole nervous system), but another kind of 

property entirely. As he puts it: “I propose the hypothesis that pain, or the state of being in 

pain, is a functional state of a whole organism” (Putnam 1967, 433). But what does it mean to 

say that being in pain is a functional property, or that pain is a functional state? This is what we 

are going to start with in section 1, before briefly considering the most important arguments 

for and against functionalism in sections 2 and 3. 

 

1. Functionalism 

Let us start with the notion of a functional analysis or functional decomposition. A functional analysis 

is a type of explanation that relies on a decomposition of a system into its component parts 

and in effect explains the working of the system in terms of the capacities of the parts and the 

way the parts are integrated with one another. For the sake of illustration, you could provide a 

functional analysis of an ordinary digital computer by decomposing it into its parts (the CPU, 

RAM etc.) and explaining how they interact with each other. In cognitive science, the core 

idea behind something that could be called “computational functionalism” (or, more to the 

point, perhaps: the computer model of the mind) is that our mind, our mental life, can be 

decomposed by a functional analysis of mental processes to the point where they are seen to 

be composed of computations as mechanical as the primitive operations of a digital computer. 

The key notions of functionalism in this sense are representation and computation. Mental states 

are seen as systematically representing the outside world, and mental processes are seen as 

algorithmically specifiable computations over these representations. This, however, is a 

decidedly cognitive science perspective. It is related to, but not exactly identical to what is 

meant by “functionalism” in the philosophy of mind. 

Functionalism in the philosophy of mind, you may also call it “metaphysical 

functionalism,” is a theory of the nature of the mind, rather than a theory of the best kind of 

psychological explanation to be given of the mind. It is not concerned with how mental states 

or properties account for behavior, but rather with what they are. Like the identity theory, 

which was also decidedly a theory about what mental states or properties are (viz., brain states 

or neurophysiological properties), functionalism makes a claim about what mental states or 

properties are: What is it that all and only those who have a certain mental property M, say 

pain, have in common in virtue of which they count as having M? The identity theory would 

have said: a certain neurophysiological property. That answer, however, seems no longer 

viable. But what else could it be, then, that all and only those who have a certain mental 

property M have in common in virtue of which they count as having M?  

To understand functionalism’s answer, consider again the mouse trap analogy 

introduced in lecture 2. Mouse traps are device for catching or killing mice. Mouse traps can 

be made of almost any material, and perhaps indefinitely many designs could be employed. 

The most familiar sort involves a wooden platform and a metal strike bar that is driven by a 

coiled metal spring and can be released by a trigger. But there are mouse traps designed with 

adhesives, boxes, poisons, and so on. All that matters to something’s being a mouse trap, at 

the end of the day, is that it is capable of catching or killing mice. 

Now contrast mouse traps with diamonds. Diamonds are valued for their hardness, 

their optical properties, and their rarity in nature. But not every hard, transparent, white, rare 

crystal is a diamond. Diamonds are carbon crystals with a specific molecular lattice structure, 

and only something with that structure can be a diamond. Being a diamond basically is a matter 

of being a certain kind of physical stuff. A “physical stuff kind” such as being a diamond (or, to 

use our previous example, being water) has a physical or structural essence that has been or at 

least can be discovered scientifically. Such entities are entities of a certain composition or 

constitution, quite independently of what they do or can be used to do. It happens that 

diamonds can cut glass, but so can many other things that are not diamonds. And even if no 

diamond ever did or could cut glass, they would not cease to be diamonds. The important 

point is that not all things are “physical stuff kinds” in this sense. Some things are essentially 

constituted by their relations to other things, and by what they can do, rather than by what 

they are made of. The most obvious examples are artifacts like mousetraps and keys: Like being 

a mouse trap, being a key is not a matter of being a physical thing with a certain composition, but 

a matter of being a thing that can be used to perform a certain action, namely, opening a lock. 

There may be metal keys, wood keys, plastic keys, or digital keys. What makes something a key 

is not its material composition or lack thereof, but rather what it does, or could do, or is 

supposed to do. The activities that a key does, could do, or is supposed to do may be called its 

functions. So one can say that keys are essentially things that have certain functions, i.e., they are 

functional entities or functional kinds. 

(Metaphysical) functionalism in the philosophy of mind is the theory that mental states 

are more like mouse traps and keys than they are like diamonds. What makes something the 

mental state or property it is is (more) a matter of what it does, not what it is made of.  

Mental properties, according to functionalism, are functional kinds. They are complex 

properties that consist in an organism’s having some (physical) property or other that plays a 

certain functional or causal role. Functional or causal roles can be specified by three kinds of 

clauses: input clauses, output clauses and interaction clauses. (1) Input clauses say which 

conditions (typically) give rise to (cause) the instantiation of the mental property at issue. (2) 

Output clauses say which behavioral responses the instantiation of the mental property at issue 

(typically) gives rise to (effects). (3) Interaction clauses say how the instantiation of the mental 

property at issue (typically) interacts with the instantiations of other mental properties. For 

instance, in order for an organism to be in pain is for it to be in an internal state caused by 

bodily damage (input clause), that typically causes winces, groans, and escape behavior (output 

clause) and makes the organism hope for relief and induces the belief that this in unpleasant 

(interaction clause). What makes all instances of pain instances of the mental type pain, 

regardless of their exact physical nature, is that they fulfill this functional or causal profile: 

Nothing that plays this functional or causal role can fail to be a pain, no matter what its actual 

physical realization is. And nothing that does not play this functional or causal role can be 

pain, no matter how close its physical realization resembles typical physical realizations of 

pain. 

For the sake of illustration or analogy, consider the following simple characterization of 

what it is for something to be a vending machine that accepts 50 Cents and 1 Euro coins and 

charges 1 Euro for a coke. Suppose the machine is in its initial state, i.e., no money has been 

entered. Call this state “S1.” If 1 Euro is entered (input), the machine will have to give out a 

coke (output) and remain in state S1 (interaction; see below). If the machine is in its initial state 

S1 and 50 Cents are entered (input), it should give out no coke (output), but register that some 

money has been paid already, for instance by moving into another internal state S2 (interaction 

with other internal states). Now suppose the machine is in state S2. If another 50 Cents are 

entered (input), the machine will have to give out a coke (output) and go back to S1 

(interaction with other internal states). If instead 1 Euro is entered, the machine will have to 

give out a coke (output) and stay in S2, for there still have been 50 Cents overpaid 

(interaction). This is it. This is what it is for something to be a vending machine that accepts 

50 Cents and 1 Euro coins and charges 1 Euro for a coke. What makes all instances of a 

vending machine of this kind instances of a vending machine of this kind, regardless of their 

exact physical nature, is that they fulfill this functional or causal profile: Nothing that plays 

this functional or causal profile can fail to be a vending machine that accepts 50 Cents and 1 

Euro coins and charges 1 Euro for a coke, no matter what its actual physical realization is. 

And nothing that does not play this functional or causal profile can be a vending machine that 

accepts 50 Cents and 1 Euro coins and charges 1 Euro for a coke, no matter how close its 

physical realization resembles typical physical realizations of such vending machines. 

According to functionalism, this is how being in pain and other mental properties work as well.  

According to the identity theory, then, every mental property is (identical to) some 

physical property, and what all and only the instances of a mental property have in common is 

precisely that they are an instance of this physical property.  

According to functionalism, in contrast, mental properties are structural properties that 

are multiply realizable by different physical properties that fulfill the functional or causal role 

of the mental property in question, and what all and only the various instances of a mental 

property have in common is that they occupy a certain functional role. That is, a mental 

property like being in pain is a complex higher-level (functional or second-order) property that 

consists in having some lower-level (physical or first-order) property or other that occupies 

the functional or causal role definitive of being in pain. 

There are a couple of important conceptual distinctions that have been around in the 

literature. In case you come across one of them, here are the three most important 

distinctions. 

One important distinction is that between machine functionalism (also: computer functionalism) 

and causal-role functionalism. This distinction alludes to different (but not essentially different) 

ways of specifying the functional role of mental properties. Machine functionalism specifies the 

functional role of mental properties in terms of machine tables that list all possible inputs, 

outputs and relations to other internal/mental states. In the case of the vending machine, e.g., 

the corresponding machine table could look like this:  

 

Causal-role functionalism, in contrast, specifies the functional profile of mental properties 

explicitly in terms of actual causal roles, e.g., by saying: For an organism to be in pain is for it 

to be in an internal state caused by bodily damage, that typically causes winces, groans, and 

escape behavior and makes the organism hope for relief and induces the belief that this in 

unpleasant. 

Within causal-role functionalism there is a further distinction that has to do with who 

exactly is supposed to be responsible for specifying the functional role of a given mental 

property. According to analytical functionalism (or common sense functionalism), it is the platitudes of 

folk psychology, i.e., what we all know about what it is to be in, say, pain. According to 

analytical functionalism, defining the functional profile of a mental property does not require 

us to do any empirical science, we can do it from our cozy armchairs. According to scientific 

functionalism (or psycho-functionalism), in contrast, empirical psychology is in charge of providing 

the characterizations of functional roles, so that we actually have to do empirical science in 

order to say what pain is. There are philosophical reasons for and against these approaches, 

and I will mention one of them briefly below in section 3, but apart from that, these 

distinctions, albeit they are worth having in mind, won’t play any significant role in what 

follows. 

The third distinction, however, is important. It allows you to see exactly how Lewis’ 

theory fits into the picture. This third distinction is between what has been called “role 

functionalism” and “filler functionalism.” The question that separates these two versions of 

functionalism is: What do we identify mental states with? As we saw, the identity theory 

identifies a mental property M like being in pain with a physical property P. In contrast, the 

typical functionalist á la Putnam identifies a mental property M like being in pain with a 

complex, structural higher-level property F that is defined by quantification over lower-level 

physical properties: M is the higher-level property of having some lower-level property P1, …, 

Pn or other that occupies the functional role definitive of M. This kind of functionalism goes 

under the name of “role functionalism.” Being in pain is a role property, the property of having 

some property or other that fills a certain functional role. What the identity theory and 

classical role functionalism have in common is that they make claims about every possible world. 

If the identity theory is correct, then being in pain is having firing c-fibers in every possible world. 

And if functionalism is correct, then being in pain is having some property or other that fills the 

functional role of pain in every possible world. 

But note that there is a third option left: Being in pain could neither be a single physical 

property (identity theory) nor the higher-order property of having some property or other that 

fills a certain functional role (role functionalism) but rather whatever physical property (typically) 

happens to fill the functional role of pain in a species, possible world etc. This is what is known as “filler 

functionalism.” Filler functionalism is a version of functionalism because we first of all need 

to define the functional profile of the mental property in question. But then, we don’t take the 

mental property to be the functional property of having some property or other that plays that 

functional role, but with the actual properties that fill this role, and these might be different 

across species, worlds etc. This is Lewis’ position. We cannot identify being in pain with the 

functional property of having some property or other that plays the pain role, because then 

the mad man wouldn’t be in pain. But we cannot identify being in pain with a single physical 

property either, because then the Martian wouldn’t be in pain. Yet, if we identify being in pain 

with whatever physical property (typically) happens to fill the functional role of pain in a species, possible world 

etc., we can accommodate both cases: The mad man is in pain because he is instantiating a 

physical property that in us (typically) plays the pain role, and the Martian is in pain because he 

is instantiating a physical property that (typically) plays the pain role in Martians.  

Before we turn to the arguments pro and con functionalism, a last, but important, word 

on the ontological neutrality of functionalism. Many philosophers took and take functionalism to 

be a physicalistic theory of the mind that avoids the inherent problems of behaviorism and the 

identity theory: (1) It accepts mental states as inner states (in contrast to behaviorism), (2) it 

allows for multiple realization (in contrast to the type identity theory), and (3) it shows (at least 

in its role version; see above) that all and only the tokens of a mental type have something 

significant in common. Even if this is true, however, functionalism is not necessarily a 

physicalist position. Strictly speaking, functionalism is ontologically neutral. Qua functionalism, 

functionalism does not specify what ontological status the states that occupy the functional 

roles of mental properties have. If our world should indeed turn out to contain immaterial 

angels in which the pain role is played by some immaterial form of ectoplasm, then 

physicalism and materialism would be wrong, but nevertheless functionalism could be 

vindicated. Therefore, in order for them to adopt a physicalist position, functionalists have to 

explicitly add the further principle that all mental properties are realized only by physical 

properties. 

 

2. Arguments for functionalism 

What reasons are there for embracing functionalism? As often in philosophy, the case for 

functionalism is basically the case against its two most influential contenders (historically 

speaking): behaviorism and the identity theory. The functionalist’s argument against the 

identity theory is the multiple realizability argument. The functionalist’s argument against 

behaviorism is the charge that mere input and output are not sufficient for determining an 

organism’s mental life, if the interrelationships between internal mental states aren’t properly 

taken into account as well. Consider a “super actor” who is unable to feel pain but who acts as 

if in pain every time he hits his finger with a hammer etc. The behaviorist, the functionalist 

claims, will have to say the super actor is in pain; the functionalist, however, has an easy 

explanation for why he isn’t: unless his internal state is related in the right way to other mental 

states, he is not on pain, and his internal state does not make him hope for relief and it does 

not induce the belief that this in unpleasant etc. 

Apart from that, there are two other arguments, or better perhaps: considerations, in 

support of functionalism, neither of which I find terribly convincing. 

The “optimistic argument” for functionalism leans on the possibility of building 

artificial minds: Surely, one could build a creature that has mental states but differs from 

humans in its brain states, as long as it satisfies the right kind of functional profile. That is, the 

possibility of artificial intelligence seems to require the truth of something like functionalism. 

Functionalism, according to this line of reasoning, views the mind very much as an engineer 

does: Minds are mechanisms, and there is usually more than one way to build a mechanism. 

The trouble with the optimistic argument is that it is too close to question-begging: It assumes 

that one can create artificial thinking things without duplicating the kinds of brain states that 

human beings have, and that is just what those inclined towards something like the identity 

theory deny. 

The “pessimistic argument” claims that the alternatives to functionalism would leave 

people unable to know about and explain the mental states of one another, or of other 

creatures. After all, if two creatures function in the same ways, achieve the same results, have 

isomorphic internal states, etc., then what could justify the claim that one has mental states 

and the other does not? The identity theory says that the justification has to do with what 

kinds of stuff the creatures are made of: only the one with the right kind of brain counts as 

having mental states. But, the functionalist say, this flies in the face of our ordinary practices 

of understanding, attributing, and explaining mental states, which doesn’t rely on which brains 

state someone is in. This is more convincing, but it doesn’t seem to favor functionalism over 

behaviorism, for we ordinarily base our attributions of mental states not on input, output and 

interrelationships between internal states, but only on input and output – and this is exactly 

the part where functionalism and behaviorism don’t differ. 

If I were a functionalist (which I am not), I would probably try to dig in my heels. That 

is, I would count on the fact that my position is superior to its major alternatives: Here I 

stand, refute me if you can, and then do a better job. I won’t be able to tell you exactly how I 

would try to do a better job. But I can tell you how I would try to refute functionalism. 

 

3. Problems for functionalism  

There are a couple of standard objections against functionalism. One of them is rather 

theoretical and concerns the problem of exactly how to specify inputs and outputs. The other 

two are based on thought experiments. 

 

3.1 How to specify inputs and outputs? 

The functionalist characterizes mental properties in terms of inputs and outputs to a system. 

Exactly how, however, are the inputs and outputs themselves characterized? We already noted 

above that there is a dispute amongst functionalism concerning the question whether the 

characterization is in terms of folk psychological platitudes or whether the empirical sciences 

do have a say in it. So suppose empirical considerations do play a role. How could one then 

characterize inputs and outputs? 

One possibility would be to say that inputs are the electro-chemical signals that the 

brain gets from the sense organs, and that outputs are the electro-chemical signals the brain 

sends to the muscles. For one thing, however, this makes the brain the system that receives 

the inputs and produces the outputs and thus the logical subject of mental states. It would be 

the brain that is pain, has beliefs etc. This seems to conflict with our common sense 

understanding of mental states: We ascribe mental states to persons, not to brains! 

Furthermore, this position leads straight back to precisely the kind of speciecism (or 

chauvinism) that functionalism (rightly) criticized about the identity theory: only creatures that 

have brains could be in functional states so described. 

A second possibility that would at least resolve the first problem would be to say that 

inputs are the physical stimuli that are processed by our sense organs, and that outputs are the 

movements of our extremities. In this case, the relevant system and thus the bearer of mental 

states would indeed be us. The problem is: This still seems to be too chauvinistic or 

anthropomorphic, for it reserves mental states for creatures having our kinds of sense organs 

and extremities. At a more general level, the worry here is that since empirical psychology is 

too much focused on humans or mammals (or at least terrestrial creatures), we will give up 

precisely what made functionalism so appealing to begin with, viz., its potential to account for 

the multiple realizability of mental properties, if we let empirical psychology define what 

counts as the inputs and outputs required for a system to have a given mental property. What 

needs to be done in order to capture the true functionalist spirit, it seems, is to characterize 

them abstractly in a way that disregards facts about the bodily make-up of humans entirely.  

A third possibility that promises to accommodate this worry is to say that inputs are 

diverse events and situations in the environment we are in, and outputs are the changes in our 

environment we cause by our behavior in response to the inputs. What the functionalist has to 

seek is thus functional characterizations of mental states such as “s believes that Paris is the 

capital of France iff s receives such-and-such input, s produces such-and-such output and s’s 

internal mental states are related in such-and-such a way.” But how plausible is it to suppose 

that there are such “iff”-clauses relating mental states like beliefs with events and situations in 

the external world? How plausible is it to suppose that there are specific environmental 

situations that cause certain mental states, say the belief that Paris is the capital of France and 

only that belief? How can the functionalist of this kind non-vacuously distinguish between the 

inputs and outputs that lead me to believe that there are at least 29 people in front of me right 

now as opposed to those that lead me to believe that there are at least 30 people in front of 

me right now, and, moreover, in a way that holds true for any believer who has these beliefs? 

 

3.2 Strange realizations (and Searle’s Chinese Room) 

Since functionalism holds that being is doing, two systems that do the same things, more 

precisely: that are functionally the same, should also be the same with respect to their mental 

states. But many have thought that there is nothing to guarantee that this is indeed the case. 

One way to illustrate the objection is to envisage strange realizations: Systems that implement the 

functional profile of you or me, but that are so different from us that we would not be 

prepared to ascribe to them the same mental life that we have. 

Imagine that AI has advanced to the point where an android with a “brain” consisting 

of a computer running a program can behave much as normal humans do, maybe by 

mimicking the operation of a human brain at a neuron-by-neuron level. Next, note that from a 

functionalist perspective it won’t matter if the computer running this program is inside the 

android’s body (in its head, say), or outside of it and connected by a two-way radio link to it. 

As long as the functional profile is preserved, according to functionalism, nothing will change, 

mental wise. The final step of what has come to be called the “Chinese Nation Argument” 

against functionalism (originally due to Ned Block) gives us the “China brain”: Suppose that 

instead of the program being run on an external computer made of silicon chips, the entire 

population of China is enlisted to run the simulation. They are equipped with walkie-talkies 

and communicate with each other in a way that implements the functional profile of the 

computer, aká our brain. If they’re doing a good job, the android will behave in the various 

situations that confront it as before, and thus pretty much the same as we do. Again: As long 

as the functional profile is preserved, according to functionalism, nothing will change, mental 

wise. 

All the same, it seems fair to ask whether we should really be prepared to attribute our 

kind of mentality to the “China brain” system consisting of the robot plus the population of 

China. Functionally, the system is completely like us. The difference lies in the dramatic 

difference in how the functional roles are realized, and that difference counts for nothing as 

far as mentality is concerned, according to functionalism. Nevertheless, there has been a time 

where enough professional philosophers had the intuition that the system consisting of the 

robot plus the population of China cannot have beliefs, desires, feelings etc., at least not of the 

same kind as we do, for this to count as a serious objection to functionalism. The problem, 

however, is that from the perspective of functionalism, this argument just begs the question: 

For the functionalist, if the functional profile remains the same, then, yes, mentality does 

remain the same as well. If you’re willing to live with the Martian who is pain in virtue of 

having inflated cavities in its feet, then why not also live with the “China Brain”?  

Personally, I think that what this debate shows is that in its attempt to distance itself 

from the too restrictive focus on the material make-up of a system characteristic of the 

identity theory, functionalists have erred in the other direction by making (or trying to make) 

their theory too material-independent. In the heyday of functionalism, the general idea was 

that material doesn’t matter at all. “We could be made of Swiss cheese and it wouldn’t matter” 

Putnam once quipped (1975, 134). I always found this striking. After all, functional roles are 

causal roles, and not just anything can implement any causal role. Try to make a mouse trap 

out of Swiss Cheese and see what happens. Nevertheless, being a mousetrap is functional kind. 

It’s just that the functional role can be played by many, but not by arbitrarily many, physically 

different systems. Sometimes there is a more intimate connection between the physical 

properties of an object and the functional properties it is capable of having. Carving glass is the 

second-order property of having a property responsible for having more than five degrees on 

the Mohs scale: being a topaz, being a corundum, and being a diamond are different ways for 

something to have the property of carving glass. Nevertheless, objects having that property 

cannot be made of Swiss cheese: in order to carve glass, an object must have a very specific 

molecular structural property. Ned Block expresses this in his Disney Principle: “[i]n Walt 

Disney movies, teacups think and talk, but in the real world, anything that can do those things 

needs more structure than a teacup. … laws of nature impose constraints on ways of making 

something that satisfies a certain description” (Block 1997, 120). The same, I think, holds for 

human mentality: Maybe you don’t have to have the exact same brain as we do, in order to 

have a mind like ours. Maybe it suffices to be a system with the same functional profile. But 

maybe, also, you have to have something that at least closely resembles our brain in important 

respects in order to be a system with the same functional profile. In fact, this position sort of 

resembles a conclusion that John Searle reaches in the discussion of another thought 

experiment, the Chinese Room Argument. 

Searle’s famous Chinese Room thought experiment attacks not functionalism directly, 

but the (related) idea that machines could ever, solely in virtue of their being programmed in a 

certain way, show something like genuine semantic understanding, a claim that he attributes to 

what he calls “strong artificial intelligence.” A person who doesn’t understand Chinese is 

locked in a room. She receives three stacks of papers with Chinese writing from outside 

together with instructions in English that tell her how to modify the Chinese characters based 

on their form and their formal relationships to one another. Working with these rules, the 

person is supposed to write down Chinese symbols on some empty sheets of paper and return 

them to those waiting outside. The three Chinese texts are a story, a corresponding script that 

provides additional information and a set of questions about the story. The series of Chinese 

symbols the person creates are understood by Chinese native speakers as answers to the 

questions. The English instructions are a “program” in the sense of the Computer Model of 

the Mind, whose formal rules allow the person in the Chinese Room to transfer Chinese 

symbols in such a way that she gives answers to the questions in Chinese that are 

indistinguishable from the answers provided by Chinese native speakers. Since the person 

inside the room nevertheless doesn’t understand a single word of Chinese, Searle claims, the 

thesis of strong AI is wrong: Formal symbol crunching alone is not sufficient for the kind of 

semantic understanding that produces the (indistinguishable) answers of Chinese native 

speakers (although it might help us understand it, as “weak AI” claims).  

Searle’s controversial argument does not claim that machines per se can’t have semantic 

understanding. According to Searle, Chinese native speakers are in fact machines of a certain 

kind (just as you and me). It only claims that machines can’t have semantic understanding in 

virtue of their being programmed in a certain way. The right syntax is thus not sufficient for 

semantics: “[Y]ou cannot milk semantics out of syntactical processes” (Searle 1997, 12). 

According to Searle’s so-called biological naturalism (Searle 1992), a machine capable of 

semantic understanding and real “intrinsic” intentionality – as opposed to mere “as-if”-

intentionality – not only has to be programmed in the right way, it also has to possess a kind 

of hardware that has “causal powers” similar to those of the human brain. 

 The strange realization objection and Searle’s Chinese Room argument focus on 

contentful, mental states like beliefs, i.e., on what is generally called an “intentional mental 

state.” Some philosophers hold that functionalism is a good theory of intentional states but 

fear that it nevertheless fails because it cannot explain other sorts of mental states: in 

particular, they say that it cannot explain sensations and other conscious mental states, the 

notorious qualia. 

 

3.3 Inverted and absent qualia 

Let us suppose that a person has inverted qualia. That is, when she looks at a tomato or at the 

firemen’s car she has the experiences we have when looking at cucumbers, zucchinis, or frogs. 

Nevertheless, since she has this anomaly since birth she has adapted to respond in the right 

way to questions. If asked: “What color do tomatoes have?”, she answers: “red.” Equally, if 

asked what color cucumbers have, she says: “green.” That is, her color sensations play exactly 

the same functional or causal role they play in us. She makes exactly the same distinctions, 

names the things she sees correctly, etc. 

Philosophical zombies (not the one’s featured in bad horror movies) are creatures that 

do not have inverted qualia, but no qualia at all, although they also behave exactly like us and 

in fact implement our functional profile. From a functionalist perspective, there is no 

difference between them and us: they are in states that play the functional role of our pain 

states, greenish experiences etc., and yet these states in them are not accompanied by any feel 

at all. In them “it’s all dark,” there’s no consciousness at all. 

Isn’t it essential for at least some of our mental states to be accompanied by certain 

feelings and qualitative states? And if creatures with absent and inverted qualia are possible, 

then how can we assume that functionalism captures the mental? It just doesn’t sound true to 

say that everything that plays the pain role is thereby, ipso facto, pain, no matter what its 

physical constitution is and no matter how it feels. Some, instead, will be tempted to say: 

Look, no matter whether or not that mental state of mine plays the pain role, as long as it 

doesn’t feel painish, it’s no pain! Pain is essentially painful, and since painfulness escapes a 

characterization in terms of functional roles, functionalism can’t capture the essence of pain, 

or, for that matter, qualia in general.

 

