IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

2109

Improved Quasi-Newton Adaptive-Filtering

Algorithm

Md Zulﬁquar Ali Bhotto, Student Member, IEEE, and Andreas Antoniou, Life Fellow, IEEE

Abstract—An improved quasi-Newton (QN) algorithm that per-
forms data-selective adaptation is proposed whereby the weight
vector and the inverse of the input-signal autocorrelation matrix
are updated only when the a priori error exceeds a prespeciﬁed
error bound. The proposed algorithm also incorporates an im-
proved estimator of the inverse of the autocorrelation matrix. With
these modiﬁcations, the proposed QN algorithm takes signiﬁcantly
fewer updates to converge and yields a reduced steady-state mis-
alignment relative to a known QN algorithm proposed recently.
These features of the proposed QN algorithm are demonstrated
through extensive simulations. Simulations also show that the
proposed QN algorithm, like the known QN algorithm, is quite
robust with respect to roundoff errors introduced in ﬁxed-point
implementations.

Index Terms—Adaptation algorithms, adaptive ﬁlters, conver-
gence speed in adaptation algorithms, quasi-Newton algorithms,
steady-state misalignment.

I. INTRODUCTION

T HE least-mean-squares (LMS) algorithm minimizes the

Weiner-Hopf function iteratively by using the instanta-
neous values of the autocorrelation function of the input signal
and the crosscorrelation function between the input and desired
signals [1]. Due to its simplicity, the LMS algorithm is fre-
quently used in current practice. However, when the input signal
is highly colored or bandlimited, the LMS algorithm as well as
other algorithms of the steepest-descent family converge slowly
and the capability of such algorithms in tracking nonstationar-
ities deteriorates. In such situations, more sophisticated algo-
rithms that belong to the Newton family are preferred. However,
the computational complexity of these algorithms is usually pro-
hibitively large especially in real-time applications where low-
cost digital hardware must be employed. Numerical instability
is also a major issue in these algorithms. The conventional re-
cursive least-squares (CRLS) algorithm converges much faster
than algorithms of the steepest-descent family [1]. However, it
can become unstable and if a large forgetting factor is chosen
it can actually lose its tracking capability. The known quasi-
Newton (KQN) algorithm reported in [2], [3] offers better nu-
merical robustness whereas the LMS-Newton (LMSN) algo-

Manuscript received April 14, 2009; revised September 26, 2009; accepted
November 11, 2009. Date of publication February 05, 2010; date of current
version August 11, 2010. This work was supported by the Natural Sciences and
Engineering Research Council of Canada. This paper was recommended by As-
sociate Editor H. Johansson.

The authors are with the Department of Electrical and Computer Engineering,
University of Victoria, Victoria, BC V8W 3P6, Canada (e-mail: zbhotto@ece.
uvic.ca, aantoniou@ieee.org).

Digital Object Identiﬁer 10.1109/TCSI.2009.2038567

rithms reported in [4] offer better convergence performance than
the CRLS algorithm.

Two methods are available for the development of Newton-
type adaptation algorithms: methods based on the direct solu-
tion of the normal equations of a least-squares problem (see
Section II) and methods based on the orthogonal decomposition
of the input-signal matrix. The CRLS, KQN, and the LMSN
algorithms are based on the normal equations and the QR de-
composition algorithm (QRD) is based on the orthogonal de-
composition of the input-signal matrix [5]. The computational
designated as
complexity of these algorithms is of order
. The fast QRD (FQRD) algorithms in [6]–[12] are ac-
tually efﬁcient implementations of the QRD algorithm whose
. Fast RLS (FRLS) algo-
computational complexity is of
are also avail-
rithms with computational complexities of
able in the literature for FIR adaptive ﬁltering and autoregres-
sive (AR) prediction [5], [13]–[16]. FRLS algorithms exploit
the Toeplitz structure of the input-signal autocorrelation ma-
trix. The fast QN (FQN) algorithm reported in [17], which has a
, also exploits the Toeplitz
computational complexity of
structure of the autocorrelation matrix to reduce the computa-
tional complexity. The FRLS and FQRD algorithms suffer from
numerical instability problems that are inherited from the nu-
merical instability problems of the CRLS and QRD algorithms,
respectively, and also the simpliﬁcations used to obtain these
algorithms [8]. However, the FQRD algorithm reported in [8]
offers numerically stable operation in low-precision implemen-
tations and in the absence of persistent excitation. The numer-
ical instability problems associated with the CRLS algorithm
are discussed in [18] where an upper bound on the relative pre-
cision to assure the BIBO stability of the CRLS algorithm in
stationary and nonstationary environments is derived. Formulas
for choosing the forgetting factor to avoid explosive divergence
for a given precision in the CRLS algorithm are also given in
[18]. However, these formulas were derived on the assumption
that the input signal is persistently exciting. Furthermore, the
input-signal statistics must be known a priori in order to use
these formulas. Consequently, a prudent strategy for the deriva-
tion of fast Newton-type algorithms would be to start with a
parent algorithm that is inherently stable. The numerical robust-
ness of the quasi-Newton (QN) algorithm reported in [2], [3] is
achieved by using a biased estimate of the autocorrelation ma-
trix, which can reduce the tracking capability of the algorithm
relative to that of the CRLS algorithm (see [19, p. 678]).

In this paper, we propose an improved version of the QN algo-
rithm reported in [2], [3] that incorporates data-selective adap-
tation. The proposed QN (PQN) algorithm takes fewer weight
updates to converge and yields a reduced steady-state misalign-

1549-8328/$26.00 © 2010 IEEE

2110

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

is the step size and

is an estimate of the inverse
where
of the autocorrelation matrix at iteration . The a posteriori error
at iteration

is deﬁned as

(4)

is determined by minimizing

In the KQN algorithm,
with respect to
selective weight adaptation is performed whereby
only when the magnitude of the a priori error
prespeciﬁed error bound . In other words, whenever

in every iteration. In the PQN algorithm, data-
is updated
is greater than a
,
. The associated

is chosen to force the equality

optimization problem can be stated as

Fig. 1. Adaptive ﬁlter.

Straightforward analysis leads to

if
otherwise

(5)

(6)

ment relative to the KQN algorithm in [2], [3]. These features of
the new algorithm are demonstrated through MATLAB simula-
tions in stationary and nonstationary environments. Simulations
also show that the PQN algorithm, like the KQN algorithm, is
quite robust with respect to roundoff errors in ﬁxed-precision
implementations.

The paper is organized as follows. In Section II, the pro-
posed QN algorithm is described. Performance analysis is car-
ried out in Section III. Simulation results for stationary and non-
stationary environments are given in Section IV. Finally, con-
clusions are drawn in Section V.

II. PROPOSED QUASI-NEWTON ALGORITHM

The simplest and most commonly used conﬁguration
the tapped delay-line structure
for adaptive ﬁltering is
illustrated in Fig. 1, which is essentially an FIR dig-
and
ital ﬁlter. Vectors
represent the weight and
input-signal sequences at iteration , respectively. The a priori
error signal at iteration

is given by

where
weight vector,
optimization problem

is the desired signal. Adaptation algorithms update the
, in such a way as to obtain the solution of the

(1)

where
is the expectation operator. This is usually referred
to as the Wiener solution. Adaptation algorithms use either the
steepest-descent or the Newton direction. LMS-type algorithms
use the steepest-descent direction and, therefore, their conver-
gence rate depends on the nature of the objective function and
spread of the eigenvalues of the Hessian matrix of the objective
function. Most algorithms of the Newton family use the update
formula

holds true during each update, we use

and

(2)

(8)

(9)

(10)

where

and

if
otherwise

As in the KQN algorithm in [3], the PQN algorithm also uses the
rank-one quasi-Newton updating formula given in Eq. (7.20) in
[20] to deduce the update of the inverse of the autocorrelation
matrix as

(7)

The estimator in (7) satisﬁes the classical QN hereditary con-
dition and, therefore, the associated algorithm belongs to the
QN family according to Fletcher’s classiﬁcation in [21]. For the
adaptation of
along
the Newton direction

is chosen to be in proportion to

. Since the relation

,

Unfortunately, in the context of adaptive ﬁltering, gradient
not available at iteration since it requires future data

is
and

. However, parameter

in (10) can be approximated as

(3)

(11)

BHOTTO AND ANTONIOU: IMPROVED QUASI-NEWTON ADAPTIVE-FILTERING ALGORITHM

TABLE I

PROPOSED QN ALGORITHM.

Using (1) in (14), straightforward manipulation yields

Using the normal equation

in (17) and simplifying the expression obtained, we have

Now using (13) and (15) in (18), we obtain

2111

(17)

(18)

(19)

which comprises the normal equations of the objective function

(20)

. Hence the weight-vector up-
where
date equation of the PQN algorithm in (14) minimizes the ob-
jective function in (20).

As can be seen, the objective function of the PQN algorithm
is similar to that of the weighted LS or LMSN algorithm (see,
(8), [4]). The updating formulas for the KQN algorithm are

where
tuting (1) and (8) in (11), we obtain

and

are deﬁned in (1) and (4), respectively. Substi-

Now substituting (9) and (12) in (7), we obtain an update for-
mula for the inverse of the autocorrelation matrix as

(12)

where

(13)

(21)

(22)

(23)

The weight-vector update formula can be obtained by using (6)
in (3) as

The objective function of the KQN algorithm, on the other hand,
assumes the form

(14)

and

only if the a priori error
Updates are applied to
exceeds the prespeciﬁed error bound as is done in the basic QN
optimization algorithm (see [20, p. 184]). Otherwise, no updates
are applied. The PQN algorithm can be implemented as detailed
in Table I.

The crosscorrelation vector of the PQN algorithm can be de-

ﬁned as

(15)

. If we apply the matrix inversion
where
lemma given in [1], [22] to (13), we obtain the input-signal au-
tocorrelation matrix as

(16)

(24)
where
. It turns out that in the KQN al-
in the estimator in (21) approaches
gorithm the value of
zero, as can be veriﬁed by examining Fig. 4 in [3]. As a result,
the adaptation of
will stop after a certain number of itera-
whereas the basic QN op-
tions regardless of the value of
timization algorithm as reported in [20] suggests that the adap-
tation of
becomes
sufﬁciently small. Consequently, the steady-state value of the
misalignment and the speed of convergence will be affected.

should continue until the value of

An unbiased estimate of the inverse of the input-signal auto-
correlation matrix cannot be obtained by using the rank-one up-
date formula given in (7). However, the undesired consequences
of using a biased estimate in the adaptation of the weight vector

2112

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

is bounded (see Theorem 2 below),

in (20) can also be observed in
and it is also bounded due to the fact that
can approach zero
, i.e., when the solution of (20) is achieved.
;
is large during transience and becomes small at

and during steady state,

The temporal behavior of

Fig. 2. Since

only when
During transience,
therefore,
steady state.

Fig. 2. Evolution of = .

can be avoided by using a convergence factor
autocorrelation matrix in (16) can be expressed as

in (3) [22]. The

[3], [4]. Furthermore,

The stability of Newton-type algorithms depends on the pos-
itive deﬁniteness of
must be
bounded for a bounded input signal. Otherwise, the BIBO sta-
bility of the algorithm cannot be assured. The formula in (14)
could also lead to a biased solution for an unbounded
. Both
of these requirements are satisﬁed in the PQN algorithm ac-
cording to the following theorems.

Theorem 1: If

is a symmetric positive deﬁnite matrix,
is also a symmetric positive deﬁnite matrix for all

then

.

Proof: Since

is a real symmetric matrix, we can ex-

press

as

(25)

is a weighted sum
As can be seen, the update formula for
and the weights depend on the input-
of the outer product
signal statistics and the a priori error signal. Taking the expec-
tation of both sides in (25) and invoking the assumption that
and

are statistically independent, we obtain

where
let

is a unitary matrix such that

. If we
, then for any nonzero vector

and

(27)

, the relation in (13) can be used to obtain

(26)

An expression for the expectation of

is difﬁcult to de-
duce but an approximate value can be obtained in terms of its
time average based on simulation. In such an experiment the
is the vari-
error bound can be chosen as
for a
ance of the measurement noise. The evolution of
white Gaussian input signal with zero mean and unit variance
is
assuming a variance of the measurement noise,
illustrated in Fig. 2. This was determined by averaging the en-
in 100 runs in a 36th-order system identiﬁcation
semble of
application.

where

, of

Note that since

for

and

as

is

positive deﬁnite (see Theorem 1 below), we have
for
be speciﬁed if

, the value of

. Since

.

does not need to

with respect to

As can be seen in Fig. 2, the time average is a positive quan-
tity and, therefore, on the average a signiﬁcant improvement can
. The
be brought about in the estimate of
effect of using a biased estimate on the weight vector will, there-
fore, be less pronounced in the proposed algorithm as the quan-
in (6) approaches zero at steady state.
tity
Since
, the weights used in (20)
and (25) are nonnegative. Therefore, if
is positive deﬁnite it
is straightforward to show that the estimate in (16), i.e., the Hes-
sian of (20), would remain positive deﬁnite indeﬁnitely. Con-
sequently, the objective function in (20) would remain convex
indeﬁnitely.

in the step size
and

for all

Substituting (27) and the deﬁnitions of
equation, we obtain

and

in the above

Since
side in the above equation will occur when

, the lowest possible value of the right-hand
and, therefore,

(28)

for any
for all
by noting the symmetry of

. Hence the estimate in (13) is positive deﬁnite
can be easily demonstrated

. The symmetry of

.

Theorem 2: The estimate of

in the sense that the quadratic factor
vided that the input signal is bounded.

in (13) is always bounded
is bounded pro-

BHOTTO AND ANTONIOU: IMPROVED QUASI-NEWTON ADAPTIVE-FILTERING ALGORITHM

2113

Proof: If we premultiply both sides in (13) by

and post-

multiply them by

, we obtain

(29)

Since

holds true for each adaptation, we have

(30)

Therefore, we conclude that if the input signal is bounded, then

is also bounded.

III. ANALYSIS OF THE PROPOSED QN ALGORITHM

In this section, we examine the performance of the PQN al-
gorithm in terms of the mean square-error (MSE) after initial
convergence in stationary and nonstationary environments. The
update formula for the weight vector in the PQN algorithm is
similar to that of the LMS-Newton algorithm given in (29) in [4,
p. 620]. The difference resides in the estimation of the inverse of
. The KQN
the autocorrelation matrix and the reduction factor,
instead of a prespeciﬁed ﬁxed reduction
algorithm uses
factor
and the PQN algorithm uses a variable reduction factor
. However, the steady-state MSE of the PQN al-
, not on its
gorithm depends on the steady-state value of
transient value. As reported in [4], the steady-state mean-square
error given in Eqs. (40) and (46) of [4] for stationary and non-
stationary environments, respectively, is independent of the way
the inverse of the autocorrelation matrix is estimated and hence
it will not be different for other Newton-type algorithms as long
as (1) they use a weight-vector update equation of the type given
in [4], (2) they use an approximate Newton direction, and (3) the
assumptions made in [4] hold true. This conclusion is conﬁrmed
in [p. 931, [3]] where the expression for the steady-state MSE
in the KQN algorithm is shown to be identical with that of the
in Eq. (46)
LMSN algorithms. As can be veriﬁed, using
of [4], (22) of [3] can be obtained. Since the PQN algorithm fol-
lows an approximate Newton direction, employs a similar step
size, and uses the same update equations for the weight vector,
the formulas for the excess mean-square error are the same as
those in Eqs. (40) and (46) in [4]. For stationary environments,
the excess mean-square error for the PQN algorithm is given by

(31)

where
value of
this becomes
have

is the minimum mean-square error and
as

is the
. For the case of the KQN algorithm,
at steady state, we

. As

. In addition, we have

for any prespeciﬁed .

If the weights of the unknown plant change according to the

update formula

the excess mean-square error is given by

(32)

(33)

and, therefore,

is the vari-
. As can be seen, the second term in

is the variance of the input signal and

where
ance of the elements of
the parenthesis is inversely proportional to
should not be allowed to become too small.
The optimal value of the reduction factor,

, that minimizes
the excess mean-square error is given in Eq. (47) of [4]. As can
be easily veriﬁed, it is difﬁcult to determine the optimal reduc-
in Eq. (47) of [4] is unknown a priori.
tion factor as parameter
involves certain assumptions, there is
Since the derivation of
no guarantee that the minimum excess mean-square error will be
[4]. To circumvent these problems, a small pos-
obtained with
to be used in the weight up-
can be added to
itive constant
.
date formula for nonstationary environments so that
is difﬁcult to obtain if the input signal
is colored. However, for a white Gaussian input signal an ap-
proximate range for

A numerical value for

can be obtained as

(34)

is the variance of the noise signal added to the desired
where
is the complementary Gaussian cumulative dis-
signal and
tribution function [23]. The maximum reduction in the number
of weight updates can be obtained by using the Chebyshev in-
equality

where
integer multiple of

is the minimum value of

, i.e.,

(35)

and
with

is chosen as an

.

The number of updates, convergence speed, and steady-state
. From (35), a larger
MSE depend on the value of parameter
value of would reduce the number of updates. From (34), on
the other hand, we note that a larger would reduce , and ac-
cording to (31) a reduced steady-state misalignment would be
achieved in stationary environments. However, in such a case the
convergence speed of the algorithm would be compromised. A
, on the other hand, would increase the number
smaller value of
of updates and
and, therefore, an increased steady-state mis-
alignment would result in the case of stationary environments.
The convergence speed of the algorithm in this case would be
on the
improved. Similar conclusions about the inﬂuence of
number of updates, convergence speed, and steady-state MSE
were also drawn in [23]. However, such conclusions cannot be
deduced for nonstationary environments as the relation in (33) is
nonlinear. In nonstationary environments, a reduced error bound
would improve the tracking capability of the algorithm because
the algorithm would continue to carry out updates after conver-
gence. Experimentation has shown that good performance can
be achieved in stationary and nonstationary environments by
in the range of 1 to 5 in the ﬁrst case
choosing integer
and 1 to 3 in the second case.

in

As far as stability is concerned, the proposed algorithm is in-
herently stable as the a posteriori error is forced to be equal to
, whenever an update is made. A
the prespeciﬁed error bound,
rough approximation of the variance of the measurement noise
would be enough to choose the error bound. In certain engi-
neering applications, the measurement noise has an upper bound

2114

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

Fig. 3. Learning curves, stationary environment (a) SNR = 20 dB (b) SNR = 60 dB (c) SNR = 100 dB (d) SNR = 140 dB.

COMPARISON OF PROPOSED WITH KNOWN QN ALGORITHM.

TABLE II

[24] and in such applications the PQN algorithm can be readily
applied.

IV. SIMULATION RESULTS

, where

In order to evaluate the performance of the proposed QN al-
gorithm several experiments were carried out as detailed below.
In the ﬁrst experiment, an adaptive ﬁlter was used to iden-
tify a 36th-order lowpass FIR ﬁlter with a cutoff frequency of
is the sampling frequency, using normalized
coefﬁcients to assure that the total power is unity. The input
signal was a sinusoid of amplitude 1 and frequency of
and was contaminated by a Gaussian noise signal of zero mean
and variance 0.1. The contaminating Gaussian noise signal was
colored using a 10th-order lowpass FIR ﬁlter with a cutoff fre-
. A sinusoidal signal was chosen because it
quency of
causes the input signal to be severely ill-conditioned. With such
a system identiﬁcation problem, the convergence behavior of
Newton-type algorithms can be better understood as their con-
vergence speed would be low enough to facilitate comparison.
The measurement noise added to the desired signal was white
,
Gaussian with zero mean and variance of

where

to achieve signal-to-noise ratios (SNRs) of 20, 60,
and
100, and 140 dB, respectively. The prespeciﬁed error bound was
is the variance of the measure-
chosen as
ment noise. The initial weight vector was assumed to be the zero
vector and the estimate of the inverse of the autocorrelation ma-
trix was assumed to be the identity matrix in all experiments in
the PQN as well as the KQN algorithms. The tolerance factor
used in the ﬁxed-point implementations was
[2]. The
learning curves obtained for different SNRs from 1000 indepen-
dent trials by using the PQN and the KQN algorithms in a sta-
tionary environment are illustrated Figs. 3(a)–3(d). The number
of iterations required for convergence, the steady-state misalign-
ment, the number of weight updates required by the PQN and
KQN algorithms in the above experiment in 3000 iterations, and
the reductions in the number of updates achieved are given in
Table II.

The second experiment was identical to the ﬁrst experiment
except that a nonstationarity was introduced in the ﬁlter taps
according to the ﬁrst-order Markov model

(36)

BHOTTO AND ANTONIOU: IMPROVED QUASI-NEWTON ADAPTIVE-FILTERING ALGORITHM

2115

Fig. 4. Learning curves, nonstationary environment (a) SNR = 20 dB (b) SNR = 60 dB. (c) SNR = 100 dB (d) SNR = 140 dB.

COMPARISON OF PROPOSED WITH KNOWN QN ALGORITHM.

TABLE III

, and

where the entries of were the samples of a white Gaussian
noise sequence with zero mean and variance equal
to
. The prespeciﬁed error bound
. With
in nonstationary environments was chosen as
a smaller error bound, the number of adaptations is increased
as given in (36)
and, therefore, the tracking of the changes in
improves after reaching steady state. The learning curves for
different SNRs obtained from 1000 independent trials by using
the PQN and KQN algorithms in a nonstationary environment
are illustrated Figs. 4(a)–4(d). The number of iterations to con-
verge, the steady-state misalignment, and the number of weight
updates required by the PQN and KQN algorithms in 3000
iterations and the reductions achieved are given in Table III.
As can be seen in Figs. 3 and 4 and Tables II and III, the PQN
algorithm yields reduced misalignment while requiring fewer
iterations to converge than the KQN algorithm for stationary
and nonstationary environments. As in the KQN algorithm,
the learning curves at steady-state in the PQN algorithm are
not noisy. The improvement in the steady-state misalignment
becomes more prominent for high SNRs in the PQN algorithm
because in the KQN algorithm adaptation of the inverse of

the autocorrelation matrix stops prior to reaching steady state.
Tables II and III also show that the required numbers of weight
updates in the PQN algorithm are only a fraction of those
required by the KQN algorithm.

for the error bound

In the third and fourth experiments, we veriﬁed the formulas
in (31) and (33) for the excess MSE for different system or-
ders and different values of the error bound. The input signal
was white Gaussian noise with zero mean and variance 0.1.
for
The limiting values of
, were obtained using (34). The results presented
in Table IV are the outcome of an ensemble of 100 runs where
de-
note the variance of the measurement noise and system order,
respectively. As can be seen in Table IV, the excess MSE ob-
tained form simulation lies within the range of the excess MSE
in (31) for stationary environ-
obtained by using
ments as expected. In addition, the excess MSE reduces as the
error bound is increased as discussed in the Section III.

are the limiting values of

and

and

and

and

The fourth experiment was the same as the third except that a
nonstationarity was introduced as in the second experiment. The
is the variance of
results obtained are given in Table V where

2116

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

EXCESS MSE IN DB IN PROPOSED QN ALGORITHM.

TABLE IV

EXCESS MSE IN DB IN PROPOSED QN ALGORITHM.

TABLE V

the Gaussian noise added to the weight vector. In nonstationary
environments, the excess MSE obtained by simulation may not
be within the range of the excess MSE obtained by using (33)
since this formula remains nonlinear at steady state. However,
the simulation results given in Table V are very close to the
theoretical results.

The last and ﬁnal experiment was carried out to demonstrate
the robustness of the proposed algorithm in the case of a ﬁxed-
point implementation. The system to be identiﬁed was the same
as that used in the ﬁrst experiment. Here the input signal was
a zero-mean white Gaussian noise with a variance of unity and

. The error bound was chosen as

was colored using an 10th-order lowpass FIR ﬁlter with a cutoff
frequency of
where
is the variance of the measurement noise. Fixed-point
arithmetic was assumed using a word length of 20 bits with
no scaling or rescuing procedures and overﬂow was handled
using saturation arithmetic. The error signal, error bound, and
the desired signal were quantized and the learning curves were
not smoothed. The learning curves obtained by using the PQN
,
and KQN algorithms in 100 trials with
and SNRs of 40 and 80 dB are illustrated in Fig. 5. These re-
sults are consistent with the results obtained with ﬂoating-point

and

BHOTTO AND ANTONIOU: IMPROVED QUASI-NEWTON ADAPTIVE-FILTERING ALGORITHM

2117

[5] N. Kalouptsidis and S. Theodoridis, Efﬁcient System Identiﬁcaiton and
Signal Processing Algorithms. Englewood cliffs, NJ: Prentice-Hall,
1993.

[6] J. M. Ciofﬁ, “The fast adaptive rotor’s RLS algorithm,” IEEE Trans.
Acoust., Speech, Signal Process., vol. 38, no. 4, pp. 631–651, Apr.
1990.

[7] I. K. Proudler, J. G. McWhirter, and T. J. Shepherd, “Fast QRD-based
algorithms for least squares linear prediction,” in Proc. IMA Conf.
Math. Signal Process., Warwick, England, Dec. 1998.

[8] P. A. Regalia and M. G. Bellanger, “On the duality between fast QR
methods and lattice methods in least squares adaptive ﬁltering,” IEEE
Trans. Signal Process., vol. 39, no. 4, pp. 879–892, Apr. 1991.

[9] D. Slock, “Reconsiling fast RLS lattice and QR algorithms,” in
IEEE Intern. Conf. Acoust., Speech, Signal Process., Apr. 1990, pp.
1591–1594.

[10] B. Yang and J. F. Bohme, “Rotation-based RLS algorithms: Uniﬁed
derivations, numerical properties and parallel implementation,” IEEE
Trans. Signal Process., vol. 40, no. 5, pp. 1151–1167, May 1992.

[11] S. T. Alexander and A. Ghirnikar, “A method for recursive least squares
ﬁltering based upon an inverse QR decomposition,” IEEE Trans. Signal
Process., vol. 41, no. 1, pp. 20–30, Jan. 1993.

[12] A. A. Rontogiannis and S. Theodoridis, “New fast QR least squares
adaptive algorithms,” in IEEE Intern. Conf. on Acoust., Speech, and
Signal Process., May 1995, pp. 1412–1415.

[13] L. Jung, M. Morf, and D. Falconer, “Fast calculations of gain matrices
for recursive estimation schemes,” Intern. Journal Contr., vol. 27, pp.
1–19, Jan. 1978.

[14] G. Carayannis, D. Manolakis, and N. Kalouptsidis, “A fast sequen-
tial algorithm for least-squares ﬁltering and prediction,” IEEE Trans.
Acoust., Speech, Signal Process., vol. 31, no. 6, pp. 1394–1402, Dec.
1983.

[15] J. Coifﬁ and T. Kailath, “Fast recursive LS tansversal ﬁlters for adaptive
processing,” IEEE Trans. Acoust., Speech, Signal Process., vol. 32, no.
4, pp. 304–337, Apr. 1984.

[16] G. Carayannis, D. Manolakis, and N. Kalouptsidis, “A uniﬁed view
of parametric processing algorithms for prewindowed signals,” Signal
Process., vol. 10, pp. 335–368, 1986.

[17] D. F. Marshall and W. K. Jenkins, “A fast quasi-Newton adaptive
ﬁltering algorithm,” IEEE Trans. Signal Process., vol. 7, no. 40, pp.
1652–1662, Jul. 1992.

[18] A. P. Liavas and P. A. Regalia, “On the numerical stability and ac-
curacy of the conventional recursive least squares algorithm,” IEEE
Trans. Signal Process., vol. 47, no. 1, pp. 88–96, Jan. 1999.

[19] Y. Zou and S. C. Chan, “A robust quasi-newtion adaptive ﬁltering al-
gorithm for impulse noise suppression,” in IEEE Intern. Symp. Circuits
Syst., May 2001, pp. 677–680.

[20] A. Antoniou and W. S. Lu, Pracitcal Optimization. New York, USA:

Springer, 2007.

[21] R. Fletcher, Practical Methods of Optimization. New York: Wiley,

1987.

[22] P. S. R. Diniz, Adaptive Filtering: Algorithms and Practical Implemen-

tation, 3rd ed. NewYork: Springer, 2008.

[23] P. S. R. Diniz and S. Werner, “Set-membership binormalized data-
reusing LMS algorithms,” IEEE Trans. Signal Process., vol. 51, no.
1, pp. 124–134, 2003.

[24] B. Peterson and K. Narendra, “Bounded error adaptive control,” IEEE

Trans. Automatic Contol, vol. 27, no. 6, pp. 1161–1168, Dec. 1982.

Md Zulﬁquar Ali Bhotto (SM’09) obtained the
B.Sc. in electrical and electronic engineering from
the Department of Electrical and Electronic Engi-
neering of Rajshahi University of Engineering and
Technology, Bangladesh in 2002. He worked as
a lecturer and assistant professor in the Rajshahi
University of Engineering and Technology from
2003 to 2007. Currently, he is pursuing a Ph.D.
degree in the Department of Electrical and Computer
Engineering at the University of Victoria, Victoria,
Canada. He was awarded a University of Victoria

fellowship for 2007–2008.

Fig. 5. MSE for a ﬁxed-point implementation.

arithmetic. Furthermore, Fig. 5 shows that when implemented
in ﬁxed-point arithmetic the PQN algorithm is as robust as the
KQN algorithm.

V. CONCLUSIONS

An improved QN algorithm for adaptive ﬁltering that incor-
porates data selective adaptation for the weight vector and the
inverse of the autocorrelation matrix has been proposed. The
proposed algorithm was developed on the basis of the frame-
work of classical QN optimization algorithms and in this way
an improved estimator of the autocorrelation matrix was de-
duced. Analysis has shown that the PQN algorithm should per-
form better than the KQN algorithm in terms of convergence
speed and steady-state misalignment. This expectation has been
substantiated by simulations which have shown that the pro-
posed algorithm requires fewer iterations to converge, fewer
weight updates, and yields reduced steady-state misalignment
when compared with the KQN algorithm in stationary as well as
nonstationary environments. In addition, analytical results ob-
tained by using closed-form formulas for the steady-state mis-
alignment agree well with those obtained by simulations. The
PQN algorithm was also found to be as robust as the KQN al-
gorithm in the case of ﬁxed-point implementations.

VI. ACKNOWLEDGMENT

The authors are grateful to the Natural Sciences and Engi-
neering Research Council of Canada for supporting this work.

REFERENCES

[1] A. H. Sayed, Fundamentals of Adaptive Filtering. New Jersey: Wiley,

2003.

[2] M. L. R. de Campos and A. Antoniou, “A robust quasi-Newton adaptive
ﬁltering algorithm,” in IEEE Int. Symp. Circuits and Syst., Nov. 1994,
pp. 229–232.

[3] M. L. R. de Campos and A. Antoniou, “A new quasi-Newton adaptive
ﬁltering algorithm,” IEEE Trans. Circuits and Syst., vol. 44, no. 11, pp.
924–934, Nov. 1997.

[4] P. S. R. Diniz, M. L. R. de Campos, and A. Antoniou, “Analysis of
LMS-Newton adaptive adaptive ﬁltering algorithms with variable con-
vergence factor,” IEEE Trans. Signal Process., vol. 43, pp. 617–627,
Mar. 1995.

2118

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 8, AUGUST 2010

Andreas Antoniou
(M’69-SM’79-F’82-LF’04)
received the B.Sc.(Eng.) and Ph.D. degrees in elec-
trical engineering from the University of London in
1963 and 1966, respectively, and is a Fellow of the
IET and IEEE. He taught at Concordia University
from 1970 to 1983, was the founding Chair of the
Department of Electrical and Computer Engineering,
University of Victoria, B.C., Canada, from 1983 to
1990, and is now Professor Emeritus. His teaching
and research interests are in the area of digital signal
processing. He is the author of Digital Signal Pro-
cessing: Signals, Systems, and Filters, McGraw-Hill, 2005 and the co-author
with Wu-Sheng Lu of Practical Optimization: Algorithms and Engineering
Applications, Springer, 2007.

Dr. Antoniou served as Associate/Chief Editor for IEEE Transactions on
Circuits and Systems (CAS) from 1983 to 1987, as a Distinguished Lecturer
of the IEEE Signal Processing and the Circuits and Systems Societies during
2003–2004 and 2006–2007, respectively, and as General Chair of the 2004
International Symposium on Circuits and Systems.

He was awarded the CAS Golden Jubilee Medal by the IEEE Circuits and
Systems Society, the B.C. Science Council Chairman’s Award for Career
Achievement for 2000, the Doctor Honoris Causa degree by the National
Technical University, Athens, Greece, in 2002, the IEEE Circuits and Systems
Society Technical Achievement Award for 2005, the 2008 IEEE Canada
Outstanding Engineering Educator Silver Medal, and the IEEE Circuits and
Systems Society Education Award for 2009.

