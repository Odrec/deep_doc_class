Modeling Single-Neuron Dynamics and
Computations: A Balance of Detail and Abstraction
Andreas V. M. Herz,
Science
DOI: 10.1126/science.1127240 

, 80 (2006);

 et al.
 

 314

This copy is for your personal, non-commercial use only.

If you wish to distribute this article to others
colleagues, clients, or customers by 

clicking here

. 

, you can order high-quality copies for your

Permission to republish or repurpose articles or portions of articles
following the guidelines 

here

. 

 can be obtained by

The following resources related to this article are available online at www.sciencemag.org
(this information is current as of March 16, 2010 ):

 

Updated information and services,
version of this article at: 
http://www.sciencemag.org/cgi/content/full/314/5796/80
 

 including high-resolution figures, can be found in the online

A list of selected additional articles on the Science Web sites 
found at: 
http://www.sciencemag.org/cgi/content/full/314/5796/80#related-content
 
This article 
, 27 of which can be accessed for free: 
http://www.sciencemag.org/cgi/content/full/314/5796/80#otherarticles
 

cites 79 articles

related to this article

 can be

 

0
1
0
2

 
,

6
1

 

h
c
r
a
M
n
o

 

 

This article has been 

cited by

 23 article(s) on the ISI Web of Science. 

This article has been 
http://www.sciencemag.org/cgi/content/full/314/5796/80#otherarticles
 

 8 articles hosted by HighWire Press; see: 

cited by

This article appears in the following 
Neuroscience 
http://www.sciencemag.org/cgi/collection/neuroscience
 

subject collections
: 

i

.

g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 

 

d
e
d
a
o
n
w
o
D

l

 (print ISSN 0036-8075; online ISSN 1095-9203) is published weekly, except the last week in December, by the
Copyright

Science
American Association for the Advancement of Science, 1200 New York Avenue NW, Washington, DC 20005. 
2006 by the American Association for the Advancement of Science; all rights reserved. The title 
 is a
registered trademark of AAAS. 

Science

Modeling the Mind

REVIEW

Modeling Single-Neuron Dynamics
and Computations: A Balance of
Detail and Abstraction

Andreas V. M. Herz,1* Tim Gollisch,2 Christian K. Machens,3 Dieter Jaeger4

The fundamental building block of every nervous system is the single neuron. Understanding how
these exquisitely structured elements operate is an integral part of the quest to solve the mysteries
of the brain. Quantitative mathematical models have proved to be an indispensable tool in
pursuing this goal. We review recent advances and examine how single-cell models on five levels of
complexity, from black-box approaches to detailed compartmental simulations, address key
questions about neural dynamics and signal processing.

B

[
integrate-and-fire

A hundred years ago, Lapicque (1) pro-

posed that action potentials are gen-
erated when the integrated sensory or
synaptic inputs to a neuron reach a threshold value.
This
model remains one of the
most influential concepts in neurobiology because
it provides a simple mechanistic explanation for
basic neural operations, such as the encoding of
stimulus amplitude in spike frequency. However,
advances in experimental technique have shown
that the integrate-and-fire model is far from
accurate in describing real neurons. Their mor-
phology, composition of ionic conductances, and
distribution of synaptic inputs generate a plethora
of dynamical phenomena and support various fun-
damental computations (Table 1 and Table 2).

Understanding the dynamics and computa-
tions of single neurons and their role within
larger neural networks is therefore at the core of
neuroscience: How do single-cell properties
contribute to information processing and, ulti-
mately, behavior? Quantitative models address
these questions, summarize and organize the
rapidly growing amount and sophistication of
experimental data, and make testable predictions.
As single-cell models and experiments become
more closely interwoven, the development of data
analysis tools for efficient parameter estimation
and assessment of model performance constitutes
a central element of computational studies.

All these tasks require a delicate balance
between incorporating sufficient details to ac-
count for complex single-cell dynamics and
reducing this complexity to the essential charac-
teristics to make a model tractable. The appro-
priate level of description depends on the
particular goal of the model. Indeed, finding the

1Bernstein Center for Computational Neuroscience Berlin and
Humboldt-Universita¨t zu Berlin, Berlin 10099, Germany.
2Department of Molecular and Cellular Biology, Harvard
University, Cambridge, MA 02138, USA. 3Cold Spring Harbor
Laboratory, Cold Spring Harbor, NY 11724, USA. 4Depart-
ment of Biology, Emory University, Atlanta, GA 30322, USA.

*To whom correspondence should be addressed. E-mail:
a.herz@biologie.hu-berlin.de

best abstraction level is often the key to success.
We highlight these aspects for five main levels
(Fig. 1) of single-cell modeling.

Level I: Detailed Compartmental Models
Morphologically realistic models are based on
anatomical reconstructions and focus on how the
spatial structure of a neuron contributes to its
dynamics and function. These models extend the
cable theory of Rall, who showed mathematically
that dendritic voltage attenuation spreads asym-
metrically (2). This phenomenon allows dendrites
to compute the direction of synaptic activation pat-
terns, and thus provides a mechanism for motion
detection (3). When voltage-dependent conduct-
ances are taken into account, numerical integration
over the spatially discretized dendrite—the ‘‘com-
partmental model’’ (3)—is needed to solve the
resulting high-dimensional system of equations.
For complex dendritic trees, more than 1000
compartments are required to capture the cell’s
specific electrotonic structure (e.g., to simulate
spike backpropagation in pyramidal neurons) (4).
Such detailed models also generate testable
mechanistic hypotheses. For instance, simula-
tions of Purkinje cells predicted that a net
inhibitory synaptic current underlies specific
spike patterns in vivo (5), in accordance with
later experimental findings (6). In turn, even
established models such as the thalamocortical
neuron (7) are constantly improved by adding
new biophysical details such as dendritic calci-
um currents responsible for fast oscillations (8).
A large body of morphologically realistic mod-
els demonstrates how spatial aspects of synaptic
integration in dendrites support specific computa-
tions (Table 1 and Table 2), as discussed in
various reviews (9, 10). In pyramidal cells, for ex-
ample, distal inputs are amplified via dendritic
spikes or plateau potentials, supporting local
coincidence detection and gain modulation. Den-
dritic inward currents play a major role in the
control of spiking (6) or the modulation of re-
sponses to synchronous inputs (11). Such inter-
actions among synaptic inputs, voltage-gated

 

0
1
0
2

 
,

6
1

 

.

 

 

h
c
r
a
M
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 

i

 

d
e
d
a
o
n
w
o
D

l

Fig. 1. Examples for five levels of single-cell
modeling. Level I: Detailed compartmental model
of a Purkinje cell. The dendritic tree is segmented
into electrically coupled Hodgkin-Huxley–type com-
partments (level III). Level II: Two-compartment
model as in (23). The dendrite receives synaptic
inputs and is coupled to the soma where the
neuron’s response is generated. Level III: Hodgkin-
Huxley model, the prototype of single-compartment
models. The cell’s inside and outside are separated
by a capacitance Cm and ionic conductances in
series with batteries describing ionic reversal
potentials. Sodium and potassium conductances
(gNa, gK) depend on voltage; the leak gleak is fixed.
Level IV: Linear-nonlinear cascade. Stimuli S(t) are
convolved with a filter and then fed through a
nonlinearity to generate responses R(t), typically
time-dependent firing rates. Level V: Black-box
model. Neglecting biophysical mechanisms, condi-
tional probabilities p(R|S) describe responses R for
given stimuli S.

80

6 OCTOBER 2006 VOL 314 SCIENCE www.sciencemag.org

SPECIALSECTION

conductances, and spiking output can be specifi-
cally affected by dendritic branching structures
(12); axonal geometries, on the other hand,
influence activity-dependent branch point failures

and may thus implement filter and routing opera-
tions on the neuron’s output side (13).

Finally, detailed spatial representations help
predict the effects of extracellular electrical

stimulations. This is of great interest for deep-
brain stimulation used in the treatment of
Parkinson’s disease (14) and underscores the
need for morphologically realistic models.

Table 1. Information processing in single neurons: Basic computations that follow from generic
neuronal properties.

Computation

Biophysical mechanism

Model level

Addition or subtraction

Dendritic summation of excitatory and/or

inhibitory inputs (3)

Subtraction

Shunting inhibition plus integrate-and-fire

Multiplication or division

High-pass filter
Low-pass filter
Toggle switch

mechanism (76)

Synaptic interaction (67,
)77
Gain modulation via synaptic
background noise (35, 36)

Firing rate adaptation (32)
Passive membrane properties (28)
Bistable spike generation (30)

I, II

I, II

I, II
I, II

III

I, II, III

III

Level II: Reduced Compartmental Models
Although detailed compartmental models can
approximate the dynamics of single neurons quite
well, they suffer from several drawbacks. Their
high dimensionality and intricate structure rule out
any mathematical understanding of their emergent
properties. Detailed models are also computation-
ally expensive and are thus not well suited for
large-scale network simulations. Reduced models
with only one or few dendritic compartments
overcome these problems and are often sufficient
to understand somatodendritic interactions that
govern spiking (15) or bursting (16).

A well-matched task for such models is to
relate behaviorally relevant computations on

 

0
1
0
2

 
,

6
1

 

 

.

i

 

h
c
r
a
M
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 

 

d
e
d
a
o
n
w
o
D

l

Table 2. Information processing in single neurons: Task-specific computations of direct behavioral relevance.

Biological goal

Computation

Collision avoidance

Multiplication:

object size x times
angular velocity y

Sound localization

Logical AND: comparison

of interaural
time difference

Motion detection

Logical AND or

Motion anticipation

Intensity-invariant

recognition of
analog patterns

AND-NOT: comparison
of spatially adjacent but
temporally shifted local
light intensities

Linear filtering with
negative feedback

Separation of

pattern identity and
pattern intensity;
subsequent comparison
with stored template

Biophysical
mechanism

xy

0

exp(log x + log y) via
input nonlinearity (log),
dendritic summation (+), and
output nonlinearity (exp)

Coincidence detection of two
spikes, lagged by different
axon delays (17, 18)

Coincidence detection of one
lagged (axonal delay) and
one nonlagged spike (59)

Nonlinear dendritic

processing (3, 78)

Adaptation of neuronal gain

Model level

III

II

IV

I, II

IV

Transformation: local stimulus

III, IV

intensity mapped to spike time
using subthreshold membrane
potential oscillations; readout:
coincidence detection (82)

Short-term memory

Temporal integration or storage

Dendritic Ca waves (20)

Transitions between two

Ca-conductance states (21)

Time interval prediction

Temporal integration or storage

Calcium dynamics with

positive feedback (84)

Redundancy reduction

Subtraction: local

Dendritic summation (3)

signal minus
background signal

II

II

III

IV

Experimental

systems

Lobula giant

movement detector
in locusts (52)

Binaural neurons
in the auditory
brainstem (17)

Peripheral neurons in

the fly visual system
[see, e.g., (59, 60)]
Retinal amacrine and

ganglion cells (79, 80)

Salamander and rabbit

retinal ganglion cells (81)

Insect and vertebrate
olfactory neurons,
in particular in
antennal lobe and
olfactory bulb,
respectively (83)

Layer V neurons in

entorhinal cortex (19)

Layer V neurons in

entorhinal cortex (21)

Climbing activity in

prefrontal neurons (85)

Center-surround receptive

fields in the visual
system (57)

Efficient coding in

Modification of

variable environment

tuning curve to
track time-varying
stimulus ensemble

Adaptation of single-cell

input-output function (23, 58)

Consequence (60) of Reichardt
motion detector circuit (59)

II, IV, V

Motion-sensitive H1

IV, V

neuron in the
fly visual system (58, 60)

www.sciencemag.org SCIENCE VOL 314

6 OCTOBER 2006

81

Modeling the Mind

various time scales to salient features of neural
structure and dynamics. For example, the detec-
tion of binaural time differences within Jeffress’
time-delay framework (17) has been explained in
a three-compartment model of bipolar cells by
local nonlinear input interactions and the fact
that each of the two dendrites provides a sink for
inputs received by the other dendrite (18). Com-
putations involving short-term memory may rely in
part on the existence of multiple stable firing rates
in single neurons. Reduced compartmental models
suggest that calcium currents are essential for this
phenomenon (19),
through dendritic calcium
wavefronts (20) or transitions between different
conductance states (21). On longer time scales,
neurons self-adjust their activity patterns, both
during development and after external perturba-
tions (22). Simulations with a two-compartment
model show that such homeostatic plasticity can
follow from cellular ‘‘learning’’ rules that recal-
ibrate dendritic channel densities to yield optimal
spike encoding of synaptic inputs (23).

For large-scale network studies, reduced com-
partmental models offer a good compromise be-
tween realism and computational efficiency. For

example, a simulation involving several classes of
multicompartmental cortical and thalamic neurons
and a total of more than 3000 cells demonstrates
that gap junctions are instrumental for cortical
gamma oscillations (24). A slightly less complex
network with two-compartment neurons repro-
duces slow-wave sleep oscillations (25). Clearly,
the challenge for all such studies is to find the
least complex neuron models with which the
observed phenomena can still be recreated (26).

Level III: Single-Compartment Models
Single-compartment models such as the classic
Hodgkin-Huxley model (27) neglect the neuron’s
spatial structure and focus entirely on how its
various ionic currents contribute to subthreshold
behavior and spike generation. These models
have led to a quantitative understanding of many
dynamical phenomena including phasic spiking,
bursting, and spike-frequency adaptation (28).

Systematic mathematical reductions of
Hodgkin-Huxley–type models and subsequent
bifurcation and phase-plane analysis (29, 30)
explain why, for example, some neurons resemble
integrate-and-fire elements or why the membrane

potential of others oscillates in response to current
injections enabling a ‘‘resonate-and-fire’’ behavior.
They also show which combination of dynamical
variables governs the threshold operation (31)
and how adaptation (32) and spike-generation
mechanisms (33) influence spike trains (Fig. 2).
Spike generation is not a deterministic process.
The stochastic dynamics of ion channels generate
voltage noise that limits the reliability and pre-
cision of spikes (34). Background synaptic noise
(35), on the other hand, can modulate the neural
gain without changing spike variability or mean
firing rates (36). But even without intrinsic noise,
the all-or-none characteristics of spike generation
amplify the input variability (37)—perhaps this is
the price of long-distance communication.

More than 50 years after Hodgkin and
Huxley analyzed the squid axon, simple neuron
models still offer surprises, as these findings
show. A recent study even indicates that the
standard Hodgkin-Huxley formalism does not
explain the sharp kink at the onset of cortical
spikes (38). Its mechanistic origin and functional
consequences require further investigation.

Fig. 2. Diversity of neural response patterns. As illustrated in the top row, neurons can respond with rather different spike-
train patterns to identical step currents. For time-varying inputs (middle row), the computational power of even simple
single-neuron models becomes apparent: A first current pulse might trigger a subthreshold oscillation. Only if a second
pulse arrives at the right phase of this oscillation is a spike triggered through resonance. An integrator, on the other hand,
is driven most effectively by quickly succeeding pulses. Finally, a bistable cell can realize a toggle-switch. These
phenomena [and many more; see (30)] are exhibited by the same point-neuron model: Its time evolution (bottom row,
left) is derived from Hodgkin-Huxley–type dynamics; involves the membrane potential v and a slower auxiliary variable
u; and generates the different responses for specific values (right) of the parameters c (reset of voltage v with peak p)
and a, b, and d (decay rate, sensitivity, and reset of the auxiliary variable u). Figure courtesy of E. Izhikevich.

82

6 OCTOBER 2006 VOL 314 SCIENCE www.sciencemag.org

 

0
1
0
2

 
,

6
1

 

 

.

i

 

h
c
r
a
M
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 

 

d
e
d
a
o
n
w
o
D

l

Level IV: Cascade Models
Whereas models incorporating
specific ionic currents or morpho-
logical details are needed to in-
vestigate the biophysics of single
neurons, modeling on a more con-
ceptual level allows one to directly
address their computations. To this
end, cascade models based on a
concatenation of mathematical
primitives, such as linear filters,
nonlinear transformations, and
random processes, present an
excellent framework for distill-
ing key processing steps from mea-
sured data.

Consider, for example, a mod-
el that first convolves its time-
varying input with a linear filter
and then applies a rectifying non-
linearity. In studies of sensory
systems, this simple structure is
often considered as the canonical
model for the receptive field of
a neuron and the transformation
of its internal activation state
into a firing rate. The appeal of
this linear-nonlinear (LN) cas-
cade stems from its conceptual
simplicity and the fact that, for
white-noise stimulation, it can
be easily fitted to experimen-
tal data by correlating response
and stimulus (39). Recent studies
even demonstrate that LN cas-
cades can be obtained under
far more naturalistic stimulation
(40, 41).

stimulation context, affect the transition between
different experimental conditions (53, 54). In
specific cases, however, LN models yield ac-
curate information-theoretical descriptions of
neuronal responses (55).

Level V: Black-Box Models
Last but not least, one may want to understand
and quantify the signal-processing capabilities of
a single neuron without considering its bio-
physical machinery. This approach may reveal
general principles that explain, for example,
where neurons place their operating points and
how they alter their responses when the input
statistics are modified.

Cascade models have a long tradition in the
investigation of the visual system. More recently,
they have been used to assess neuronal sensitivity
for different stimulus features and have helped to
elucidate the simultaneous adaptation to mean light
intensity and light contrast (42) and the generic
nature of adaptation in the retina (43). New
analysis tools have opened up the possibility of
using multiple parallel linear filters in an LN
cascade to investigate, for example, complex
cells in visual cortex (44) and thus improve on
classical energy-integration models (45).

Extending LN cascades allows one to capture
additional neural characteristics while retaining the
ability to fit these more complex models to ex-
perimental data. To reveal filter
mechanisms that are otherwise
hidden by spike-time jitter, one
may append a noise process to
the cascade (46, 47) or measure
spike probabilities instead of
spike times (48). For the latter
method,
temporal resolution is
limited only by the precision of
stimulus presentation so that
parameters of more elaborate
models (e.g., LNLN cascades)
can be obtained.

SPECIALSECTION

estimates of p(R|S) that are directly taken from the
measured data. Such models have, for example,
been used to estimate the information that the
spike train of a neuron conveys about its inputs
and have revealed that sensory neurons operate
highly efficiently, often close to their physical
limits (56). Indeed, Barlow’s ‘‘efficient coding
hypothesis’’ suggests that neurons optimize the in-
formation about frequently occurring stimuli (57).
Theoretical studies have shown how individual
neurons may shift their input-output curves to reach
that goal (23). Moreover, recordings of a motion-
sensitive neuron in the fly visual system reveal that
adaptation can modify a neuron’s input-output
time-
function to maximize information about
varying sensory stimuli (58). In
this case, however, it is possible
that the adaptive mechanism is
not implemented on the single-
cell
instead results
from the underlying multicel-
lular Reichardt motion detec-
tion circuitry (59, 60). Similar
ambiguities between single-cell
and network adaptation exist
in the auditory midbrain (61).
Evolutionary adaptations may
not be guided to optimize the in-
formation about all natural stimu-
li. In acoustic communication
systems, for example, neural re-
sponses are well matched to
particular behaviorally relevant
subensembles. Most likely, stimu-
li from those ensembles were
selected as communication sig-
nals because they lead to efficient
neural representations (62, 63).

level but

Challenges
‘‘A good theoretical model of a
complex system should be like a
good caricature: it should empha-
size those features which are most
important and should downplay
the inessential details. Now the
only snag with this advice is that
one does not really know which
are the inessential details until one has understood
the phenomena under study’’ (64).

This general dilemma, formulated by the
physicist Frenkel almost a century ago, applies in
particular to the single neuron. Which details of
ionic conductances and morphology are relevant
for particular aspects of its cell type–specific or
individual dynamics? How do these dynamics
contribute to the neuron’s information processing?
Identification of a fundamental computation per-
formed by the neuron (Table 1 and Table 2) may
help address these questions. Brain function,
however, relies on the interplay of hundreds to
billions of neurons that are arranged in specialized
modules on multiple anatomical hierarchies. Even

 

0
1
0
2

 
,

6
1

 

i

.

 

 

h
c
r
a
M
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 

 

d
e
d
a
o
n
w
o
D

l

The analog output of tradi-
tional cascade models describes
a firing rate. An important con-
ceptual extension is therefore
achieved by adding an explicit
spike generation stage. Using a
fixed firing threshold and feed-
back mimicking neural refrac-
toriness (49), this has led to a
successful model of spike timing
in early vision (50). Even when
augmented with an integrate-
and-fire mechanism and intrinsic
bursting, this model structure still
allows generic fits to measured
spike trains (51).

Fig. 3. Single-neuron computation. The neuron in the center (86) can be ap-
proximated by an NLN cascade (left) for stationary inputs (67), or, more generally, by
a compartmental model (right). The cascade (level IV) is equivalent to a two-layer
feedforward network and shows that under a firing-rate assumption, a single neuron
may perform the function of an entire artificial neural net. Electrical couplings within
compartmental models (levels I and II) are bidirectional. The right model therefore
corresponds to a feedback network and can exhibit persistent activity, hysteresis,
periodic oscillations, and even chaos. These phenomena are impossible in
feedforward systems and may support complex computations in the time domain.
The relevance of either model depends on the statistics of synaptic inputs (i.e., on
the neural code of the investigated brain area).

Cascade models can also
directly translate into specific
computations: Experiments in-
dicate that in locusts, an identi-
fied neuron multiplies the visual size x and
angular velocity y of an object while tracking
its approach (52). The nearly exponential shape
of this neuron’s output curve suggests that
logarithmic transforms of x and y are summed
on the dendrite and then passed through the
output nonlinearity, implementing the multipli-
cative operation as an NLN cascade via the
identity exp(log x + log y)

xy.

0

Despite their success, simple model structures
have their limitations—especially when applied
to neurons far downstream from the sensory
periphery and when aimed at generalizing over
different stimulus types—because additional
nonlinear dynamics, negligible within a specific

For such questions about neural efficiency
and adaptability, a neuron is best regarded as a
black box that receives a set of time-dependent
inputs—sensory stimuli or spike trains from
other neurons—and responds with an output
spike train. To account for cell-intrinsic noise, it
is necessary to characterize the input-output rela-
tion by a probability distribution, p(R|S), which
measures the probability that response R occurs
when stimulus S is presented.

Although models on levels I to IV make spe-
cific assumptions about neural processes and hence
about the functional form of p(R|S), such assump-
tions can be overly restrictive at level V. Here, it
is often advantageous to work with nonparametric

www.sciencemag.org SCIENCE VOL 314

6 OCTOBER 2006

83

Modeling the Mind

today, it remains unclear which level of single-cell
modeling is appropriate to understand the dynam-
ics and computations carried out by such large
systems. However, only by understanding how
single cells operate as part of a network (35) can
we assess their coding and thus the level of detail
required for modeling. For example, most
network models use point-neuron models (65),
whereas several aspects of brain function require
multicompartmental models (24, 25).

It has thus become increasingly clear that a
thorough understanding of single-neuron func-
tion can be obtained only by relating different
levels of abstraction. Trying to incorporate every
biological detail of the investigated neuron is
likely to obscure the focus on the essential dy-
namics, whereas limiting investigations to highly
abstract processing schemes casts doubt on the
biological relevance of specific findings. Help
may come from analyzing the transition between
different modeling levels. Interesting connections
have been drawn, for example, by transforming a
Hodgkin-Huxley–type model (level III) into a
phenomenological firing rate description (66)
or a cascade on level IV (31). And the integra-
tive properties of dendritic trees as evolved as
those of pyramidal cells can be captured by a two-
layer feedforward network (i.e., an NLN cascade)
(level IV), at least for stationary stimuli (67). For
nonstationary stimuli, however, the cascade fails
(Fig. 3). This underscores the need to alternate
between different levels of single-neuron models
in close connection with considerations about the
neural codes of larger cell populations.

Deriving model parameters from experimen-
tal data brings about its own collection of prob-
lems: How should we deal with the cell-to-cell
variability of parameter values? The common
resort, population averaging, can be misleading
because the dynamical behavior of single-cell
models is, in general, not a monotone function of
their parameters; the mean behavior within a class
of models may strongly differ from that of a model
with mean parameter values (68), and nearly
identical dynamical characteristics may be imple-
mented by rather different parameter combina-
tions (69). With increasing model complexity, the
number of parameters to be estimated increases to
such an extent that they must be taken from
different cells or even different preparations, further
lowering the model’s trustworthiness. Furthermore,
models are often calibrated using in vitro data, yet
they are designed to capture the neural dynamics
and computations of behaving animals.

biological neuron are already included. Wrong
models may therefore be falsely ‘‘verified,’’ and
long-term progress may require many iterations
of the model-experiment loop until an incorrect
assumption is eventually falsified.

There is good news, too: The rapid develop-
ment of experimental tools to study single neurons
in vivo (70) will generate data urgently needed to
advance quantitative models. With powerful
computers tightly integrated in modern laborato-
ries, advanced on-line techniques such as the
‘‘dynamic clamp’’ (71) will be used routinely in
the future. In this technique, voltage-gated con-
ductances that cannot be selectively blocked by
pharmacological agents are counterbalanced by
currents that are artificially generated using the
neuron’s present state. This approach has clarified,
for example, the influence of persistent sodium
currents on spike generation (72). To mimic in
vivo input patterns during in vitro experiments,
synaptic conductances can be inserted with a
dynamic clamp (6, 36, 73). Adaptive stimulations
with real-time data analysis can also be used to
optimize recording times, allowing one to extend
traditional concepts such as ‘‘best stimulus’’ to
the information-theoretic level (62).

These developments show that the divide
between experiment and theory is disappearing.
There is also a change in attitude reflected by var-
ious international initiatives (74, 75): More and
more experimentalists are willing to share their raw
data with modelers. Many modelers, in turn, make
their computer codes available. Both move-
ments will play a key role in solving the many
open questions of neural dynamics and information
processing—from single cells to the entire brain.

References and Notes

1. L. Lapicque, J. Physiol. Pathol. Gen. 9, 620 (1907).
2. W. Rall, Exp. Neurol. 1, 491 (1959).
3. W. Rall, in Neural Theory and Modeling, R. F. Reiss, Ed.
(Stanford Univ. Press, Stanford, CA, 1964), pp. 73–97.
4. N. L. Golding, W. L. Kath, N. Spruston, J. Neurophysiol.

22. G. G. Turrigiano, S. B. Nelson, Nat. Rev. Neurosci. 5, 97

(2004).

23. M. Stemmler, C. Koch, Nat. Neurosci. 2, 521 (1999).
24. R. D. Traub et al., J. Neurophysiol. 93, 2194 (2005).
25. M. Bazhenov, I. Timofeev, M. Steriade, T. J. Sejnowski,

J. Neurosci. 22, 8691 (2002).

26. P. F. Pinsky, J. Rinzel, J. Comp. Neurosci. 1, 39 (1994).
27. A. L. Hodgkin, A. F. Huxley, J. Physiol. 117, 500 (1952).
28. C. Koch, Biophysics of Computation: Information

29.

Processing in Single Neurons (Oxford Univ. Press, New
York, 1998).
J. Rinzel, G. B. Ermentrout, in Methods in Neuronal
Modeling: From Synapses to Networks, C. Koch,
I. Segev, Eds. (MIT Press, Cambridge, MA, 1989),
pp. 135–169.

30. E. M. Izhikevich, IEEE Trans. Neur. Networks 15, 1063

(2004).

31. B. Aguera y Arcas, A. L. Fairhall, W. Bialek, Neural

Comput. 15, 1715 (2003).
J. Benda, A. V. M. Herz, Neural Comput. 15, 2523 (2003).

32.
33. N. Fourcaud-Trocme´, D. Hansel, C. van Vreeswijk,

N. Brunel, J. Neurosci. 23, 11628 (2003).

34. E. Schneidman, B. Freedman, I. Segev, Neural Comput.

10, 1679 (1998).

35. A. Destexhe, D. Contreras, Science 314, 85 (2006).
36. F. S. Chance, L. F. Abbott, A. D. Reyes, Neuron 35, 773

(2002).

37. M. Carandini, PLoS Biol. 2, e264 (2004).
38. B. Naundorf, F. Wolf, M. Volgushev, Nature 440, 1060

(2006).

39. E. J. Chichilnisky, Network Comput. Neural Syst. 12, 199

(2001).

40. L. Paninski, Neural Comput. 15, 1191 (2003).
41. T. O. Sharpee et al., Nature 439, 936 (2006).
42. V. Mante, R. A. Frazor, V. Bonin, W. S. Geisler, M. Carandini,

Nat. Neurosci. 8, 1690 (2005).

43. T. Hosoya, S. A. Baccus, M. Meister, Nature 436, 71 (2005).
44. N. C. Rust, O. Schwartz, J. A. Movshon, E. P. Simoncelli,

Neuron 46, 945 (2005).

45. E. H. Adelson, J. R. Bergen, J. Opt. Soc. Am. A 2, 284 (1985).
46. Z. N. Aldworth, J. P. Miller, T. S. Gedeon, G. I. Cummins,

A. G. Dimitrov, J. Neurosci. 25, 5323 (2005).

47. T. Gollisch, Network Comput. Neural Syst. 17, 103 (2006).
48. T. Gollisch, A. V. M. Herz, PLoS Biol. 3, e8 (2005).
49. W. Gerstner, J. L. van Hemmen, Network Comput. Neural

50.

51.

Syst. 3, 139 (1992).
J. Keat, P. Reinagel, R. C. Reid, M. Meister, Neuron 30,
803 (2001).
J. W. Pillow, L. Paninski, V. J. Uzzell, E. P. Simoncelli,
E. J. Chichilnisky, J. Neurosci. 25, 11003 (2005).

52. F. Gabbiani, H. G. Krapp, C. Koch, G. Laurent, Nature 420,

320 (2002).

86, 2998 (2001).

53. C. K. Machens, M. S. Wehr, A. M. Zador, J. Neurosci. 24,

5. D. Jaeger, E. De Schutter, J. M. Bower, J. Neurosci. 17, 91

1089 (2004).

(1997).

6. D. Jaeger, J. M. Bower, J. Neurosci. 19, 6090 (1999).
7. A. Destexhe, M. Neubig, D. Ulrich, J. Huguenard,

J. Neurosci. 18, 3574 (1998).

8. P. A. Rhodes, R. Llinas, J. Physiol. 565, 765 (2005).
9. C. Koch, I. Segev, Nat. Neurosci. 3, 1171 (2000).
10. M. London, M. Ha¨usser, Annu. Rev. Neurosci. 28, 503

(2005).

11. F. Santamaria, J. M. Bower, J. Neurophysiol. 93, 237 (2005).
12. A. T. Schaefer, M. E. Larkum, B. Sakmann, A. Roth,

J. Neurophysiol. 89, 3143 (2003).

13. D. Debanne, Nat. Rev. Neurosci. 5, 304 (2004).
14. C. C. McIntyre, W. M. Grill, D. L. Sherman, N. V. Thakor,

J. Neurophysiol. 91, 1457 (2004).

54. M. Carandini et al., J. Neurosci. 25, 10577 (2005).
55. T. L. Adelman, W. Bialek, R. M. Olberg, Neuron 40, 823

(2003).

56. A. Borst, F. E. Theunissen, Nat. Neurosci. 2, 947 (1999).
57. H. B. Barlow, in Sensory Communication, W. A. Rosenblatt,

Ed. (MIT Press, Cambridge, MA, 1961), pp. 217–234.

58. A. L. Fairhall, G. D. Lewen, W. Bialek, R. R. D.

van Steveninck, Nature 412, 787 (2001).

59. B. Hassenstein, W. Z. Reichardt, Z. Naturforsch. 11b, 513

(1956).

60. A. Borst, V. L. Flanagin, H. Sompolinsky, Proc. Natl. Acad.

61.

Sci. U.S.A. 102, 6172 (2005).
I. Dean, N. S. Harper, D. McAlpine, Nat. Neurosci. 8,
1684 (2005).

Conclusions
There is no general solution for any of these chal-
lenges. Iterating the loop of model prediction,
experimental test, and model adjustment is an
obvious strategy for stepwise progress. One should
be aware, however, that elaborate single-cell models
are not sufficiently constrained by data, nor is there
any guarantee that crucial components of the real

15. A. S. Kuznetsov, N. J. Kopell, C. J. Wilson, J. Neurophysiol.

62. C. K. Machens, T. Gollisch, O. Kolesnikova, A. V. M. Herz,

95, 932 (2006).

Neuron 47, 447 (2005).

16. A. Kepecs, X. J. Wang, J. Lisman, J. Neurosci. 22, 9053 (2002).
17. L. A. Jeffress, J. Comp. Physiol. Psychol. 41, 35 (1948).
18. H. Agmon-Snir, C. E. Carr, J. Rinzel, Nature 393, 268 (1998).
19. A. V. Egorov, B. N. Hamam, E. Fransen, M. E. Hasselmo,

63. E. C. Smith, M. S. Lewicki, Nature 439, 978 (2006).
64. M. E. Fisher, in Springer Lecture Notes in Physics,
Vol. 186, F. J. Hahne, Ed. (Springer, Berlin, 1983),
pp. 1–139.

A. A. Alonso, Nature 420, 173 (2002).

65. T. P. Vogels, K. Rajan, L. F. Abbott, Annu. Rev. Neurosci.

20. Y. Loewenstein, H. Sompolinsky, Nat. Neurosci. 6, 961 (2003).
21. E. Fransen, B. Tahvildari, A. V. Egorov, M. E. Hasselmo,

28, 357 (2005).

66. O. Shriki, D. Hansel, H. Sompolinsky, Neural Comput. 15,

A. A. Alonso, Neuron 49, 735 (2006).

1809 (2003).

84

6 OCTOBER 2006 VOL 314 SCIENCE www.sciencemag.org

 

0
1
0
2

 
,

6
1

 

 

.

i

 

h
c
r
a
M
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 

 

d
e
d
a
o
n
w
o
D

l

67. P. Poirazi, T. Brannon, B. W. Mel, Neuron 37, 989

68.

(2003).
J. Golowasch, M. S. Goldman, L. F. Abbott, E. Marder,
J. Neurophysiol. 87, 1129 (2002).

69. M. S. Goldman, J. Golowash, E. Marder, L. F. Abbott,

J. Neurosci. 21, 5229 (2001).

70. M. Brecht et al., J. Neurosci. 24, 9223 (2004).
71. A. A. Prinz, L. F. Abbott, E. Marder, Trends Neurosci. 27,

218 (2004).

74. Neuroscience Database Gateway (http://ndg.sfn.org).
International Neuroinformatics Coordinating Facility
75.
(www.incf.org).

76. G. R. Holt, C. Koch, Neural Comput. 9, 1001 (1997).
77. B. W. Mel, Neural Comput. 4, 502 (1992).
78. C. Koch, T. Poggio, V. Torre, Proc. Natl. Acad. Sci. U.S.A.

80, 2799 (1983).

79. L. J. Borg-Graham, Nat. Neurosci. 4, 176 (2001).
80. T. Euler, P. B. Detwiler, W. Denk, Nature 418, 845

72. K. Vervaeke, H. Hu, L. J. Graham, J. F. Storm, Neuron 49,

(2002).

73.

257 (2006).
J. Wolfart, D. Debay, G. LeMasson, A. Destexhe, T. Bal,
Nat. Neurosci. 8, 1760 (2005).

81. M. J. Berry, I. H. Brivanlou, T. A. Jordan, M. Meister,

Nature 398, 334 (1999).
J. J. Hopfield, Nature 376, 33 (1995).

82.

REVIEW

Neuronal Computations with
Stochastic Network States

Alain Destexhe1* and Diego Contreras2

Neuronal networks in vivo are characterized by considerable spontaneous activity, which is
highly complex and intrinsically generated by a combination of single-cell electrophysiological
properties and recurrent circuits. As seen, for example, during waking compared with being
asleep or under anesthesia, neuronal responsiveness differs, concomitant with the pattern of
spontaneous brain activity. This pattern, which defines the state of the network, has a dramatic
influence on how local networks are engaged by inputs and, therefore, on how information is
represented. We review here experimental and theoretical evidence of the decisive role played by
stochastic network states in sensory responsiveness with emphasis on activated states such as
waking. From single cells to networks, experiments and computational models have addressed the
relation between neuronal responsiveness and the complex spatiotemporal patterns of network
activity. The understanding of the relation between network state dynamics and information
representation is a major challenge that will require developing, in conjunction, specific
experimental paradigms and theoretical frameworks.

Brain operations are embedded in a con-

tinuous stream of complex spontaneous
activity that interacts nonlinearly with
incoming sensory inputs, outgoing motor com-
mands, and internal association processes. Spon-
taneous brain activity refers to ongoing network
activity not dominated by any particular sin-
gle sensory input. Spontaneous brain activity
is generated by the combination of intrinsic
electrophysiological properties of single neu-
rons (1) and synaptic interactions in networks
(2); it is dependent on the level of activation of
neuromodulatory systems (3, 4) and is cor-
related with the functional state of the brain (2).
Most of the existing knowledge about the re-
lation between neuronal responsiveness and
spontaneous brain activity comes from the com-
parison between waking and sleep states (5).
However, even within the stable state of wak-

1Integrative and Computational Neuroscience Unit (UNIC),
CNRS, Gif sur Yvette, France. 2Department of Neuroscience,
University of Pennsylvania School of Medicine, Philadelphia, PA
19104, USA.

*To whom correspondence should be addressed. E-mail:
Destexhe@iaf.cnrs-gif.fr.

ing, subtle variations in the spatiotemporal
pattern of network activation strongly influ-
ence information processing, and vice versa,
sensory inputs modify ongoing activity. Such
interplay between intrinsically generated ac-
tivity and its modulation by external input is
at the very core of the mechanisms by which
the brain represents the external world and
elaborates successful response strategies. The
complexity of network dynamics is beyond
the reach of current recording methods and
requires appropriate computational methods
carefully constrained by biological data. Pre-
dictions from current modeling efforts are a
critical guide for designing new experimental
approaches.

Experimental Characterization of Intrinsic
Dynamics in Neocortex
Understanding the neuronal mechanisms of
spontaneous brain activity is of critical impor-
tance in understanding its role in information
processing. For example, the cellular mecha-
nisms of synchronized oscillations during sleep
and anesthesia explain why neural responsive-

SPECIALSECTION

83. T. W. Margrie, A. T. Schaefer, J. Physiol. 546, 363

(2003).

84. D. Durstewitz, J. Neurosci. 23, 5342 (2003).
85. G. Rainer, S. C. Rao, E. K. Miller, J. Neurosci. 19, 5493

(1999).

86. R. C. Cannon, D. A. Turner, G. K. Pyapali, H. V. Wheal,

J. Neurosci. Methods 84, 49 (1998).

87. We thank M. Brecht, A. Destexhe, F. Gabbiani, E. Izhikevich,
and A. Roth for helpful comments on earlier versions of this
review, and E. Izhikevich for providing Fig. 2.

10.1126/science.1127240

ness is reduced during those states (2). How-
ever, much less is known about the complex
intrinsic dynamics that characterize the sponta-
neous activity during the waking state. It is
during the waking state that response variability
and the spatiotemporal patterns of network ac-
tivation are key elements of the brain operations
that generate adaptive behavior.

The spontaneous activity recorded in the
electroencephalogram (EEG) from cortex and
thalamus varies greatly between waking and
sleep states. During sleep, the EEG is dominated
by large-amplitude waves with high temporal
and spatial coherence (Fig. 1A), and most of its
spectral power is below 15 Hz (4). Rhythmic
components are prevalent, although they are
highly aperiodic and interspersed with non-
rhythmic large-amplitude waves. Intracellular
recordings in vivo demonstrate large variations
in membrane potential (Vm) occurring synchro-
nously across large populations (5, 6).

In contrast, upon awakening or during rapid
eye movement (REM) sleep (also termed brain
activated states), EEG spontaneous activity is
characterized by low-amplitude waves, with
low spatial and temporal coherence and high
spatiotemporal complexity (Fig. 1B), not domi-
nated by any identifiable pattern (4). The spec-
tral power of the activated EEG is characterized
by frequencies above 15 Hz. Intracellular rec-
ordings in vivo during activated states demon-
strate absence of slow oscillations or any large
Vm fluctuations characteristic of sleep (7). In-
stead, cortical and thalamic neurons show a
stable resting Vm at a depolarized level close to
firing threshold and a noisy, highly irregular
pattern of background synaptic activity (7) (Fig.
1C). Interspersed within the synaptic back-
ground activity, there are short bouts of fast
(20 to 80 Hz) oscillations, which last a few tens
of milliseconds and which are occasionally
crowned by spikes (8). Therefore, fast oscillations
in cortical and thalamic networks are different
from the intrinsically generated, well-organized,
and stable subthreshold oscillations in highly
rhythmic structures such as the inferior olive (9).
Fast oscillations also appear in relation to sen-
sory stimuli and have been proposed to subserve
a coordinating function among neuronal groups

www.sciencemag.org SCIENCE VOL 314

6 OCTOBER 2006

85

 

0
1
0
2

 
,

6
1

 

 

.

i

 

h
c
r
a
M
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 

 

d
e
d
a
o
n
w
o
D

l

