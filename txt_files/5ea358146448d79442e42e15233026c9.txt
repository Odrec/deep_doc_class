IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—II: EXPRESS BRIEFS, VOL. 58, NO. 8, AUGUST 2011

537

Robust Quasi-Newton Adaptive Filtering Algorithms

Md. Zulﬁquar Ali Bhotto, Student Member, IEEE, and Andreas Antoniou, Life Fellow, IEEE

Abstract—Two robust quasi-Newton (QN) adaptive ﬁltering al-
gorithms that perform well in impulsive-noise environments are
proposed. The new algorithms use an improved estimate of the in-
verse of the autocorrelation matrix and an improved weight-vector
update equation, which lead to improved speed of convergence and
steady-state misalignment relative to those achieved in the known
QN algorithms. A stability analysis shows that the proposed
algorithms are asymptotically stable. The proposed algorithms
perform data-selective adaptation, which signiﬁcantly reduces the
amount of computation required. Simulation results presented
demonstrate the attractive features of the proposed algorithms.

Index

Terms—Adaptive

in
adaptive ﬁlters, quasi-Newton algorithms, robust adaptation
algorithms.

impulsive

ﬁlters,

noise

I. INTRODUCTION

K NOWN approaches for improving the performance of

adaptive ﬁlters in impulsive-noise environments involve
the use of nonlinear clipping [1], [2], robust statistics [3]–[5],
or order statistics. The common step in the adaptive ﬁlters
reported in [1]–[5] is to detect the presence of impulsive noise
by comparing the magnitude of the error signal with the value of
a threshold parameter, which is a scalar multiple of the variance
of the error signal, and then either stop or reduce the learning
rate of the adaptive ﬁlter. In [1], the variance of the error
signal is estimated by averaging the square of its instantaneous
values, but this approach is not robust with respect to impulsive
noise. In [2]–[5], improved robustness is achieved by estimating
the variance of the error signal using the median absolute
deviation [6].

The adaptation algorithms in [1] and [3] use the Huber
mixed-norm M-estimate objective function [6], and the algo-
rithms in [4] and [5] use the Hampel three-part redescending
M-estimate objective function [6]. The Huber function uses the
L1 norm to measure the amplitude of the error signal when the
absolute error is larger than the threshold. The Hampel function,
on the other hand, assigns a constant value to the error signal
when the absolute error becomes larger than the threshold.
Algorithms based on the Huber and Hampel functions of-
fer similar performance. The nonlinear recursive least-squares
(NRLS) algorithm in [2] uses nonlinear clipping to control the
learning rate and offers better performance in impulsive-noise

Manuscript received October 1, 2010; revised December 31, 2010 and
March 16, 2011; accepted May 14, 2011. Date of publication July 22, 2011;
date of current version August 17, 2011. This work was supported by the
Natural Sciences and Engineering Research Council of Canada. This paper was
recommended by Associate Editor Z. Lin.

The authors are with the Department of Electrical and Computer Engi-
neering, University of Victoria, Victoria, BC V8W 3P6, Canada (e-mail:
zbhotto@ece.uvic.ca; aantoniou@ieee.org).

Color versions of one or more of the ﬁgures in this paper are available online

environments than the conventional RLS algorithm. The recur-
sive least-mean (RLM) algorithm reported in [4] offers faster
convergence and better robustness than the NRLS algorithm
in impulsive-noise environments. The quasi-Newton algorithm
(QN) in [7] is not robust against impulsive noise. Simulation re-
sults in [5] show that the recursive QN (RQN) algorithm offers
faster convergence and improved robustness in impulsive-noise
environments relative to the QN algorithm in [7]. Algorithms
of the Newton family such as those in [2], [4], [5], and [7]
converge much faster than algorithms of the steepest-descent
family [8]. The RLS, RLM, and RQN algorithms exponentially
forget past input signal vectors in the estimate of the autocor-
relation matrix, and therefore, the positive deﬁniteness of the
inverse of the autocorrelation matrix can be lost [9], [10]. As a
result, this type of adaptive ﬁlter can become unstable in ﬁnite-
precision implementations. This form of explosive divergence
in RLS adaptive ﬁlters is well documented in the literature, and
ways and conditions to stabilize these ﬁlters are addressed in
[9] and [10]. The QN algorithm reported in [7] is shown to be
robust in terms of explosive divergence, as compared with RLS
adaptive ﬁlters.

In this brief, we propose two new robust QN algorithms
that perform data-selective adaptation in updating the inverse
of the autocorrelation matrix and the weight vector. The new
algorithms are essentially enhancements of the algorithms we
reported in [16] for applications that entail impulsive noise. A
stability analysis shows that the proposed algorithms are as-
ymptotically stable. Furthermore, simulation results show that
the proposed algorithms offer improved performance relative
to two known QN (KQN) algorithms with respect to robust-
ness, steady-state misalignment, computational efﬁciency, and
tracking capability.

This brief is organized as follows. In Section II, the proposed
robust QN algorithms are described. In Section III, stability
issues of the algorithms are discussed, and in Section IV, some
practical issues concerning the implementation of the proposed
algorithms are examined. Simulation results are presented in
Section V, and conclusions are drawn in Section VI.

II. PROPOSED ROBUST QN ALGORITHMS

Two slightly different robust QN algorithms are possible, one
using a ﬁxed threshold and the other using a variable threshold,
as will now be demonstrated.

A. RQN Algorithm with a Fixed Threshold

The objective of the proposed adaptation algorithm is to
generate a series of weight vectors that would eventually solve
the optimization problem

(dk − wT xk)2

(1)

at http://ieeexplore.ieee.org.

Digital Object Identiﬁer 10.1109/TCSII.2011.2158722

minimize

w

E

1549-7747/$26.00 © 2011 IEEE

(cid:2)

(cid:3)

538

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—II: EXPRESS BRIEFS, VOL. 58, NO. 8, AUGUST 2011

recursively, where E[·] is the expected value of [·], xk is a
vector of dimension M representing the input signal, dk is the
desired signal, and w is the weight vector, which is also of
dimension M. An approximate solution of the problem in (1)
can be obtained by using the weight-vector update equation

wk = wk−1 + 2μkSk−1xkek

(2)

where μk is the step size, Sk−1 is a positive deﬁnite matrix of
dimension M × M, and

ek = dk − yk

(3)

k−1xk. If Sk−1
is the a priori error for the output signal yk = wT
in (2) is chosen as the M × M identity matrix, then the update
equation in (2) would minimize the objective function

(cid:4)

(cid:5)2

Jwk−1 =

dk − wT

k−1xk

with respect to the steepest-descent direction, and a series of
updates would eventually yield an approximate solution of the
problem in (1). Other choices of Sk−1 would entail different
search directions but would serve the same purpose as long as
Sk−1 is positive deﬁnite. In order to use an approximation of
the Newton direction Sk−1 in (2), we obtain Sk−1 by using the
gradient of Jwk−1 in (4) in the rank-one update formula of the
classical QN optimization algorithm [11], which is given by

Sk = Sk−1 − (δk−Sk−1ρk)(δk − Sk−1ρk)T

(δk − Sk−1ρk)T ρk

where

δk = wk − wk−1

ρk =

∂e2
k+1
∂wk

− ∂e2
∂wk−1

k

.

(4)

(5)

(6)

(7)

value of the prespeciﬁed error bound, i.e.,
k = γ · sign(ek)

we obtain ∇2

k = 0, and hence from (7), we have

ρk = 2ekxk.

(12)

(13)

Vector δk, which is linearly dependent on Sk−1xk, can be
obtained by using (2) and (6). Since the equality in (12) is
satisﬁed for each update, we can use the a posteriori error to
obtain

δk = 2γ · sign(ek)Sk−1xk

(14)

instead of the a priori error used in the KQN algorithm reported
in [8]. Now, substituting ρk and δk given by (13) and (14) in (5),
we obtain an update equation for matrix Sk for the proposed
robust QN algorithm as

Sk = Sk−1 − αk

Sk−1xkxT

k Sk−1

k Sk−1xk
xT

.

(15)

Substituting μk given by (10) in (2), we obtain the correspond-
ing weight-vector update equation as

wk = wk−1 + αk

Sk−1xk
k Sk−1xk
xT

ek.

(16)

In order to achieve robust performance against impulsive noise,
we choose error bound γ as

(cid:6)|ek| − υθk,

γ =

γc,

if |ek| > θk
otherwise

√

(17)

5σv, where
where γc is a prespeciﬁed error bound chosen as
v is the variance of the measurement noise, 0 ≤ υ (cid:4) 1 is a
σ2
scalar, and θk is a threshold parameter, which is estimated as

θk = 1.98σk

(18)

This formula satisﬁes the Fletcher QN condition Skρk = δk
[12]. From (3), we note that ek+1 would require future data
xk+1 and dk+1. To circumvent this problem, we use the a
posteriori error

where

k = dk − xT

k wk

(8)

in place of ek+1 in (7). As a ﬁrst step in the proposed algo-
rithm, we obtain a value of step size μk in (2) by solving the
optimization problem

(cid:6)(cid:7)(cid:7)dk − xT

k wk

(cid:7)(cid:7) − γ,

if |ek| > γ
otherwise

(9)

minimize

μk

0,

where γ is a prespeciﬁed error bound. The solution of this
problem can be obtained as

μk = αk

1
2τk

(10)

(cid:6)

where τk = xT

k Sk−1xk, and

1 − γ|ek| ,
0,

if |ek| > γ
otherwise.

αk =

(11)
In effect, step size μk is chosen to force equality |k| = γ
whenever |ek| > γ. Since μk in (10) forces k to assume the

(19)

···

k−1 + (1 − λ) min(gk)
k + 

k = λσ2
σ2
with 0 (cid:4) λ < 1, gk = [ e2
k−P +1 +  ] is a vector
e2
of dimension P , where  is a small scalar. Whenever |ek| < γ,
no update is applied. Consequently, the amount of computation
and the required storage are signiﬁcantly reduced since Sk is
not evaluated in every iteration. A similar weight-vector update
strategy has been used in set-membership adaptation algorithms
[13]–[15], but the mathematical framework of those algorithms
is very different from that of the proposed robust QN algorithm.
The estimator in (19) is robust to outliers. A large σ2
0 would
cause |ek| to be less than θk during the transient state, and
therefore, the algorithm would work with error bound γc, which
would increase the initial rate of convergence. For a sudden
system disturbance, θk would also be very large, in which case
we obtain γc < |ek| < θk, and thus, the algorithm would again
use error bound γc, and therefore, the tracking capability of the
algorithm would be retained. For an impulsive noise-corrupted
error signal, θk would not increase, in which case the error
bound would be θk = |ek| − υθk, and this would suppress the
impulsive noise.

