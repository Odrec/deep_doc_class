Reservoir Computing (C)

By Khanh Le and Ferdinand Schlatt

Contents

● General Concept

● General Procedure

● Properties

● Global Parameters

2

General Concept
and some Notations

3

What is Reservoir Computing?

● Temporal Recurrent Neural Network 

(Dominey 1995)

● Liquid State Machine (Natschläger, 

Maass and Markram 2002)

● Echo State Network (Jaeger 2001)
● Decorrelation-Backpropagation 

Learning (Steil 2004)

4

What is Reservoir Computing?

Reservoir 

5

General Concept

● Reservoir: random, large, fixed 

RNN with input signal

● Output signal read out by a 

trainable, linear combination of 
all response signals

6

Reservoir vs. Multilayer Perceptron

7

Some Notations

● At time step t: 

○ x(t): activation of internal units
○ u(t): activation of input units
○ y(t): activation of output units 

● W: weight matrix
● Y: output matrix

8

General Procedure
for both ESN and LSM

9

General Procedure for Using ESN and LSM

Initialize random network and input weights between 
-1 and 1

Initialize neuronal states to 0

10

General Procedure for Using ESN and LSM

Initialize random network and input weights between 
-1 and 1

Initialize neuronal states to 0

Pseudocode:

W = random(# of neurons, # of neurons) * 2 - 1
Win= random(# of neurons, dimension of input) * 2 - 1
x = zeros(# number of neurons)

11

General Procedure for Using ESN’s and LSM’s

Feed input into network and record network states
x(t) = f(Win * u(t) + W * x(t-1))
z(t) = [u(t) ; x(t)]
y(t) = g(Woutz(t))

u(t)

z(t)

12

General Procedure for Using ESN and LSM

Feed input into network and record network states
x(t) = f(Win * u(t) + W * x(t-1))
z(t) = [u(t) ; x(t)]
y(t) = g(Woutz(t))

Pseudocode:

for vector in inputs:

x = f(Win * vector + W * x)
Z.append(vector ; x)

13

Example

+2

1

0

0

+
0
3

.

-0.7

+0.4

-0.2

0.2

+

-0.5

+0.6

3

0

2

1:  tanh(2*0.2 + 0*0.4) = 0.38
2: tanh(2*(-0.5) + 0*(-0.7)) = -0.76
3: tanh(2*0.6 + 0*0.3 + 0*(-0.2)) = 0.83

14

Example

0.2

+

-0.5

+0.6

+2

1

0.38

-0.7

+0.4

+
0
3

.

0.83

3

-0.2

-0.76

2

z(1) = [2 , 0.38 , -0.76 , 0.83]

15

Example

-1.5

0.2

+

-0.5

+0.6

1

0.38

-0.7

+0.4

+
0
3

.

0.83

3

-0.2

-0.76

2

1:  tanh((-1.5)*0.2 + (-0.76*0.4) = -0.54
2: tanh((-1.5)*(-0.5) + (-0.76)*(-0.7)) = 0.49
3: tanh((-1.5)*0.6 + 0.38*0.3 + 0.83*(-0.2)) = 0.74

16

Example

-1.5

0.2

+

-0.5

+0.6

1

-0.54

-0.7

+0.4

+
0
3

.

0.74

3

-0.2

0.49

2

z(2) = [-1.5 , -0.15 , 0.49 , 0.74 ]

17

General Procedure for Using ESN and LSM

Calculate output weights by linear regression (if 
activation function g is the identity function)
Minimize MSE between output values and targets

Ytarget = Wout * Z
Wout = Ytarget * Z♱

Pseudocode:

Wout = Y * pseudo_inverse(Z)

Wout

18

General Procedure for Using ESN and LSM

Input new data and calculate system state

Calculate output with extended system state (input + 
neuronal states) and Wout

input

z(t)

19

General Procedure for Using ESN and LSM

Input new data and calculate system state

Calculate output with extended system state (input + 
neuronal states) and Wout

z

Wout

y

General Procedure for Using ESN and LSM

Input new data and calculate system state

Calculate output with extended system state (input + 
neuronal states) and Wout

Pseudocode:

x = f(Win * input + W * x)
y = Wout * (input ; x)

21

Properties
and Differences between ESN and LSM

22

Properties

LSM

Separation  Property

Approximation Property

ESN

Echo State Property

Continuity (fading 
memory)

Continuity (fading 
memory)

23

Separation & Approximation Properties (LSM)

● Separation property: 

○ Different input patterns return different 

liquid states

○ Depends on complexity of liquid

● Approximation property:

○ Distinguishes & transforms different internal 

states of liquid into given target outputs

○ Depends on adaptability of readout 

mechanism

24

Spectral Radius 

… maximal absolute eigenvalue of 
matrix W

Denoted as |λmax|

25

Echo State Property

● Reservoir asymptotically washes out 

any information from initial 
conditions

● Granted for any input if spectral 

radius is smaller than 1

● Violated for zero input if spectral 

radius is larger than 1

26

Continuity Property of ESN (“fading memory”)

● Current network output with a 
given precision ⇐ most recent 
inputs with a similar precision

● Information in reservoir is 

clearer for newly introduced 
information and fades away for 
previous ones 

27

Differences Between LSM’s and ESN’s

LSM
Biological motivation; tries 
to model neurodynamical 
systems in the brain

ESN
Motivation from 
informatics and 
engineering

Neurons in reservoir 
typically biological spiking 
models of neurons

Neurons in reservoir 
usually sigmoid units

Different readout 
mechanisms used; 
reservoir seen as 
preprocessing

Readout done by single 
linear readout layer

Generally performance 
better

28

Global Parameters
and Advantages & Disadvantages of Reservoir Computing

29

Global Parameters of Reservoir Computing

Many parameters exist which can be tuned to 
optimize performance

Few actually make significant impact and all 
are highly task dependent

size of reservoir, sparsity, distribution of nonzero 
elements, spectral radius, scaling of Win, leaking rate, 
regularization coefficient, Removal of x # of neuronal 
states

30

Global Parameters of Reservoir Computing

Size of reservoir

Bigger reservoir for 
more complicated tasks; 
as large possible

Sparsity

Distribution of non-
zero elements in 
reservoir matrix

Reduces computational 
cost; no significant 
performance increase

Usually uniform or 
gaussian distribution; 
binary distribution can 
help to understand 
network dynamics

31

Global Parameters of Reservoir Computing

Spectral radius

Tasks which require 
longer memory profit 
from higher SR

Scaling of Win

Like in all neural 
networks highly 
recommended

Leaking rate
x = (1-α)x(n-1)*α*x’(n)

Can help in 
performance if fitted to 
relative loss over time

32

Global Parameters of Reservoir Computing

Regularization 
Coefficient
Wout = Ytarget * XT (XXT+β*I)-1

Counteracts overfitting; 
punishes large output 
weights

Removal of x # of 
neuronal states

Removes bias resulting 
from initial 0 states

33

Advantages and Disadvantages of Reservoir Computing

Good for predicting and generating time series 
dependent data (Mackey-Glass) 
Different types of networks usually solve other 
desired tasks better

Interesting features:
- Low computational cost; especially for RNN
- Multiple readouts from one reservoir
- Preprocessing for different types of ANNs
- Possibility of using a different medium as 

reservoir

34

http://users.sussex.ac.uk/~ctf20/dphil_2005/Publications/bucket.pdf

35

References

Jaeger, H., "Echo state network", Scholarpedia, vol. 2, no. 9, pp. 2330, 2007.

Lukoševičius, M., and H. Jaeger, "Reservoir computing approaches to 
recurrent neural network training", 
Computer Science Review , vol. 3, no. 3, pp. 127-149, August, 2009.

Maass, W., T. Natschlaeger, and H. Markram, "Real-time Computing without 
stable states: A New Framework for Neural Computation Based on 
Perturbations", Neural Computation, vol. 14, no. 11, pp. 2531–2560, 2002.

http://organic.elis.ugent.be/flavors

Lukosevicius M., “A Practical Guide to Applying Echo State Networks” 2012

36

