Vorlesung Multivariate Verfahren

Prof. Dr. Thomas Staufenbiel

Universität Osnabrück

Forschungsmethodik, Diagnostik und Evaluation

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

0

Multivariate Verfahren

Multiple Regression & Korrelation

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

1

Multiple Regression & Korrelation: Gliederung Teil III

Stichproben‐ und Prädiktorauswahl

9
10 Spezialfälle der Regression 

(Nicht‐lineare Regression …)

11 Besondere Anwendungen der Regression

(Mediatoranalysen, Moderatoranalysen)

12 Verschiedenes

(Kreuzvalidierung, Multikollinearität, Allgemeines 
lineares Modell)

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

2

Multiple Regression: Stichproben‐ und Prädiktorauswahl

 Faustregeln: Der Stichprobenumfang n sollte in Abhängigkeit von der Zahl der 

Prädiktoren m …
• mindestens 15·m betragen (Stevens, 2009, S. 117ff) 
• bei der Interpretation von R mindestens 50 + 8·m und bei der Interpretation der 

Gewichte mindestens 104 + m betragen (bei mittlerer Effektstärke sowie Fehlerniveaus 
 = .05 und  = .20; Green, 1991)

 Grundsätzlich lassen sich zumindest drei Strategien bei der Bestimmung des "optimalen" 

Prädiktorsatzes bestimmen:
• Standard: Alle (theoretisch relevanten!) Prädiktoren werden gemeinsam in die 

Regressionsgleichung aufgenommen.

• Sequentielle (auch: hierarchische oder schrittweise) Regression

‐ automatisch aufgrund statistischer Kriterien (hier als statistische sequentielle 

Regression bezeichnet)

‐ in inhaltlich festgelegten Blöcken (hier als blockweise sequentielle Regression

bezeichnet)

‐ eine Kombination von beiden Vorgehensweisen

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

3

Multiple Regression: Statistische sequentielle Regression

 Statistische schrittweise Verfahren versuchen, eine möglichst kleine Zahl von Prädiktoren 
aus der Gesamtmenge aller Prädiktoren auszuwählen, die gleichzeitig aber noch möglichst 
viel Varianz in dem Kriterium erklärt.

 Im Prinzip könnte man alle möglichen Prädiktor‐Kombinationen (das sind 2m; also z.B. bei 

m = 15 entsprechend 215 = 32768 Kombinationen) bestimmen und dann diejenige aus‐
wählen, die hinsichtlich eines Kriteriums die beste ist. Dieses Vorgehen ist sehr aufwendig 
(und unüblich). Stattdessen:

 Bei den statistischen Verfahren gibt es verschiedene Strategien:

• Bei der forward selection wird beginnend mit 0 Prädiktoren sukzessive jeweils der 

Prädiktor in das Regressionsmodell aufgenommen, der den Gesamtfit R2 am stärksten 
ansteigen lässt.

• Bei der backward elimination werden ausgehend von der Menge aller Prädiktoren 

schrittweise die Prädiktoren eliminiert, die den geringsten Schwund in erklärter 
Varianz bedingen.

• Das Vorgehen des stepwise regression ist eine Variante der forward selection, bei der 

auch bereits aufgenommene Prädiktoren zur Maximierung des Fits in späteren 
Schritten wieder eliminiert werden können.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

4

Multiple Regression: Statistische sequentielle Regression

 Spezifiziert werden können dabei

• Minimalanforderungen an die selegierten Prädiktoren, z.B. hinsichtlich des p‐ oder des 

Tol‐Wertes

• Abbruchregeln, z.B. die Zahl der Schritte, kein sign. Zuwachs im F‐Wert

 Probleme:

• Prädiktorkombinationen mit einer unterschiedlichen Zahl an Prädiktoren sind schwierig 

zu vergleichen.

• Der inferenzstatistische Gesamttest und die Einzeltests der Gewichte sind nicht mehr 

korrekt ("capitalization on chance").

• Die Verfahren finden nicht notwendigerweise die optimale Teilmenge von Prädiktoren.
• Das Vorgehen basiert nur auf statistischer Optimierung; die Selektion erfolgt völlig 

atheoretisch.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

5

Multiple Regression: Blockweise sequentielle Regression

 Bei der blockweisen sequentiellen Regression werden nacheinander Blöcke von ( 1) 

Prädiktoren in die Regression aufgenommen und jeweils untersucht, ob durch den 
zusätzlichen Block ein bedeutsamer zusätzlicher Teil an Varianz erklärt werden kann.

 Beispiel: Im Kontext der Personalauswahl wird untersucht, ob es mit einem neu ent‐

wickelten Diagnostikum zur Erfassung sozialer Kompetenzen gelingt, den Berufserfolg über 
das hinaus vorherzusagen, was die bekanntermaßen validen Prädiktoren Intelligenz und 
Gewissenhaftigkeit zu leisten imstande sind (Kriterium: Berufserfolg, Prädiktor‐Block 1: 
Intelligenz, Gewissenhaftigkeit, Prädiktorblock 2: Test zur Erfassung der sozialen 
Kompetenzen).

 Die Überprüfung der Frage, ob durch die Aufnahme eines zusätzlichen Blocks von k Prädik‐
toren ein statistisch signifikanter Anstieg des Zuwachses in R2 resultiert, kann mittels des 
folgenden F‐Tests geprüft werden:

F




1




R
2
m k

R
2
m k



R m
2

m
 
n m k
  


1

mit

df
 

Zähler



k

und

df
 

Nenner

   

n m k

1

R
2
 
m k

R
2
m

wird häufig auch als R2 bezeichnet.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

6

Multiple Regression: Blockweise sequentielle Regression

 Beispiel: Volmer und Staufenbiel (2006) untersuchten die Wirksamkeit eines strukturierten 
Telefoninterviews zur internationalen Selektion von Trainees (n = 61 deutsche Studierende 
für Praktikum in Firmen in den USA). Erfolgs‐Kriterium (hier): Leistungsbeurteilung der 
Anleiter.

Schritt/ 
Block

Alter
Geschlecht
Abiturnote
Vordiplomsnote
Auslandserfahrung
Zahl bisheriger Praktika
Interviewleistung

R2:
R2:

r
.16
.08
‐.21
.20
.27*
.36**
.47**

1
.03
.12

.04
.04

b-Gewichte
2
.04
.11
‐.54**
.38**

3
.03
.03
‐.36*
.38**
.02
.10

.22**
.19**

.31**
.08*

4
.02
.09
‐.30
.30*
.01
.04
.31*
.31*
.36**
.06*
.06*

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

7

Multiple Regression: Blockweise sequentielle Regression

Hier werden blockweise die 
Prädiktoren angeben; durch 
(Weiter) kommt man in den 
nächsten Block; Methode bleibt 
"Einschluss"

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

8

Multiple Regression: Blockweise sequentielle Regression

Die inferenzstatistischen Prüfungen für die R2‐
Werte erhält man unter (Statistiken) durch die 
Aktivierung der Option „Änderung in R‐Quadrat“

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

9

Multiple Regression: Blockweise sequentielle Regression

Blöcke 1 bis 4

R2‐Werte mit F‐Prüfgrößen und p‐Werten

Z.B. ist für den vierten Block R2 = 0.37, p < .01
mit R2 = 0.06, F(1, 53) = 4.66, p < .05.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

10

Multiple Regression: Blockweise sequentielle Regression

nur Block 1

Block 1 & 2

Block 1 bis 3

Block 1 bis 4

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

11

Multiple Regression & Korrelation: Gliederung Teil III

Stichproben‐ und Prädiktorauswahl

9
10 Spezialfälle der Regression 

(Nicht‐lineare Regression …)

11 Besondere Anwendungen der Regression

(Mediatoranalysen, Moderatoranalysen)

12 Verschiedenes

(Kreuzvalidierung, Multikollinearität, Allgemeines 
lineares Modell)

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

12

Multiple Regression: Spezialfälle der Regression

 Die Multiple Regression ist ein Spezialfall des sog. Allgemeinen Linearen Modells (ALM), mit 

dem darüber hinaus auch mehrkategorielle nominal skalierte Variablen (z.B. Religions‐
zugehörigkeit) als Prädiktoren eingesetzt werden können.
Dies erfordert spezielle (z.B. sog. Dummy‐) Codierungen der Variablen und erlaubt dann 
auch Varianzanalysen und Kontraste als Spezialfälle des ALM zu betrachten (und zu 
berechnen).

 Multiple Regression kann seinerseits auf bestimmte spezielle Situationen angewendet 

werden, u.a.
• Nichtlineare Regression
• Regression mit Interaktionseffekten

 Daneben gibt es neben der hier behandelten ordinary least squares method (OLM) andere 

bzw. modifizierte Regressionsverfahren, u.a.
• logistische Regression (für dichotome Kriterien)
• multi‐level Regression (für Daten auf verschiedenen Aggregationsebenen)
• latent growth models, time series analysis (für Längsschnittanalysen)
• ridge regression, two‐stage‐least squares, weightes least squares …

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

13

Multiple Regression: Nicht‐lineare Zusammenhänge

 Mithilfe der multiplen Regression können auch bestimmte nicht‐lineare Zusammenhänge 

untersucht werden (Aiken & West, 1991; Cohen et al., 2003; Darlington, 1990).

 Die Untersuchung nicht‐linearer Zusammenhänge kann aufgrund theoretischer Erwägun‐
gen erfolgen oder z.B. auch als Folge bestimmter Anomalien in den Regressionsdiagnos‐
tiken (z.B. im Plot der ei gegen die 

) des linearen Modells.

ˆiy

 Die nicht‐linearen Prädiktoren/Terme können den linearen hinzugefügt werden oder sie 
ersetzen. Im ersteren Fall kann eine Variable in zwei oder mehrere Prädiktoren eingehen.

 Untersucht wird, ob die (Hinzufügung der) nicht‐linearen Regressionsterme einen statistisch 

signifikanten (zusätzlichen) Anteil an Varianz erklärt.

Y

 Vermutet man z.B. einen umgekehrt U‐förmigen Zusammenhang, so 
kann dieser durch einen quadratischen Term modelliert werden. Wir 
erhalten dann das Regressionsmodell:



b x
2
i
2

y
ˆ
i

b
0



b x
i
1

 Mittels blockweiser Regression prüft man zunächst nur den linearen 

X
Term und dann im zweiten Block zusätzlich den quadratischen. Eine Bestätigung für den 
umgekehrt U‐förmigen Zusammenhang ergibt sich, wenn R2 statistisch signifikant ist 
(und b2 < 0). 

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

14

Nicht‐lineare Zusammenhänge

Y

90
80
70
60
50
40
30
20
10
0
‐10

‐10

‐5

0

5

10

X

Y X

2

Y

10
0
‐10
‐20
‐30
‐40
‐50
‐60
‐70
‐80
‐90

‐10

‐5

0

5

10

Y
5 3
  

X 
X

Y

X

2

2
 

2

X

Bsp.: X: Frequenz eines Tones

Y: Wahrnehmungsschwelle

Bsp.: X: Aktivierung

Y: Prüfungsleistung

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

15

Nicht‐lineare Zusammenhänge

Y

4

3

2

1

0

0

4

8

12

16

20

24

28
X

)

Y

X
(

X
ln(
0)


Bsp.: X: Reizintensität

Y: Empfindungsstärke

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

3

Y

1

‐1

‐3

‐2

‐1

0

1

2

X

3

Y X


Y X

3 3
 

X

Bsp.: X: Zeit

Y: Affekt bei Pt mit manisch‐

depressiver Erkrankung

(Stand 24.10.2014)

16

Multiple Regression: Nicht‐lineare Zusammenhänge

 Die auf den beiden vorangegangenen Folien dargestellten funktionalen Beziehungen sind 
mittels multipler Regression prüfbar. Für den logarithmischen Zusammenhang würde dann 
beispielsweise die Regressionsgleichung wie folgt lauten: 

ln



b
0

b
 
1



x
i



y
ˆ
i

 Per multipler Regression können allerdings nicht beliebige funktionale Zusammenhänge 

modelliert werden. 

 Sie müssen auf der Prädiktorseite immer die Form b1·f1(X1)+b2·f2(X2) + ... aufweisen, wobei 

die Funktionen f1, f2 usw. ohne zu schätzende Parameter beliebig gewählt werden dürfen. 
Die Beziehungen dürfen also nicht‐linear in den Variablen sein. Sie müssen aber linear in 
den Parametern (Gewichten) sein. Beziehungen, für die das z.B. nicht gilt sind:

b
0



b
1

x

b
 oder 
0



b x
i
2

2

b x

i
1
1
b x
i
3

3

 Manche solcher Beziehungen können linearisiert werden (vgl. Bortz & Schuster, 2010, Kap. 

 

x

b
1

y
ˆ
ln( )



b
ln(
0

)

b
 
1

x
ln( )

11.3.2), z.B.

y b
ˆ
0



 Allgemein können in solchen Fällen z.B. Verfahren der nicht‐linearen Regression (general

curve‐fitting) herangezogen werden.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

17

Multiple Regression: Nicht‐lineare Zusammenhänge

Y

20

15
15

10
10

5
5

0
0

0

2

4

6

8 10 12 14 16 18 20

X

20

15
15

10
10

5
5

0
0

20

15
15

10
10

5
5

0
0

9.87

x

b x
 
i
1

p

n
30,

b
y
ˆ

i
0
y
ˆ
7.48 0.28


i
r
.05
.40,

r
.16
2




x
i

0

2

4

6

8 10 12 14 16 18 20

y
ˆ
i
y
ˆ
i




b x
b
b x
2


 
i
i
2
1
0

x
7.72 0.28



i

x
0.07


i
p
.01




x
x




2

R
R
R


.63,

.40
2

.24,
2


p



.01

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

18

0

2

4

6

8 10 12 14 16 18 20

Multiple Regression: Nicht‐lineare Zusammenhänge

 (Umgekehrt) U‐förmige Zusammenhänge lassen sich durch ein Polynom zweiter Ordnung 

mittels multipler Regression prüfen: 

y
ˆi



b
0



b x
i
1



b x
2
i
2

 Dabei wird auch die lineare Komponente in die Gleichung aufgenommen. Durch die Hinzu‐

nahme des quadratischen Terms ändert sich allerdings die Deutung des Regressions‐
gewichts des linearen Terms. Das Gewicht gibt nun nur noch lokal die Steigung der Parabel 
an der Stelle x = 0 an (was meistens nicht von Interesse ist).

 Besonders wenn das Verhältnis zwischen dem Mittelwert und der Standardabweichung 

eines Prädiktors sehr groß ist, dann korrelieren X und X2 (und auch X3...) sehr hoch 
miteinander und Multikollinearität kann die Folge sein. Beispiel: Die Werte 1000, 1001, 
1002, 1003, 1004 und die Werte 10002, 10012, 10022, 10032, 10042 korrelieren zu 
0.99999983 (vgl. Darlington, 1990).

 Durch eine vorherige Zentrierung von X kann eine bedeutsame Verringerung der Korrela‐
tion zwischen X und X2 erzielt werden. Dadurch kann das Problem der Multikollinearität
verschwinden. So korrelieren im obigen Beispiel die zentrierten Werte              mit deren 
Quadraten                  zu 0.

x

x

ix



ix

2

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

19

Multiple Regression: Nicht‐lineare Zusammenhänge

 Um in SPSS eine Variable zu zentrieren, bestimmt man zunächst deren Mittelwert und 

erzeugt dann eine neue zentrierte Variable.

 Im obigen Beispiel sieht die Syntax dann wie folgt aus:

COMPUTE xc=x-9.87.
COMPUTE x2c=xc*xc.
EXECUTE.
REGRESSION
REGRESSION

Zentrierung der Variable X

Quadrierung der 
zentrierten Variable

/DESCRIPTIVES MEAN STDDEV CORR SIG N
/DESCRIPTIVES MEAN STDDEV CORR SIG N
/MISSING LISTWISE
/MISSING LISTWISE
/STATISTICS COEFF OUTS R ANOVA COLLIN TOL ZPP CHANGE
/STATISTICS COEFF OUTS R ANOVA COLLIN TOL ZPP CHANGE
/CRITERIA=PIN(.05) POUT(.10)
/CRITERIA=PIN(.05) POUT(.10)
/NOORIGIN 
/NOORIGIN 
/DEPENDENT y
/DEPENDENT y
/METHOD=ENTER xc
/METHOD=ENTER xc
/METHOD=ENTER x2c.
/METHOD=ENTER x2c.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

20

Multiple Regression & Korrelation: Gliederung Teil III

Stichproben‐ und Prädiktorauswahl

9
10 Spezialfälle der Regression 

(Nicht‐lineare Regression …)

11 Besondere Anwendungen der Regression

(Mediatoranalysen, Moderatoranalysen)

12 Verschiedenes

(Kreuzvalidierung, Multikollinearität, Allgemeines 
lineares Modell)

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

21

Multiple Regression: Mediatoranalysen

 Ein Mediator ist eine Variable Z, die den Zusammen‐

hang zwischen zwei Variablen X und Y vermittelt 
(d.h. X wirkt auf Z und Z auf Y). 
Möglich ist, dass der Zusammenhang nur partiell
mediiert wird, also auch noch eine direkte Wirkung 
von X auf Y bleibt.

X

Z

Y

 Beispiel: Die positive Wirkung des X = „Interesses am Unterricht“ auf die Y = „Schulleistung“ 

wird vermittelt durch Z = „das Ausmaß der Beteiligung am Unterricht“.

 Eine Mediatorhypothese kann über eine Serie von drei Regressionsgleichungen geprüft 

werden (Baron & Kenny, 1986):
1.
2.
3.

b x
i
1
b x
i
1
b x
i
1

b
0
b
0
b
0









:  X muss einen signifikanten Einfluss auf Y haben
:  X muss einen signifikanten Einfluss auf Z haben
zb

i
2

y
ˆi
z
ˆi
y
ˆi
statistisch kontrolliert wird. Wenn zusätzlich das Gewicht b1 von X statistisch 
insignifikant wird, dann liegt vollständige Mediation vor.

:  Z muss auf einen signifikanten Einfluss auf Y haben, wenn für X

 Eine Alternative besteht in der Überprüfung per Strukturgleichungsmodellierung.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

22

Multiple Regression: Moderatoranalysen

 Ein Moderator ist eine Variable Z, die die Stärke und/oder 

die Richtung des Zusammenhangs zwischen zwei 
Variablen X und Y beeinflusst. Z kann kontinuierlich oder 
kategorial sein.

X

Y

Z

 Beispiel: Der negative Zusammenhang zwischen X = „Ausmaß eines Stressors“ und Y = 

„psychischer Gesundheit“ wird moderiert durch Z = „Kontrollierbarkeit der Stressoren“ (z.B. 
wird vermutet, dass er stärker negativ bei unkontrollierbaren Stressoren ausfällt).

 Eine Moderatorhypothese kann per Regression über die Aufnahme des Interaktionsterms
getestet werden:                                               . Ein Moderatoreffekt liegt dann vor, wenn das 
Regressionsgewicht der Interaktion statistisch signifikant von Null verschieden ist 
("moderierte Regression"). 

b x z
i
3

b x
i
1

b z
2

b
0

y
ˆi









i

i

 Bei dieser Form der Analyse treten häufiger Multikollinearitätsprobleme auf, die manchmal 

durch die Zentrierung der Variablen x und z (z.B.                   ) mit dem folgenden Modell 
behoben werden kann: 

x
i













x

z

iy
ˆ

b
0

b x
1

'
i

b
2

'
i

b x z
'
3
i

x
'
i
'
i

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

23

Multiple Regression: Moderatoranalysen

 Eine Alternative ist die Subgruppenanalyse, bei der der Zusammenhang zwischen X und Y
getrennt für k durch die Ausprägungen des Moderators definierte Subgruppen bestimmt 
wird. Ein Moderatoreffekt liegt dann vor, wenn die H0: 1 = 2 = .. = k zurückgewiesen 
werden kann (Nachteil: geringere Power). Ebenfalls möglich: Moderatoranalysen per 
Strukturgleichungsmodellierung.

 Wurde eine Moderatorhypothese bestätigt, interessiert man sich dafür, ob die Unter‐

schiede in den Zusammenhängen so ausfallen, wie vermutet.

 Beispiel: In einer Studie von Staufenbiel, Batinic und 

Menne (2009) wurde die folgende Hypothese 
aufgestellt:  Der Zusammenhang zwischen kognitiver 
und affektiver Arbeitsplatzunsicherheit (=AU) ist stärker, 
wenn die finanzielle Abhängigkeit vom Job höher ist.

 Ergebnis: Int = 0.24, p < 0.01

 

U
A
e
v
i
t
k
e
f
f
a

(Abbildung rechts erstellt mit dem SPSS‐Makro simple-
2way von O‘Connor (1998), frei herunterladbar unter 
https://people.ok.ubc.ca/brioconn/simple/simple.html)

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

hohe finanzielle 
Abhängigkeit

niedrige finanzielle
Abhängigkeit

kognitive AU

(Stand 24.10.2014)

24

Multiple Regression: Mediator und Moderatoren am Beispiel 

Job characteristics model (JCM) von Hackman & Oldham (1980)

Tätigkeitsmerkmale

Anforderungsvielfalt

Ganzheitlichkeit

Wichtigkeit

Autonomie

Feedback durch Tätigk.

Erlebniszustände

= Mediatorvariablen

Erlebte Bedeutsamkeit 
der eigenen Tätigkeit

Erlebte Verantwortung 
für die Ergebnisse der Tät.

Wissen über die 

Resultate der Tätigkeit

Arbeitsergebnisse

intrinsische 

Arbeitsmotivation

allgemeine 

Arbeitszufriedenheit

Zufriedenheit mit 
Wachstumsmöglichk.

Arbeitseffektivität

Moderatorvariablen

Wachstumsbedürfnis, Wissen und Fähigkeiten, 

Kontextzufriedenheiten

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

25

Multiple Regression & Korrelation: Gliederung Teil III

Stichproben‐ und Prädiktorauswahl

9
10 Spezialfälle der Regression 

(Nicht‐lineare Regression …)

11 Besondere Anwendungen der Regression

(Mediatoranalysen, Moderatoranalysen)

12 Verschiedenes

(Kreuzvalidierung, Multikollinearität, Allgemeines 
lineares Modell)

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

26

Multiple Regression: Kreuzvalidierung

 Die Kreuzvalidierung dient der Überprüfung der Regressionsstatistiken anhand einer 

zweiten, unabhängigen (Teil‐) Stichprobe. Diese kann entweder
• eine vorhandene zweite Stichprobe sein oder
• durch eine Teilung der ursprünglichen Stichprobe entstanden sein (z.B. im Verhältnis 

80% : 20%)

 Dabei werden wie immer zunächst im ersten Schritt die Regressionsgewichte an der ersten 

Stichprobe bestimmt. Im zweiten Schritt setzt man dann die Prädiktorwerte der zweiten 
Stichprobe in diese Gleichung ein. Die multiple Korrelation (der so geschätzten mit den 
empirischen Kriteriumswerten) kann dann mit der der ersten verglichen werden (und sollte 
nicht wesentlich geringer sein).

 Möglich ist auch, das Vorgehen zusätzlich in umgekehrter Richtung (d.h. Schätzung der 
Regressionsgewichte an der zweiten und Überprüfen an der ersten Stichprobe) vorzu‐
nehmen ("double cross validation").

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

27

Multiple Regression: Multikollinearität

 (Multi)‐Kollinearität bezeichnet einmal allgemein eine wechselseitige (lineare) Abhängig‐
keit der Prädiktoren untereinander und meist spezieller den Fall starker Abhängigkeit, der 
zu Problemen führt (Extremfall: Singularität).

 Effekt:

• Die Genauigkeit der Schätzungen der Regressionsgewichte der multikollinearen 
Prädiktoren nimmt ab (d.h. deren Standardfehler       werden groß und damit die 
Regressionsgewichte häufig statistisch nicht mehr signifikant). Es wird damit schwierig, 
den Einfluss eines multikollinearen Prädiktors auf das Kriterium abzuschätzen.

jb

 Diagnose:

• Betrachtung der Tolj ‐Werte bzw. VIFj ‐Werte (Variance Inflation Factor) wobei 

VIFj = 1/Tolj. Daumenregel: VIFj ‐Werte  10 sind problematisch.

 Mögliche Lösungen:

• Elimination eines der hoch korrelierenden Prädiktoren
• Zusammenfassung hoch korrelierender Prädiktoren in einen Prädiktor
• Ridge‐Regression (wird hier nicht näher behandelt)

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

28

Multiple Regression: Multikollinearität in SPSS

 SPSS prüft standardmäßig auf den Fall von Singularität (innerhalb der Rundungsgenauig‐
keit). Falls dieser Test (default: Tolj < .0001) positiv ausfällt, wird der Prädiktor nicht mit in 
die Regressionsgleichung aufgenommen und ist dann in einer Tabelle "Ausgeschlossene 
Variablen" aufgeführt.

 Statistiken, die über diesen (ungewöhnlichen) Grenzfall hinaus 

gehen, erhält man unter (Statistiken) durch Aktivierung 
von „Kollinearitätsdiagnose“.

 Die Tol- bzw. VIF‐Werte werden dann als zusätzliche Spalten 

für alle Prädiktoren in der Koeffizienten‐Tabelle aufgeführt:

Koeffizientena

Nicht standardisierte

Koeffizienten

Standardisierte
Koeffizienten

Modell
1

(Konstante)
WITAL
WITSCH
WITBO
MTP

B
15,077
-,007
-,069
,013
-,055

a. 

Abhängige Variable: BERNOTE

Standardf

ehler

2,649
,017
,018
,013
,014

Beta

-,063
-,550
,154
-,613

T
5,691
-,436
-3,788
,976
-3,817

Signifikanz
,000
,667
,001
,339
,001

Kollinearitätsstatistik

Toleranz

VIF

,942
,932
,788
,760

1,061
1,072
1,269
1,315

Im Beispiel sind die VIF‐
Werte niedrig, also un‐
problematisch.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

29

Multiple Regression: Allgemeines lineares Modell

 Die multiple Regression lässt sich so erweitern, dass statt den intervallskalierten 

Prädiktoren auch kategoriale (nominalskalierte) Variablen berücksichtigt werden können. 
Man spricht dann auch vom Allgemeinen Linearen Modell (ALM).

 Die kategorialen Variablen können beispielsweise natürlich nominale Variablen (z.B. 

Religionszugehörigkeit), künstlich dichotomisiert sein (z.B. Alter in  30 J. und >30 J.) oder 
Faktoren aus (Quasi‐) Experimenten darstellen (z.B. Lehrmethode A vs. B vs. KG).

 Dadurch ergibt sich die Möglichkeit, Analysen wie beispielsweise T‐Tests oder Varianz‐

analysen über das ALM durchzuführen.

 Kategoriale Variablen können nicht direkt als Prädiktoren einer Regressionsgleichung 

eingesetzt werden, sondern müssen vorher in bestimmter Weise codiert werden.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

30

Multiple Regression: Allgemeines lineares Modell

 Dazu werden bestimmte künstliche neue (auch als Indikatorvariablen oder Codierungs‐

variablen bezeichnete) Variablen erzeugt, die die Informationen über die Gruppenzu‐
gehörigkeiten der einzelnen Personen enthalten. Eine Voraussetzung dabei ist, dass jede 
Person genau einer Gruppe bzw. Kategorie angehört.

 Es existieren verschiedene alternative Codierungssysteme (z.B. Dummy‐Codierung, Effekt‐

Codierung), die ...
• alle für die Codierung einer Variable mit g Gruppen (Kategorien) g  1 Codierungs‐

variablen benötigen

• identische Ergebnisse in Bezug auf R und dessen Signifikanztestung produzieren
• zu unterschiedlichen Deutungen der Regressionsgewichte führen und damit für 

verschiedene Fragestellungen unterschiedlich gut geeignet sind.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

31

Multiple Regression: Allgemeines lineares Modell

 Bei der Dummy‐Codierung wird den Personen einer Referenzgruppe (z.B. der Kontroll‐

gruppe in einem Experiment) die Codierung (0, 0, … ,0) zugewiesen. Allen anderen Personen 
erhalten in allen Codierungsvariablen eine 0 bis auf die Codierungsvariable, die ihre Gruppe 
repräsentiert, in der sie eine 1 erhalten.

 Beispiel: Einfaktorieller Versuchsplan mit UV 
= Lehrmethode und AV = Lernleistung mit je 
n = 3 Vpn pro Gruppe:

Kontroll‐
Kontroll‐
gruppe
gruppe

Faktor: Lehrmethode
konven‐
konven‐
tionell
tionell

ver‐
ver‐
zweigt
zweigt

24
24
21
21
16
16

32
32
37
37
33
33

43
43
30
30
41
41

Web‐
Web‐
gestützt
gestützt

42
42
37
37
33
33

AV
24
21
16
32
37
33
43
30
41
42
37
33

C1
0
0
0
1
1
1
0
0
0
0
0
0

C2
0
0
0
0
0
0
1
1
1
0
0
0

C3
0
0
0
0
0
0
0
0
0
1
1
1

Kontroll‐
gruppe

konven‐
tionell

ver‐
zweigt

Web‐
gestützt

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

32

Multiple Regression: Allgemeines lineares Modell

 Im Unterschied zur Dummy‐Codierung wird bei der (ungewichteten) Effekt‐Codierung der 

Referenzgruppe der Vektor (‐1, ‐1, …, ‐1) zugewiesen:

AV
24
21
16
32
37
33
43
30
41
42
37
33

C1
0
0
0
1
1
1
0
0
0
0
0
0

C2
0
0
0
0
0
0
1
1
1
0
0
0

C3
0
0
0
0
0
0
0
0
0
1
1
1

AV
24
21
16
32
37
33
43
30
41
42
37
33

C1
‐1
‐1
‐1
1
1
1
0
0
0
0
0
0

C2
‐1
‐1
‐1
0
0
0
1
1
1
0
0
0

C3
‐1
‐1
‐1
0
0
0
0
0
0
1
1
1

Dummy‐Codierung

Effekt‐Codierung

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

33

Multiple Regression: Allgemeines lineares Modell

 Nachdem man die kategoriale(n) Variable(n) codiert hat, werden die Codierungsvariablen 

als Prädiktoren in die Multiple Regression aufgenommen.

 Im Beispiel mit der Untersuchung des Effektes der Lehrmethode auf die Lernleistung sieht 

die Regressionsgleichung entsprechend wie folgt aus:
y b
ˆ
0

bC
 
3

b
 
1

C
3

C
1

b
2







2

 Der Signifikanztest für R entspricht dann dem F‐Test in der einfaktoriellen Varianzanalyse.

 Bei der Dummy‐Codierung resultiert für die Kontrollgruppe (mit Nullen in allen 
b
0
b
0

Codierungsvariablen) dann:
und für die erste Gruppe (konventionell):
Die additive Konstante b0 in der multiplen Regression entspricht hier der durchschnittlichen 
Merkmalsausprägung der AV in der Referenzgruppe (hier: der KG); das Regressionsgewicht 
bi gibt die Differenz der Mittelwerte der Gruppe i und der Referenzgruppe an.

0
0
   
0
0
   

0
  
1
  

b
2
b
2

ˆy b
0
ˆy b
0




b
3
b
3

usw.

b
1
b
1

b
1



 Bei der Effekt‐Codierung entspricht b0 dem Gesamtmittelwert der AV und bi der Differenz 
des Gruppenmittelwertes vom Gesamtmittelwert (nicht so ohne Weiteres zu erkennen, zur 
Herleitung siehe etwa Bortz & Schuster, 2010, S. 365f).

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

34

Multiple Regression: Zitierte Quellen

Zitierte Quellen:

 Aiken, L. S. & West, S. G. (1991). Multiple regression: Testing and interpreting interactions. Newbury Park: Sage. 
 Backhaus, K., Erichson, B., Plinke, W. & Weiber, R. (2003). Multivariate Analysemethoden. Eine anwendungsorientierte 

Einführung (10. Auflage). Berlin: Springer.

 Green, S. B. (1991). How many subjects does it take to do a regression analysis? Multivariate Behavioral Research, 26, 

499‐510.

 Volmer, J. & Staufenbiel, T. (2006). Entwicklung und Erprobung eines strukturierten Interviews zur internationalen 

Personalauswahl. Zeitschrift für Arbeits‐ und Organisationspsychologie, 50, 17‐22.

Vorlesung »Multivariate Verfahren« (WS 2014/2015)
Prof. Dr. Thomas Staufenbiel

(Stand 24.10.2014)

35

