Public Opinion Quarterly, Vol. 79, No. 4, Winter 2015, pp. 952–975

COMPARING EXTREME RESPONSE STYLES BETWEEN 
AGREE-DISAGREE AND ITEM-SPECIFIC SCALES

MINGNAN LIU*
SUNGHEE LEE
FREDERICK G. CONRAD

Abstract  Although Likert scales in agree-disagree (A/D) format are 
popular  in  surveys,  the  data  quality  yielded  by  them  is  controversial 
among  researchers.  Recognizing  the  measurement  issues  involved 
with  the A/D  format,  researchers  have  developed  other  question  for-
mats to measure attitudes. In this study, we focused on an alternative 
question type, the item-specific (IS) question, which asks the respond-
ent to choose an option that best describes his or her attitude. Using 
political  efficacy  items  from  the American  National  Election  Studies 
(ANES), we compare extreme response style (ERS) between A/D and 
IS scales. Latent class factor analysis showed that ERS exists in both 
A/D  and  IS  scale  formats,  but  differs  slightly  across  the  two. Also, 
when analyzing ERS within subjects across two waves, there is only a 
single ERS for both question formats, after controlling for the correla-
tion within respondents. The last finding suggests that ERS is a stable 
characteristic.

Mingnan Liu is a survey scientist at SurveyMonkey, Palo Alto, CA, USA. Sunghee Lee is 
an assistant research scientist at Survey Research Center, University of Michigan, Ann Arbor, 
MI, USA. Frederick G. Conrad is a research professor and director of the Program in Survey 
Methodology,  Institute  for  Social  Research,  University  of  Michigan, Ann Arbor,  MI,  USA, 
and  the  Joint  Program  in  Survey  Methodology,  University  of  Maryland,  College  Park,  MD, 
USA, and a professor of psychology at the University of Michigan, Ann Arbor, MI, USA. The 
authors thank Norbert Schwarz, Yu Xie, and the editors and reviewers for their feedback on 
previous  drafts  of  this  manuscript.  This  work  was  partially  supported  by  the  Rensis  Likert 
Fund  in  Research  in  Survey  Methodology,  Program  in  Survey  Methodology,  University  of 
Michigan. American National Election Studies data were collected by Stanford University and 
the  University  of  Michigan,  supported  by  the  National  Science  Foundation  [grant  numbers 
SES-0937715  and  SES-0937727]. Any  opinions,  findings,  and  conclusions  or  recommenda-
tions expressed in this study are those of the authors and do not necessarily reflect the views of 
the funding organizations. *Address correspondence to Mingnan Liu, SurveyMonkey, 101 Lytton 
Avenue, CA, 94301, USA; e-mail: mingnanl@surveymonkey.com.

doi:10.1093/poq/nfv034 
Advance Access publication August 14, 2015
© The Author 2015. Published by Oxford University Press on behalf of the American Association for Public Opinion Research. 
All rights reserved. For permissions, please e-mail: journals.permissions@oup.com

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

Comparing Extreme Response Styles

953

When designing survey questions to measure opinions and attitudes, one of 
the first ideas that comes to a researcher’s mind is probably a Likert scale. As 
Bradburn, Sudman, and Wansink (2004) point out, attitudes comprise cogni-
tions,  evaluations,  and  behaviors. The  cognitive  component  is  one’s  beliefs 
about the target object. The evaluative component is one’s evaluation of the 
object.  The  behavioral  component  is  the  connection  between  attitude  and 
behavior. A Likert scale can be used to measure both the respondent’s evalua-
tion (agree vs. disagree) of the rating object and the strength of the evaluation 
(strongly  vs.  somewhat). That  is,  it  measures  the  intensity  of  one’s  attitude 
toward  an  object  (Krosnick  and Abelson  1992).  Furthermore,  it  may  seem 
easy to design Likert scales, since researchers have only to create questions 
describing  the  objects  they  want  to  measure  without  varying  the  response 
options.  These  advantages  make  Likert  scales  very  popular  among  social 
scientists  and  marketing  researchers.  For  example,  the  Marketing  Scales 
Handbook and other similar references provide numerous citations to publica-
tions using Likert scales (Bearden and Netemeyer 1999; Bruner, Hensel, and 
James 2001).

Although they are popular in many fields, the data quality yielded by Likert 
scales  in  the  agree-disagree  (A/D)  format  is  controversial  among  research-
ers (for examples, see Clark and Clark 1977; Billiet and McClendon 2000; 
Fowler and Cosenza 2008; DeVellis 2011; Revilla, Saris, and Krosnick 2013). 
Some  survey  methodologists  have  even  argued  that  “researchers  will  have 
more  reliable,  valid,  and  interpretable  data  if  they  avoid  the  agree-disagree 
question form” (Fowler 2008, 105). One of the concerns about the A/D scale 
is acquiescent response style bias, defined as the tendency to choose “agree” 
or “yes” responses more frequently than other response options (Baumgartner 
and Steenkamp 2001). Recognizing the measurement issues involved with the 
A/D  format,  researchers  have  developed  other  question  formats  to  measure 
attitudes. In this study, we will focus on one of these question types, the item-
specific  (IS)  question,  which  asks  the  respondent  to  choose  an  option  that 
best describes his or her attitude. Distinct from the A/D format, which offers 
the same response options for all questions, an IS question presents response 
options that are specific to the question contents (see appendix A for examples 
of IS questions).

Efforts have been taken to compare the quality of data from A/D and IS 
rating scales. Although some research has examined extreme response style 
(ERS) using the A/D scale, no work has yet examined ERS in the context of 
the IS scale. ERS, the tendency to select the two endpoints of a response scale 
more frequently than the intermediate ones (Paulhus 1991), produces another 
common measurement bias in rating scales. This study fills that gap by exam-
ining ERS with A/D and IS scales using experimental data in a panel study. 
We also answer two research questions. First, do respondents show different 
patterns of ERS when using an IS scale compared to an A/D scale? Since ERS 
is regarded as a type of measurement error, a scale that yields lower levels of 

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
p
o
q

.

o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 
a
t
 

U
n
i
v
e
r
s
i
t

Ã
¤
t
s
b
i
b
l
i
o
t
h
e
k
O
s
n
a
b
r
Ã
¼
c
k

 

 

 

o
n
A
p
r
i
l
 

1

,
 

2
0
1
6

