 
Neuroinformatics Lecture (L15) 

Prof. Dr. Gordon Pipa  
Institute of Cognitive Science University of Osnabrück 

 

Curve fitting using cubic spines ML 

12/4/2014 

Neuroinformatics - Prof. Dr. Gordon Pipa 

Idea: use maximum 
likelihood to estimate 
mML(x) and sML from a 
data set x.   
 
 
 
We thereby assume 
an explicit 
dependence of m on x 

Note that x indicates 
the dataset and x is 
a parameter of m 

To estimate  

 

 

We define a flexible function  

 

 

And adjust the parameters 
such that the likelihood for the 
data t = (t1, . . . , tN)T sampled 
at the points x = (x1, . . . , xN)T  
is maximized.  

For a Gaussian error distribution, we 
will see that this means minimizing 
the squared error.  

12/4/2014 

Neuroinformatics - Prof. Dr. Gordon Pipa 

Curve fitting using cubic spines ML 

We use a linear model of the parameters w 

12/4/2014 

Neuroinformatics - Prof. Dr. Gordon Pipa 

(|())MLptµx(|()(,))MLptµxyxwSo far:  

With x becoming a vector this product of the parameter vector and the 
basis function is evaluated for every data point xi, leading to a set of y 
values for every x.  
 
Therefore y becomes a vector that has as many components a we have 
samples. However, for the reason of simplification y is not explicitly written 
as a vector but just a function that is evaluated for all xi in the vector x.  

10(,)()()MTijjiijyxwwxwx12/4/2014 

Lecture Neuroinformatics - Cognitive Science Osnabrück - Dr. G. Pipa 

Maximum Likelihood for Regressions 

score function: 

 
r
a
f
 
o
S

Assumption:  
Linear function 

We get: 

To get an ML estimator, we set the score function for all wi to zero: 

Maximum Likelihood for Regressions 

To get an ML estimator (here normal distributed residuals), we set the score function 
for all wi to zero: 

Parameters and  
Basis functions 

Samples and  
Basis functions 

Neuroinformatics - Prof. Dr. Gordon Pipa 

Maximum Likelihood for Regressions 

We want to rewrite this in a vector/Matrix Notation. We start with:  

Dimensions:  

 1 x M*[ M x 1 * 1x1] = 1 x 1 

 

 1 x 1 

One eqn. for every single  
Basis function / parameter 

In the vector notation, this can be written as (                                                              ) : 

Dimensions:  

 1 x M*[ M x 1 * 1 x M] = 1 x M 

Neuroinformatics - Prof. Dr. Gordon Pipa 

Maximum Likelihood for Regressions 

We want to rewrite this in a vector/Matrix Notation. We start with:  

Dimensions:  

 1 x M*[ M x 1 * 1x1] = 1 x 1 

One eqn. for every single  
Basis function / parameter 

In the vector notation, this can be written as (                                                              ) : 

Dimensions:  

 1 x M*[ M x 1 * 1 x M] = 1 x M 

Neuroinformatics - Prof. Dr. Gordon Pipa 

One eqn. for all  
Basis function / parameter 

Maximum Likelihood for Regressions 

one eqn. for all  
parameter / basis functions 

 
r
a
f
 
o
S

Next we use a vector notation again to express the sum over the N data points:  
 
To this end we define the design matrix composed of M basis functions, that are each 
defined for N data points xn 

basis  
function 
M-1 

basis  
function 
0 

basis  
function 
1 

Maximum Likelihood for Regressions 

one eqn. for all  
parameter / basis functions 

 
r
a
f
 
o
S

Next we use a vector notation again to express the sum over the N data points:  
 
To this end we define the design matrix composed of M basis functions, that are each 
defined for N data points xn 

basis  
function 
M-1 

basis  
function 
0 

basis  
function 
1 

Matrix Multiplication   

c11 

c1q 

row times column vectors 

12/4/2014 

Lecture Advanced Neuroinformatics - Cognitive Science Osnabrück - Dr. G. Pipa 

Maximum Likelihood for Regressions 

So far :  
 
 
 
 
 
 

Dimensions:  

 1 x M*[ M x 1 * 1 x M] = 1 x M 

We use the design matrix and matrix notation and rewrite the eqn. above:  
 

Dimensions:        (1xM) (MxN) (NxM) = ((MxN) (Nx1))T 
                                          (1xM)(MxM)            = 1xM 

Maximum Likelihood for Regressions 

To get an ML estimator, get the following set of eqn. that are can be written in 
vector/matrix notation: 
 

Parameters and  
Basis functions 

Samples and  
Basis functions 

To  get the ML parameter w we need to solve that eqn. 
 
We want a following type of eqn: 
 
  

(,)wftMaximum Likelihood for Regressions 

So far :  
 
 
 
 
We now reorder the matrix notation and transpose it 
 
 
 

 
r
a
f
 
o
S

We transpose both sides 

We reorder both sides 

We reorder left side 

Dimensions:        (MxN) (NxM) (Mx1) = (MxN)(Nx1)) 
                                         (MxM) (Mx1)            =  Mx1 

Summary  

To get an ML estimator, we set the score function for all wi to zero: 

In the vector notation, this can be written as: 

With: 

To find wML , we need to solve the eqn:  

The Pseudo Inverse for every matrix A of               is defined and 
unique         and it satisfies the following two properties: 

 

 
For an invertible matrix, the pseudo inverse becomes equivalent to 
the inverse.  
 
It can be computed using Single Value Dicomposition (SVD), which 
is implemented in MATLAB by the function pinv.m  

12/4/2014 

Neuroinformatics - Prof. Dr. Gordon Pipa 

Maximum Likelihood for Regressions 

As before, we are determining the ML estimate by setting the score 
function to zero.  

 

 

 

 
 We want to solve this equation for w: 
 
We now multiply both sides with                     from left 

Pseudo inverse ! 

After Reording we got 

Matlab command: pinv(PHI)*t = w 

Maximum Likelihood for Regressions 

ML solution for the parameter w, for a Gaussian error and a linear model we got: 

 
r
a
f
 
o
S

Dimensions:        =(MxN)(NxM)-1)(MxN) 
                                             
                            =(MxM)-1 (MxN)     Computional Complexity ~ M2 
 
 
 
To find the ML solution for the parameter using a tool like Matlab we construct the 
design matrix PHI let matlab compute its pseudo inverse.  Than we multiply this with 
the data t.        
 
 
 
 

pinv(PHI)*t = w 

Matlab command: pinv(PHI)*t = w 

 

 

Maximum Likelihood for Regressions 

On the last slide, we introduced 

 

This includes the Moore-Penrose Pseudo Inverse   

 

 

 

 

The Moore-Penrose Pseudo allows us to compute an ‘inverse’ matrix 
even if the matrix is NxM with N>M being different.   

 
Keep in mind that the standard inverse 1 is only defined for MxM 
matrices. 

 

Trick Pseudo inverse (derives inverse for MxM) and then expands 
back to an MxN matrix.  

 

Therefore the maximal rank is M.  

Regression  

• We assume we have a Gaussian 

distribution with an expected value being 
a function of x 

 

 

• We assume homoscedasticity (Variance 

is identical for all x) 

 

 

 

 

 

• We model the dependence of µ(x) by using a linear combinations of 

fixed nonlinear basis functions φj(x) , of the form 
 

 

 

 

• We find the parameters wj by using ML estimate that is the minimum 
squared error. To find this ML, we use the Penrose pseudo inverse. 

 

Neuroinformatics Questionnaire  

Timer (5min):  

Start 

Stop 

Neuroinformatics - Prof. Dr. Gordon Pipa 

Suppose we have identical distributed and independent data 
sets      : 

Likelihoods: 
 

Likelihood: 
 

Log Likelihood: 
 

Important for successive updates of ML estimates  

Maximum Likelihood for Regressions 

As before, we are determining the ML estimate by setting the score 
function to zero.  

 

 

 

 
 We want to solve this equation for w: 
 
We now multiply both sides with   

Pseudo inverse ! 

After Reording we got 

Matlab command: pinv(PHI)*t = w 

